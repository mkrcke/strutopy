{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0e8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gensim import corpora\n",
    "from scipy import optimize\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8522ba",
   "metadata": {},
   "source": [
    "### Ingest corpus to create documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "documents = corpora.MmCorpus('corpus.mm')\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stm(documents, settings): \n",
    "      \n",
    "    K = settings['dim']['K']\n",
    "    V = settings['dim']['V']\n",
    "    A = settings['dim']['A']\n",
    "    N = settings['dim']['N']\n",
    "    \n",
    "    #Random initialization\n",
    "    mu = np.array([0]*(K-1))[:,None]\n",
    "    sigma = np.zeros(((K-1),(K-1)))\n",
    "    diag = np.diagonal(sigma, 0)\n",
    "    diag.setflags(write=True)\n",
    "    diag.fill(20)\n",
    "    beta = random.gamma(.1,1, V*K).reshape(K,V)\n",
    "    beta = (beta / beta.sum(axis=1)[:,None])\n",
    "    lambd = np.zeros((N, (K-1)))\n",
    "    \n",
    "    #turn beta into a list and assign it for each aspect\n",
    "    beta = [beta, beta] # FOR A=2\n",
    "    kappa_initialized = init_kappa(documents, K, V, A, interactions=settings['kappa']['interactions'])\n",
    "    \n",
    "    #create model object\n",
    "    model = {'mu':mu, 'sigma':sigma, 'beta': beta, 'lambda': lambd, 'kappa':kappa_initialized}\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def init_kappa(documents, K, V, A, interactions): \n",
    "    # read in documents and vocab\n",
    "    flat_documents = [item for sublist in documents for item in sublist]\n",
    "    m = []\n",
    "\n",
    "    total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "    for elem in flat_documents: \n",
    "        m.append(elem[1] / total_sum)\n",
    "\n",
    "    m = np.log(m) - np.log(np.mean(m)) #logit of m\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    aspectmod = A > 1 # if there is more than one topical content variable\n",
    "    if(aspectmod):\n",
    "        interact = interactions # allow for the choice to interact\n",
    "    else:\n",
    "        interact = FALSE\n",
    "\n",
    "    #Create the parameters object\n",
    "    parLength = K + A * aspectmod + (K*A)*interact\n",
    "\n",
    "    #create covariates. one element per item in parameter list.\n",
    "    #generation by type because its conceptually simpler\n",
    "    if not aspectmod & interact:\n",
    "        covar = {'k': np.arange(K),\n",
    "             'a': np.repeat(np.nan, parLength), #why parLength? \n",
    "             'type': np.repeat(1, K)}\n",
    "\n",
    "    if(aspectmod & interact == False):\n",
    "        covar = {'k': np.append(np.arange(K), np.repeat(np.nan, A)),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.arange(A)), \n",
    "                 'type': np.append(np.repeat(1, K), np.repeat(2, A))}      \n",
    "    if(interact):\n",
    "        covar = {'k': np.append(np.arange(K), np.append(np.repeat(np.nan, A), np.repeat(np.arange(K), A))),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.append(np.arange(A), np.repeat(np.arange(A), K))), \n",
    "                 'type': np.append(np.repeat(1, K), np.append(np.repeat(2, A),  np.repeat(3,K*A)))}\n",
    "\n",
    "    kappa = {'out': {'m':m,\n",
    "                     'params' : np.tile(np.repeat(0,V), (parLength, 1)),\n",
    "                     'covar' : covar\n",
    "                     #'kappasum':, why rolling sum?\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    return(kappa['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c486270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_weights(x, weight):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = weight*np.exp(x - np.max(x))[:,None]\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def lhood(eta, mu, siginv, doc_ct, Ndoc, eta_long, beta_tuple, phi, theta, neta):\n",
    "    \n",
    "    #formula \n",
    "    #rewrite LSE to prevent overflow\n",
    "    part1 = np.sum(doc_ct * (eta_long.max() + np.log(np.exp(eta_long - eta_long.max())@beta_tuple)))-np.sum(doc_ct)*scipy.special.logsumexp(eta)\n",
    "    part2 = .5*(eta-mu)@siginv@(eta-mu)\n",
    "    \n",
    "    out = part2 - part1\n",
    "    \n",
    "    return -out\n",
    "\n",
    "def grad(eta, mu, siginv, doc_ct,  Ndoc, eta_long, beta_tuple, phi, theta, neta):\n",
    "\n",
    "    #formula\n",
    "    part1 = np.delete(np.sum(phi * doc_ct,axis=1) - np.sum(doc_ct)*theta, neta)\n",
    "    part2 = siginv@(eta-mu)\n",
    "\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d8e2d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(documents, mu, sigma, lambd, beta):\n",
    "    #quickly define useful constants\n",
    "    V = beta['beta'][0].shape[1] # ncol\n",
    "    K = beta['beta'][0].shape[0] # nrow\n",
    "    N = len(documents)\n",
    "    A = len(beta['beta'])\n",
    "    \n",
    "    # 1) Initialize Sufficient Statistics \n",
    "    sigma_ss = np.zeros(((K-1),(K-1)))\n",
    "    if A == 2: \n",
    "        beta_ss = [np.zeros((K,V)), np.zeros((K,V))]\n",
    "    else:\n",
    "        print('Error: Only two metadata columns allowed.')\n",
    "    bound = np.repeat(0,N)\n",
    "    #lambd = np.repeat(0,N)\n",
    "    \n",
    "    # 2) Precalculate common components\n",
    "    sigobj = np.linalg.cholesky(sigma)\n",
    "    sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "    siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "    \n",
    "    # 3) Document Scheduling\n",
    "    # For right now we are just doing everything in serial.\n",
    "    # the challenge with multicore is efficient scheduling while\n",
    "    # maintaining a small dimension for the sufficient statistics.\n",
    "    \n",
    "    mu = mu.flatten()\n",
    "    \n",
    "    #set parameters for one document (i)\n",
    "    for i in range(N):\n",
    "\n",
    "        eta=lambd[i]\n",
    "        neta = len(eta)\n",
    "        eta_long = np.insert(eta,-1,0)\n",
    "\n",
    "        doc = documents[i]\n",
    "        words = [x for x,y in doc]\n",
    "        aspect = betaindex[i]\n",
    "        beta_i = beta['beta'][aspect][:,[words]]\n",
    "\n",
    "\n",
    "        #set document specs\n",
    "        doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "        Ndoc = np.sum(doc_ct)\n",
    "\n",
    "        # initial values\n",
    "        beta_tuple = beta_i.reshape(K,beta_i.shape[2])\n",
    "        theta = softmax(eta_long)\n",
    "        phi = softmax_weights(eta_long, beta_tuple)\n",
    "        # optimize variational posterior\n",
    "        result = optimize.fmin_bfgs(lhood,x0=eta,\n",
    "                           args=(mu, siginv, Ndoc, doc_ct, eta_long, beta_tuple, phi, theta, neta),\n",
    "                           fprime=grad)\n",
    "        #solve hpb \n",
    "        doc_results = hpb(result, doc_ct=doc_ct, mu=mu, siginv=siginv, beta_tuple=beta_tuple, sigmaentropy=sigmaentropy)\n",
    "        \n",
    "        # update sufficient statistics\n",
    "        #sigma_ss = sigma_ss + doc_results.eta.nu\n",
    "        #beta_ss[[aspect]][,words] = doc_results.phis + beta_ss[[aspect]][,words]\n",
    "        #bound[i] = doc_results.bound\n",
    "        #lambd[[i]] = doc_results.eta.lambd\n",
    "        \n",
    "        #4) Combine and Return Sufficient Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "8e71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = 'blog'\n",
    "content = 'rating'\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400a1b1",
   "metadata": {},
   "source": [
    "# Setting control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a16b2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopMatrix(x, data=None):\n",
    "    return(data.loc[:,x])\n",
    "\n",
    "xmat = makeTopMatrix(content, data)\n",
    "yvar = xmat.astype('category')\n",
    "yvarlevels = set(yvar)\n",
    "betaindex = yvar.cat.codes\n",
    "A = len(set(betaindex))\n",
    "\n",
    "interactions = True #settings.kappa\n",
    "verbose = True\n",
    "\n",
    "init_type = \"Random\" #settings.init\n",
    "ngroups = 1 #settings.ngroups\n",
    "max_em_its = 15 #settings.convergence\n",
    "emtol = 1e-5 #settings.convergence\n",
    "\n",
    "#gamma_prior=(\"Pooled\",\"L1\") # settings.gamma.prior\n",
    "#sigma_prior=0 #settings.sigma.prior\n",
    "#kappa_prior=(\"L1\",\"Jeffreys\") # settings.kappa.prior\n",
    "\n",
    "#Initialize parameters\n",
    "\n",
    "settings = {\n",
    "    'dim':{\n",
    "        'K': num_topics, #number of topics\n",
    "        'V' : len(dictionary), #number of words\n",
    "        'A' : A, #dimension of topical content\n",
    "        'N' : len(documents),\n",
    "    },\n",
    "    'kappa':{\n",
    "        'interactions':True,\n",
    "        'fixedintercept': True,\n",
    "        'contrats': False,\n",
    "        'mstep': {'tol':0.01, 'maxit':5}},\n",
    "    'tau':{\n",
    "        'mode': np.nan,\n",
    "        'tol': 1e-5,\n",
    "        'enet':1,\n",
    "        'nlambda':250,\n",
    "        'lambda.min.ratio':.001,\n",
    "        'ic.k':2,\n",
    "        'maxit':1e4},\n",
    "    'init':{\n",
    "        'mode':init_type, \n",
    "        'nits':20,\n",
    "        'burnin':25,\n",
    "        'alpha':50/num_topics,\n",
    "        'eta':.01,\n",
    "        's':.05,\n",
    "        'p':3000},\n",
    "    'convergence':{\n",
    "        'max.em.its':max_em_its,\n",
    "        'em.converge.thresh':emtol,\n",
    "        'allow.neg.change':True,},\n",
    "    'covariates':{\n",
    "        'X':xmat,\n",
    "        'betaindex':betaindex,\n",
    "        'yvarlevels':yvarlevels,\n",
    "        'formula': prevalence,},\n",
    "    'gamma':{\n",
    "        'mode':np.nan,\n",
    "        'prior':np.nan,\n",
    "        'enet':1, \n",
    "        'ic.k':2,\n",
    "        'maxits':1000,},\n",
    "    'sigma':{\n",
    "        #'prior':sigma_prior,\n",
    "        'ngroups':ngroups,},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "11ab119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stm_control(documents, vocab, settings, model=None):\n",
    "    \n",
    "    ##########\n",
    "    #Step 1: Initialize Parameters\n",
    "    ##########\n",
    "    \n",
    "    #ngroups = settings$ngroups\n",
    "    \n",
    "    if model == None:\n",
    "        print('Call init_stm()')\n",
    "        model = init_stm(documents, settings) #initialize\n",
    "    else: \n",
    "        model = model\n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    ############\n",
    "    #Step 2: Run EM\n",
    "    ############\n",
    "    \n",
    "    t1 = time.process_time()\n",
    "\n",
    "    #run the model\n",
    "    e_step(documents, mu, sigma, lambd, beta)\n",
    "        \n",
    "    print(\"Completed E-Step ({} seconds). \\n\".format(math.floor((time.process_time()-t1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "3ca4ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call init_stm()\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -420732706.572294\n",
      "         Iterations: 5\n",
      "         Function evaluations: 14\n",
      "         Gradient evaluations: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/mqt0qs7x0njf2_k3_mr5dcg80000gn/T/ipykernel_24610/2632759578.py:7: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  beta_temp = (np.sqrt(doc_ct)[:,None] / np.sum(beta_temp, axis=0)[:,None]) * beta_temp.T # with shape (VxK)\n",
      "/var/folders/mj/mqt0qs7x0njf2_k3_mr5dcg80000gn/T/ipykernel_24610/2632759578.py:7: RuntimeWarning: invalid value encountered in multiply\n",
      "  beta_temp = (np.sqrt(doc_ct)[:,None] / np.sum(beta_temp, axis=0)[:,None]) * beta_temp.T # with shape (VxK)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,127) (9,9) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [218]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#e-step iteration for all N documents: 52 seconds \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# compared to R (c++) implementation for the first e-step iteration: 45 seconds ! \u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mstm_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [177]\u001b[0m, in \u001b[0;36mstm_control\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     37\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m#run the model\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43me_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted E-Step (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(math\u001b[38;5;241m.\u001b[39mfloor((time\u001b[38;5;241m.\u001b[39mprocess_time()\u001b[38;5;241m-\u001b[39mt1))))\n",
      "Input \u001b[0;32mIn [159]\u001b[0m, in \u001b[0;36me_step\u001b[0;34m(documents, mu, sigma, lambd, beta)\u001b[0m\n\u001b[1;32m     51\u001b[0m result \u001b[38;5;241m=\u001b[39m optimize\u001b[38;5;241m.\u001b[39mfmin_bfgs(lhood,x0\u001b[38;5;241m=\u001b[39meta,\n\u001b[1;32m     52\u001b[0m                    args\u001b[38;5;241m=\u001b[39m(mu, siginv, Ndoc, doc_ct, eta_long, beta_tuple, phi, theta, neta),\n\u001b[1;32m     53\u001b[0m                    fprime\u001b[38;5;241m=\u001b[39mgrad)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m#solve hpb \u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m doc_results \u001b[38;5;241m=\u001b[39m \u001b[43mhpb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_ct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msiginv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msiginv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_tuple\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmaentropy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msigmaentropy\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [217]\u001b[0m, in \u001b[0;36mhpb\u001b[0;34m(eta, doc_ct, mu, siginv, beta_tuple, sigmaentropy)\u001b[0m\n\u001b[1;32m     39\u001b[0m diff \u001b[38;5;241m=\u001b[39m eta\u001b[38;5;241m-\u001b[39mmu\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m############## generate the bound and make it a scalar ##################\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m bound \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbeta_temp\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc_ct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdetTerm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdiff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43msiginv\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdiff\u001b[49m \u001b[38;5;241m-\u001b[39m sigmaentropy\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,127) (9,9) "
     ]
    }
   ],
   "source": [
    "#e-step iteration for all N documents: 52 seconds \n",
    "# compared to R (c++) implementation for the first e-step iteration: 45 seconds ! \n",
    "\n",
    "stm_control(documents, vocab, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afd20d",
   "metadata": {},
   "source": [
    "From **A Model of Text for Experimentation in the Social Sciences (Roberts et al.)**:\n",
    "- $\\beta_k$ is a V-dimensional probability mass function that controls the frequency according to which terms are generated from that topic.\n",
    "\n",
    "From **The Structural Topic Model and Applied Social Science (Roberts et al.)**:\n",
    "- 'For the nonconjugate logistic normal variables in the E-step we use a Laplace approximation.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914f3b",
   "metadata": {},
   "source": [
    "## E-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1258d49",
   "metadata": {},
   "source": [
    "### Likelihood Function (for the nonconjugate variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9a08a",
   "metadata": {},
   "source": [
    "$f(\\hat{\\eta}_{d}) \\propto  - \\frac{1}{2} (\\eta_d-\\mu_d)^T \\sum^{-1}(\\eta_d-\\mu_d)+\\big(\\sum_v c_{d,v} log\\sum_k \\beta_{k,v} e^{\\eta_{d,k}}- W_d log \\sum_k e^{\\eta_{d,k}}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a577fc2",
   "metadata": {},
   "source": [
    "### Gradient of the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51899b26",
   "metadata": {},
   "source": [
    "$\\nabla f(\\eta_d)_k = (\\sum c_{d,v} \\langle \\phi_{d,v,k} \\rangle) - W_d\\theta_{d,k}-\\big(\\sum^{-1}(\\eta_d-\\mu_d)\\big)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9264fc4",
   "metadata": {},
   "source": [
    "with $\\theta_{d,k} = \\frac{exp(\\eta)}{\\sum exp(\\eta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888695d",
   "metadata": {},
   "source": [
    "and $\\langle\\phi_{d,k,v}\\rangle = \\frac{exp(\\eta_{d,k}) \\beta_{d,v,k}}{\\sum_k exp(\\eta_{d,k}) \\beta_{d,v,k}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2480c9c",
   "metadata": {},
   "source": [
    "### Variational posterior: \n",
    "$q(\\eta_d) \\sim N(\\hat{\\eta}_d, -\\nabla^2 f(\\hat{\\eta}_d)^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0336fe91",
   "metadata": {},
   "source": [
    "# Solve for Hessian/Phi/Bound returning the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "260477fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpb(eta, doc_ct, mu,siginv, beta_tuple,sigmaentropy):\n",
    "    # copy to mess with \n",
    "    beta_temp = beta_tuple\n",
    "    #column-wise multiplication of beta and expeta\n",
    "    beta_temp = beta_temp*eta_long[:,None]\n",
    "    \n",
    "    beta_temp = (np.sqrt(doc_ct)[:,None] / np.sum(beta_temp, axis=0)[:,None]) * beta_temp.T # with shape (VxK)\n",
    "    hess = beta_temp.T@beta_temp-np.sum(doc_ct)*(theta*theta.T) # hessian with shape KxK\n",
    "    #we don't need beta_temp any more so we turn it into phi \n",
    "    beta_temp = beta_temp.T * np.sqrt(doc_ct) # should equal phi ?! \n",
    "    \n",
    "    np.fill_diagonal(hess, np.diag(hess)-np.sum(beta_temp, axis=1)-np.sum(doc_ct)*theta) #altered diagonal of h\n",
    "    \n",
    "    # drop last row and columns\n",
    "    hess = np.delete(hess,eta.size,0)\n",
    "    hess = np.delete(hess,eta.size,1)\n",
    "    hess = hess + siginv # at this point, the hessian is complete\n",
    "\n",
    "    # Invert h via cholesky decomposition \n",
    "    #np.linalg.cholesky(h)\n",
    "    # error -> not properly converged: make the matrix positive definite\n",
    "    \n",
    "    dvec = hess.diagonal()\n",
    "    magnitudes = sum(abs(hess), 1) - abs(dvec)\n",
    "    # cholesky decomposition works only for symmetric and positive definite matrices\n",
    "    dvec = np.where(dvec < magnitudes, magnitudes, dvec)\n",
    "    # A Hermitian diagonally dominant matrix A with real non-negative diagonal entries is positive semidefinite. \n",
    "    np.fill_diagonal(hess, dvec)\n",
    "    #that was sufficient to ensure positive definiteness so no we can do cholesky \n",
    "    nu = np.linalg.cholesky(hess)\n",
    "\n",
    "    #compute 1/2 the determinant from the cholesky decomposition\n",
    "    detTerm = -np.sum(np.log(nu.diagonal()))\n",
    "\n",
    "    #Finish constructing nu\n",
    "    nu = np.linalg.inv(np.triu(nu))\n",
    "    nu = nu@nu.T\n",
    "    # precompute the difference since we use it twice\n",
    "    diff = eta-mu\n",
    "    ############## generate the bound and make it a scalar ##################\n",
    "    bound = np.log(theta[:,None]*beta_temp)*doc_ct + detTerm - .5*diff.T*siginv*diff - sigmaentropy \n",
    "    ###################### return values as dictionary ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "96973e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define constant vals \n",
    "a = random.gamma(.1,1, 100*10).reshape(100,10)\n",
    "a = (a / a.sum(axis=1)[:,None]) #beta\n",
    "b = np.zeros((1, 10)) #eta\n",
    "c = np.arange(1,101,1) #doc_ct\n",
    "t = softmax(b) #theta\n",
    "sigma = np.diag(np.repeat(20,(10-1)))\n",
    "# 2) Precalculate common components\n",
    "sigobj = np.linalg.cholesky(sigma)\n",
    "sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "\n",
    "#operation\n",
    "ab = (a * b)+1 # +1 to avoid zero-divide\n",
    "ab_temp = ab*(np.sqrt(c)[:,None]/np.sum(ab,axis=0)) # with shape (VxK)\n",
    "h = (ab_temp.T@ab_temp)-(sum(c)*(t*t.T)) # hessian with shape KxK\n",
    "ab = ab.T * np.sqrt(c) #should be equal to phi ??? with shape KxV\n",
    "np.fill_diagonal(h, np.diag(h)-np.sum(ab, axis=1)-np.sum(c)*t) #altered diagonal of h\n",
    "# drop last row and columns\n",
    "h = np.delete(h,9,0)\n",
    "h = np.delete(h,9,1)\n",
    "h = h + siginv # at this point, the hessian is complete\n",
    "\n",
    "# Invert h via cholesky decomposition \n",
    "#np.linalg.cholesky(h)\n",
    "# error -> not properly converged: make the matrix positive definite\n",
    "dvec = h.diagonal()\n",
    "magnitudes = sum(abs(h), 1) - abs(dvec)\n",
    "# cholesky decomposition works only for symmetric and positive definite matrices\n",
    "dvec = np.where(dvec < magnitudes, magnitudes, dvec)\n",
    "# A Hermitian diagonally dominant matrix A with real non-negative diagonal entries is positive semidefinite. \n",
    "np.fill_diagonal(h, dvec)\n",
    "#that was sufficient to ensure positive definiteness so no we can do cholesky \n",
    "nu = np.linalg.cholesky(h)\n",
    "\n",
    "#compute 1/2 the determinant from the cholesky decomposition\n",
    "detTerm = -np.sum(np.log(nu.diagonal()))\n",
    "\n",
    "#Finish constructing nu\n",
    "nu = np.linalg.inv(np.triu(nu))\n",
    "nu = nu@nu.T\n",
    "# precompute the difference since we use it twice\n",
    "#diff = eta-mu\n",
    "# generate the bound and make it a scalar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f1d08",
   "metadata": {},
   "source": [
    "## M-Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d24d68",
   "metadata": {},
   "source": [
    "### Topical Content Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a4b008",
   "metadata": {},
   "source": [
    "### Topical Prevalence Model Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ee6c0",
   "metadata": {},
   "source": [
    "### Global Covariance Update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
