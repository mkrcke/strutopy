{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0e8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gensim import corpora\n",
    "from scipy import optimize\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8522ba",
   "metadata": {},
   "source": [
    "### Ingest corpus to create documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "documents = corpora.MmCorpus('corpus.mm')\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stm(documents, settings): \n",
    "      \n",
    "    K = settings['dim']['K']\n",
    "    V = settings['dim']['V']\n",
    "    A = settings['dim']['A']\n",
    "    N = settings['dim']['N']\n",
    "    \n",
    "    #Random initialization\n",
    "    mu = np.array([0]*(K-1))[:,None]\n",
    "    sigma = np.zeros(((K-1),(K-1)))\n",
    "    diag = np.diagonal(sigma, 0)\n",
    "    diag.setflags(write=True)\n",
    "    diag.fill(20)\n",
    "    beta = random.gamma(.1,1, V*K).reshape(K,V)\n",
    "    beta = (beta / beta.sum(axis=1)[:,None])\n",
    "    lambd = np.zeros((N, (K-1)))\n",
    "    \n",
    "    #turn beta into a list and assign it for each aspect\n",
    "    beta = [beta, beta] # FOR A=2\n",
    "    kappa_initialized = init_kappa(documents, K, V, A, interactions=settings['kappa']['interactions'])\n",
    "    \n",
    "    #create model object\n",
    "    model = {'mu':mu, 'sigma':sigma, 'beta': beta, 'lambda': lambd, 'kappa':kappa_initialized}\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def init_kappa(documents, K, V, A, interactions): \n",
    "    # read in documents and vocab\n",
    "    flat_documents = [item for sublist in documents for item in sublist]\n",
    "    m = []\n",
    "\n",
    "    total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "    for elem in flat_documents: \n",
    "        m.append(elem[1] / total_sum)\n",
    "\n",
    "    m = np.log(m) - np.log(np.mean(m)) #logit of m\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    aspectmod = A > 1 # if there is more than one topical content variable\n",
    "    if(aspectmod):\n",
    "        interact = interactions # allow for the choice to interact\n",
    "    else:\n",
    "        interact = FALSE\n",
    "\n",
    "    #Create the parameters object\n",
    "    parLength = K + A * aspectmod + (K*A)*interact\n",
    "\n",
    "    #create covariates. one element per item in parameter list.\n",
    "    #generation by type because its conceptually simpler\n",
    "    if not aspectmod & interact:\n",
    "        covar = {'k': np.arange(K),\n",
    "             'a': np.repeat(np.nan, parLength), #why parLength? \n",
    "             'type': np.repeat(1, K)}\n",
    "\n",
    "    if(aspectmod & interact == False):\n",
    "        covar = {'k': np.append(np.arange(K), np.repeat(np.nan, A)),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.arange(A)), \n",
    "                 'type': np.append(np.repeat(1, K), np.repeat(2, A))}      \n",
    "    if(interact):\n",
    "        covar = {'k': np.append(np.arange(K), np.append(np.repeat(np.nan, A), np.repeat(np.arange(K), A))),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.append(np.arange(A), np.repeat(np.arange(A), K))), \n",
    "                 'type': np.append(np.repeat(1, K), np.append(np.repeat(2, A),  np.repeat(3,K*A)))}\n",
    "\n",
    "    kappa = {'out': {'m':m,\n",
    "                     'params' : np.tile(np.repeat(0,V), (parLength, 1)),\n",
    "                     'covar' : covar\n",
    "                     #'kappasum':, why rolling sum?\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    return(kappa['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "c486270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_weights(x, weight):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = weight*np.exp(x - np.max(x))[:,None]\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def lhood(eta, mu, siginv, Ndoc):\n",
    "    \n",
    "    #formula\n",
    "    part1 = np.sum(doc_ct * (eta_long.max() + np.log(np.exp(eta_long - eta_long.max())@beta_tuple)))-np.sum(doc_ct)*scipy.special.logsumexp(eta)\n",
    "    part2 = .5*(eta-mu)@siginv@(eta-mu)\n",
    "    \n",
    "    out = part2 - part1\n",
    "    \n",
    "    return -out\n",
    "\n",
    "def grad(eta, mu, siginv, Ndoc):\n",
    "\n",
    "    #formula\n",
    "    part1 = np.delete(np.sum(phi * doc_ct,axis=1) - np.sum(doc_ct)*theta, neta)\n",
    "    part2 = siginv@(eta-mu)\n",
    "\n",
    "    return part2 - part1\n",
    "\n",
    "def logisticnormalcpp(\n",
    "    eta,\n",
    "    #eta_long,\n",
    "    mu,\n",
    "    siginv,\n",
    "    beta_i,\n",
    "    phi,\n",
    "    doc_ct,\n",
    "    Ndoc):\n",
    "    \n",
    "    # initial values\n",
    "    #eta_long = np.insert(eta,-1,0)\n",
    "    #beta = beta_i.reshape(10,beta_i.shape[2])\n",
    "    #neta = len(eta)\n",
    "    #theta = softmax(eta_long)\n",
    "    #phi = softmax_weights(eta_long, beta)\n",
    "    \n",
    "    optim_out = optimize.fmin_bfgs(lhood,\n",
    "                                   x0=eta,\n",
    "                                   args=(beta_tuple, mu, siginv, Ndoc),\n",
    "                                   fprime=grad)\n",
    "    return optim_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d8e2d779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(documents, mu, sigma, lambd, beta):\n",
    "    #quickly define useful constants\n",
    "    V = beta['beta'][0].shape[1] # ncol\n",
    "    K = beta['beta'][0].shape[0] # nrow\n",
    "    N = len(documents)\n",
    "    A = len(beta['beta'])\n",
    "    \n",
    "    # 1) Initialize Sufficient Statistics \n",
    "    sigma_ss = np.zeros(((K-1),(K-1)))\n",
    "    if A == 2: \n",
    "        beta_ss = [np.zeros((K,V)), np.zeros((K,V))]\n",
    "    else:\n",
    "        print('Error: Only two metadata columns allowed.')\n",
    "    bound = np.repeat(0,N)\n",
    "    #lambd = np.repeat(0,N)\n",
    "    \n",
    "    # 2) Precalculate common components\n",
    "    sigobj = np.linalg.cholesky(sigma)\n",
    "    sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "    siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "    \n",
    "    # 3) Document Scheduling\n",
    "    # For right now we are just doing everything in serial.\n",
    "    # the challenge with multicore is efficient scheduling while\n",
    "    # maintaining a small dimension for the sufficient statistics.\n",
    "    \n",
    "    mu = mu.flatten()\n",
    "\n",
    "\n",
    "        #set parameters for one document (i)\n",
    "    for i in range(20):\n",
    "\n",
    "        eta=lambd[i]\n",
    "        neta = len(eta)\n",
    "        eta_long = np.insert(eta,-1,0)\n",
    "\n",
    "        doc = documents[i]\n",
    "        words = [x for x,y in doc]\n",
    "        aspect = betaindex[i]\n",
    "        beta_i = beta['beta'][aspect][:,[words]]\n",
    "\n",
    "\n",
    "        #set document specs\n",
    "        doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "        Ndoc = np.sum(doc_ct)\n",
    "\n",
    "        # initial values\n",
    "        beta_tuple = beta_i.reshape(10,beta_i.shape[2])\n",
    "        theta = softmax(eta_long)\n",
    "        phi = softmax_weights(eta_long, beta_tuple)\n",
    "\n",
    "        optimize.fmin_bfgs(lhood,x0=eta,\n",
    "                           args=(mu, siginv, Ndoc),\n",
    "                           fprime=grad)\n",
    "        \n",
    "\n",
    "        # update sufficient statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "8e71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = 'blog'\n",
    "content = 'rating'\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400a1b1",
   "metadata": {},
   "source": [
    "# Setting control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "a16b2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopMatrix(x, data=None):\n",
    "    return(data.loc[:,x])\n",
    "\n",
    "xmat = makeTopMatrix(content, data)\n",
    "yvar = xmat.astype('category')\n",
    "yvarlevels = set(yvar)\n",
    "betaindex = yvar.cat.codes\n",
    "A = len(set(betaindex))\n",
    "\n",
    "interactions = True #settings.kappa\n",
    "verbose = True\n",
    "\n",
    "init_type = \"Random\" #settings.init\n",
    "ngroups = 1 #settings.ngroups\n",
    "max_em_its = 15 #settings.convergence\n",
    "emtol = 1e-5 #settings.convergence\n",
    "\n",
    "#gamma_prior=(\"Pooled\",\"L1\") # settings.gamma.prior\n",
    "#sigma_prior=0 #settings.sigma.prior\n",
    "#kappa_prior=(\"L1\",\"Jeffreys\") # settings.kappa.prior\n",
    "\n",
    "#Initialize parameters\n",
    "\n",
    "settings = {\n",
    "    'dim':{\n",
    "        'K': num_topics, #number of topics\n",
    "        'V' : len(dictionary), #number of words\n",
    "        'A' : A, #dimension of topical content\n",
    "        'N' : len(documents),\n",
    "    },\n",
    "    'kappa':{\n",
    "        'interactions':True,\n",
    "        'fixedintercept': True,\n",
    "        'contrats': False,\n",
    "        'mstep': {'tol':0.01, 'maxit':5}},\n",
    "    'tau':{\n",
    "        'mode': np.nan,\n",
    "        'tol': 1e-5,\n",
    "        'enet':1,\n",
    "        'nlambda':250,\n",
    "        'lambda.min.ratio':.001,\n",
    "        'ic.k':2,\n",
    "        'maxit':1e4},\n",
    "    'init':{\n",
    "        'mode':init_type, \n",
    "        'nits':20,\n",
    "        'burnin':25,\n",
    "        'alpha':50/num_topics,\n",
    "        'eta':.01,\n",
    "        's':.05,\n",
    "        'p':3000},\n",
    "    'convergence':{\n",
    "        'max.em.its':max_em_its,\n",
    "        'em.converge.thresh':emtol,\n",
    "        'allow.neg.change':True,},\n",
    "    'covariates':{\n",
    "        'X':xmat,\n",
    "        'betaindex':betaindex,\n",
    "        'yvarlevels':yvarlevels,\n",
    "        'formula': prevalence,},\n",
    "    'gamma':{\n",
    "        'mode':np.nan,\n",
    "        'prior':np.nan,\n",
    "        'enet':1, \n",
    "        'ic.k':2,\n",
    "        'maxits':1000,},\n",
    "    'sigma':{\n",
    "        #'prior':sigma_prior,\n",
    "        'ngroups':ngroups,},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "11ab119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stm_control(documents, vocab, settings, model=None):\n",
    "    \n",
    "    ##########\n",
    "    #Step 1: Initialize Parameters\n",
    "    ##########\n",
    "    \n",
    "    #ngroups = settings$ngroups\n",
    "    \n",
    "    if model == None:\n",
    "        print('Call init_stm()')\n",
    "        model = init_stm(documents, settings) #initialize\n",
    "    else: \n",
    "        model = model\n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    ############\n",
    "    #Step 2: Run EM\n",
    "    ############\n",
    "    \n",
    "    t1 = time.process_time()\n",
    "\n",
    "    #run the model\n",
    "    e_step(documents, mu, sigma, lambd, beta)\n",
    "        \n",
    "    print(\"Completed E-Step ({} seconds). \\n\".format(math.floor((time.process_time()-t1))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "3ca4ad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call init_stm()\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Completed E-Step (0 seconds). \n",
      "\n"
     ]
    }
   ],
   "source": [
    "stm_control(documents, vocab, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afd20d",
   "metadata": {},
   "source": [
    "From **A Model of Text for Experimentation in the Social Sciences (Roberts et al.)**:\n",
    "- $\\beta_k$ is a V-dimensional probability mass function that controls the frequency according to which terms are generated from that topic.\n",
    "\n",
    "From **The Structural Topic Model and Applied Social Science (Roberts et al.)**:\n",
    "- 'For the nonconjugate logistic normal variables in the E-step we use a Laplace approximation.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845496b3",
   "metadata": {},
   "source": [
    "# Let's start by assuming its one beta and we may have arbitrarily subset the number of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f400c5e0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = init_stm(documents, settings) #initialize\n",
    "\n",
    "# unpack initialized model\n",
    "\n",
    "mu = model['mu']\n",
    "sigma = model['sigma']\n",
    "lambd = model['lambda'] \n",
    "beta = {'beta': model['beta'],\n",
    "        'kappa': model['kappa']}\n",
    "\n",
    "convergence = None\n",
    "\n",
    "#discard the old object\n",
    "del model\n",
    "\n",
    "#Pull out some book keeping elements\n",
    "betaindex = settings['covariates']['betaindex']\n",
    "\n",
    "    \n",
    "#set global parameter for sigma\n",
    "sigobj = np.linalg.cholesky(sigma)\n",
    "sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "5f881363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: -25796.797538\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Warning: Desired error not necessarily achieved due to precision loss.\n",
      "         Current function value: -2854.016890\n",
      "         Iterations: 0\n",
      "         Function evaluations: 99\n",
      "         Gradient evaluations: 87\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -49075.520507\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -19738.178390\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -43729.044260\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -88292.654804\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -108427.983189\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -30663.973701\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -51401.557104\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18976.825992\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -11344.245765\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -57783.831225\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -17416.588476\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -36048.656327\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -56630.607726\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18512.837552\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -16855.520269\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -40143.852574\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -18722.331796\n",
      "         Iterations: 2\n",
      "         Function evaluations: 6\n",
      "         Gradient evaluations: 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: -21626.089291\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#set parameters for one document (i)\n",
    "for i in range(20):\n",
    "    \n",
    "    eta=lambd[i]\n",
    "    neta = len(eta)\n",
    "    eta_long = np.insert(eta,-1,0)\n",
    "    mu = mu_i.flatten()\n",
    "    doc = documents[i]\n",
    "    words = [x for x,y in doc]\n",
    "    aspect = betaindex[i]\n",
    "    beta_i = beta['beta'][aspect][:,[words]]\n",
    "\n",
    "\n",
    "    #set document specs\n",
    "    doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "    Ndoc = np.sum(doc_ct)\n",
    "\n",
    "    # initial values\n",
    "    beta_tuple = beta_i.reshape(10,beta_i.shape[2])\n",
    "    theta = softmax(eta_long)\n",
    "    phi = softmax_weights(eta_long, beta_tuple)\n",
    "\n",
    "    optimize.fmin_bfgs(lhood,x0=eta,\n",
    "                       args=(mu, siginv, Ndoc),\n",
    "                       fprime=grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914f3b",
   "metadata": {},
   "source": [
    "## E-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1258d49",
   "metadata": {},
   "source": [
    "### Likelihood Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9a08a",
   "metadata": {},
   "source": [
    "$f(\\hat{\\eta_{d}}) \\propto  - \\frac{1}{2} (\\eta_d-\\mu_d)^T \\sum^{-1}(\\eta_d-\\mu_d)+\\big(\\sum_v c_{d,v} log\\sum_k \\beta_{k,v} e^{\\eta_{d,k}}- W_d log \\sum_k e^{\\eta_{d,k}}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a577fc2",
   "metadata": {},
   "source": [
    "### Gradient of the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51899b26",
   "metadata": {},
   "source": [
    "$\\nabla f(\\eta_d)_k = (\\sum c_{d,v} \\langle \\phi_{d,v,k} \\rangle) - W_d\\theta_{d,k}-\\big(\\sum^{-1}(\\eta_d-\\mu_d)\\big)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9264fc4",
   "metadata": {},
   "source": [
    "with $\\theta_{d,k} = \\frac{exp(\\eta)}{\\sum exp(\\eta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888695d",
   "metadata": {},
   "source": [
    "and $\\langle\\phi_{d,k,v}\\rangle = \\frac{exp(\\eta_{d,k}) \\beta_{d,v,k}}{\\sum_k exp(\\eta_{d,k}) \\beta_{d,v,k}} $"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
