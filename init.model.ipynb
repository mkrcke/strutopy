{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd0e8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gensim import corpora\n",
    "from scipy import optimize\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3fb9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8522ba",
   "metadata": {},
   "source": [
    "### Ingest corpus to create documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ec3e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "documents = corpora.MmCorpus('corpus.mm')\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e6c49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d46df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stm(documents, settings): \n",
    "      \n",
    "    K = settings['dim']['K']\n",
    "    V = settings['dim']['V']\n",
    "    A = settings['dim']['A']\n",
    "    N = settings['dim']['N']\n",
    "    \n",
    "    #Random initialization\n",
    "    mu = np.array([0]*(K-1))[:,None]\n",
    "    sigma = np.zeros(((K-1),(K-1)))\n",
    "    diag = np.diagonal(sigma, 0)\n",
    "    diag.setflags(write=True)\n",
    "    diag.fill(20)\n",
    "    beta = random.gamma(.1,1, V*K).reshape(K,V)\n",
    "    beta = (beta / beta.sum(axis=1)[:,None])\n",
    "    lambd = np.zeros((N, (K-1)))\n",
    "    \n",
    "    #turn beta into a list and assign it for each aspect\n",
    "    beta = [beta, beta] # FOR A=2\n",
    "    kappa_initialized = init_kappa(documents, K, V, A, interactions=settings['kappa']['interactions'])\n",
    "    \n",
    "    #create model object\n",
    "    model = {'mu':mu, 'sigma':sigma, 'beta': beta, 'lambda': lambd, 'kappa':kappa_initialized}\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def init_kappa(documents, K, V, A, interactions): \n",
    "    # read in documents and vocab\n",
    "    flat_documents = [item for sublist in documents for item in sublist]\n",
    "    m = []\n",
    "\n",
    "    total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "    for elem in flat_documents: \n",
    "        m.append(elem[1] / total_sum)\n",
    "\n",
    "    m = np.log(m) - np.log(np.mean(m)) #logit of m\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    aspectmod = A > 1 # if there is more than one topical content variable\n",
    "    if(aspectmod):\n",
    "        interact = interactions # allow for the choice to interact\n",
    "    else:\n",
    "        interact = FALSE\n",
    "\n",
    "    #Create the parameters object\n",
    "    parLength = K + A * aspectmod + (K*A)*interact\n",
    "\n",
    "    #create covariates. one element per item in parameter list.\n",
    "    #generation by type because its conceptually simpler\n",
    "    if not aspectmod & interact:\n",
    "        covar = {'k': np.arange(K),\n",
    "             'a': np.repeat(np.nan, parLength), #why parLength? \n",
    "             'type': np.repeat(1, K)}\n",
    "\n",
    "    if(aspectmod & interact == False):\n",
    "        covar = {'k': np.append(np.arange(K), np.repeat(np.nan, A)),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.arange(A)), \n",
    "                 'type': np.append(np.repeat(1, K), np.repeat(2, A))}      \n",
    "    if(interact):\n",
    "        covar = {'k': np.append(np.arange(K), np.append(np.repeat(np.nan, A), np.repeat(np.arange(K), A))),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.append(np.arange(A), np.repeat(np.arange(A), K))), \n",
    "                 'type': np.append(np.repeat(1, K), np.append(np.repeat(2, A),  np.repeat(3,K*A)))}\n",
    "\n",
    "    kappa = {'out': {'m':m,\n",
    "                     'params' : np.tile(np.repeat(0,V), (parLength, 1)),\n",
    "                     'covar' : covar\n",
    "                     #'kappasum':, why rolling sum?\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    return(kappa['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cb66b",
   "metadata": {},
   "source": [
    "### Make Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "03643f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = 'blog'\n",
    "content = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "61fb823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopMatrix(x, data=None):\n",
    "    return(data.loc[:,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1af31a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmat = makeTopMatrix(content, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "46bed10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yvar = makeTopMatrix(content, data)\n",
    "yvar = yvar.astype('category')\n",
    "yvarlevels = set(yvar)\n",
    "betaindex = yvar.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "8e71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = len(set(betaindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400a1b1",
   "metadata": {},
   "source": [
    "# Setting control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a16b2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 10 #settings.dim\n",
    "V = len(dictionary) #settings.dim\n",
    "N = len(documents) #settings.dim\n",
    "\n",
    "interactions = True #settings.kappa\n",
    "verbose = True\n",
    "\n",
    "init_type = \"Random\" #settings.init\n",
    "ngroups = 1 #settings.ngroups\n",
    "max_em_its = 15 #settings.convergence\n",
    "emtol = 1e-5 #settings.convergence\n",
    "\n",
    "#gamma_prior=(\"Pooled\",\"L1\") # settings.gamma.prior\n",
    "#sigma_prior=0 #settings.sigma.prior\n",
    "#kappa_prior=(\"L1\",\"Jeffreys\") # settings.kappa.prior\n",
    "\n",
    "#Initialize parameters\n",
    "\n",
    "settings = {\n",
    "    'dim':{\n",
    "        'K': K, #number of topics\n",
    "        'V' :V, #number of words\n",
    "        'A' : A, #dimension of topical content\n",
    "        'N' : N,\n",
    "        'wcounts':V\n",
    "    },\n",
    "    'kappa':{\n",
    "        'interactions':True,\n",
    "        'fixedintercept': True,\n",
    "        'contrats': False,\n",
    "        'mstep': {'tol':0.01, 'maxit':5}},\n",
    "    'tau':{\n",
    "        'mode': np.nan,\n",
    "        'tol': 1e-5,\n",
    "        'enet':1,\n",
    "        'nlambda':250,\n",
    "        'lambda.min.ratio':.001,\n",
    "        'ic.k':2,\n",
    "        'maxit':1e4},\n",
    "    'init':{\n",
    "        'mode':init_type, \n",
    "        'nits':20,\n",
    "        'burnin':25,\n",
    "        'alpha':50/K,\n",
    "        'eta':.01,\n",
    "        's':.05,\n",
    "        'p':3000},\n",
    "    'convergence':{\n",
    "        'max.em.its':max_em_its,\n",
    "        'em.converge.thresh':emtol,\n",
    "        'allow.neg.change':True,},\n",
    "    'covariates':{\n",
    "        'X':xmat,\n",
    "        'betaindex':betaindex,\n",
    "        'yvarlevels':yvarlevels,\n",
    "        'formula': prevalence,},\n",
    "    'gamma':{\n",
    "        'mode':np.nan,\n",
    "        'prior':np.nan,\n",
    "        'enet':1, \n",
    "        'ic.k':2,\n",
    "        'maxits':1000,},\n",
    "    'sigma':{\n",
    "        #'prior':sigma_prior,\n",
    "        'ngroups':ngroups,},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "11ab119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stm_control(documents, vocab, settings, model=None):\n",
    "    \n",
    "    ##########\n",
    "    #Step 1: Initialize Parameters\n",
    "    ##########\n",
    "    \n",
    "    #ngroups = settings$ngroups\n",
    "    \n",
    "    if model == None:\n",
    "        print('Call init_stm()')\n",
    "        model = init_stm(documents, settings) #initialize\n",
    "    else: \n",
    "        model = model\n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    ntokens = settings['dim']['wcounts']\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    stopits = True\n",
    "    \n",
    "    ############\n",
    "    #Step 2: Run EM\n",
    "    ############\n",
    "    \n",
    "    while stopits == True:\n",
    "        #####\n",
    "        # Non-Blocked Updates\n",
    "        #####\n",
    "        t1 = time.process_time()\n",
    "\n",
    "        #run the model\n",
    "        suffstats = e_step(documents, mu, sigma, lambd, beta)\n",
    "        \n",
    "        print(\"Completed E-Step ({} seconds). \\n\".format(math.floor((time.process_time()-t1))))\n",
    "\n",
    "\n",
    "        #unpack variables \n",
    "        \n",
    "        #t1 = process_time()\n",
    "        #sigma_ss = suffstats['sigma']\n",
    "        #lambd <- suffstats['lambd']\n",
    "        #beta_ss <- suffstats['beta']\n",
    "        #bound_ss <- suffstats['bound']\n",
    "        #do the m-step\n",
    "        #mu = opt_mu(lambd=lambd,\n",
    "        #            mode=settings['gamma']['mode'],\n",
    "        #            covar=settings['covariates']['X'],\n",
    "        #            enet=settings['gamma']['enet'],ic.k=settings$gamma$ic.k,\n",
    "        #            maxits=settings['gamma']['maxits'])\n",
    "        #sigma = opt_sigma(nu=sigma_ss, lambd=lambd,\n",
    "        #                     mu=mu['mu'], sigprior=settings['sigma']['prior'])\n",
    "        #beta = opt_beta(beta_ss, beta['kappa'], settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "dfefc4d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call init_stm()\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'e_step' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstm_control\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36mstm_control\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     43\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mprocess_time()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#run the model\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m suffstats \u001b[38;5;241m=\u001b[39m \u001b[43me_step\u001b[49m(documents, mu, sigma, lambd, beta)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompleted E-Step (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m seconds). \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(math\u001b[38;5;241m.\u001b[39mfloor((time\u001b[38;5;241m.\u001b[39mprocess_time()\u001b[38;5;241m-\u001b[39mt1))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'e_step' is not defined"
     ]
    }
   ],
   "source": [
    "stm_control(documents, vocab, settings, model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afd20d",
   "metadata": {},
   "source": [
    "From **A Model of Text for Experimentation in the Social Sciences (Roberts et al.)**:\n",
    "- $\\beta_k$ is a V-dimensional probability mass function that controls the frequency according to which terms are generated from that topic.\n",
    "\n",
    "From **The Structural Topic Model and Applied Social Science (Roberts et al.)**:\n",
    "- 'For the nonconjugate logistic normal variables in the E-step we use a Laplace approximation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "162859f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(documents, mu, sigma, lambd, beta):\n",
    "    #quickly define useful constants\n",
    "    V = beta['beta'][0].shape[1] # ncol\n",
    "    K = beta['beta'][0].shape[0] # nrow\n",
    "    N = len(documents)\n",
    "    A = len(beta['beta'])\n",
    "    \n",
    "    # 1) Initialize Sufficient Statistics \n",
    "    sigma_ss = np.zeros(((K-1),(K-1)))\n",
    "    if A == 2: \n",
    "        beta_ss = [np.zeros((K,V)), np.zeros((K,V))]\n",
    "    else:\n",
    "        print('Error: Only two metadata columns allowed.')\n",
    "    bound = np.repeat(0,N)\n",
    "    #lambd = np.repeat(0,N)\n",
    "    \n",
    "    # 2) Precalculate common components\n",
    "    sigobj = np.linalg.cholesky(sigma)\n",
    "    sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "    siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "    \n",
    "    # 3) Document Scheduling\n",
    "    # For right now we are just doing everything in serial.\n",
    "    # the challenge with multicore is efficient scheduling while\n",
    "    # maintaining a small dimension for the sufficient statistics.\n",
    "    for i in range(N): \n",
    "        # update components\n",
    "        doc = documents[i]\n",
    "        words = [x for x,y in doc]\n",
    "        doc_ct = [y for x,y in doc]\n",
    "        aspect = betaindex[i]\n",
    "        init = lambd[i]\n",
    "        #if update_mu: mu_i = mu[i]\n",
    "        beta_i = beta['beta'][aspect][:,[words]]\n",
    "        #doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "        Ndoc = np.sum(doc_ct)\n",
    "\n",
    "        # initial values\n",
    "        eta = init\n",
    "        mu = mu.flatten()\n",
    "        eta_long = np.insert(eta,-1,0)\n",
    "        beta = beta_i.reshape(10,beta_i.shape[2])\n",
    "        neta = len(eta)\n",
    "        theta = softmax(eta_long)\n",
    "        phi = softmax_weights(eta_long, beta)\n",
    "        \n",
    "        #infer the document with the logistic normal distribution\n",
    "        doc_results = logisticnormalcpp(\n",
    "            eta=eta,\n",
    "            mu=mu,\n",
    "            siginv=siginv,\n",
    "            beta_i=beta, \n",
    "            doc_ct=doc_ct,\n",
    "            Ndoc=Ndoc)\n",
    "\n",
    "        # update sufficient statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845496b3",
   "metadata": {},
   "source": [
    "# Let's start by assuming its one beta and we may have arbitrarily subset the number of docs.\n",
    "```python\n",
    "> def estep(documents=documents, beta_index=betaindex,\n",
    "          update_mu=True,\n",
    "          beta['beta'], lambd, mu, sigma,\n",
    "          verbose):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f400c5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = init_stm(documents, settings) #initialize\n",
    "\n",
    "\n",
    "# unpack initialized model\n",
    "\n",
    "mu = model['mu']\n",
    "sigma = model['sigma']\n",
    "lambd = model['lambda'] \n",
    "beta = {'beta': model['beta'],\n",
    "        'kappa': model['kappa']}\n",
    "\n",
    "convergence = None\n",
    "\n",
    "#discard the old object\n",
    "del model\n",
    "\n",
    "betaindex = settings['covariates']['betaindex']\n",
    "\n",
    "#Pull out some book keeping elements\n",
    "ntokens = settings['dim']['wcounts']\n",
    "betaindex = settings['covariates']['betaindex']\n",
    "stopits = True\n",
    "    \n",
    "#set global parameter for sigma\n",
    "sigobj = np.linalg.cholesky(sigma)\n",
    "sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "\n",
    "#set parameters for one document (i)\n",
    "i = 0\n",
    "doc = documents[i]\n",
    "words = [x for x,y in doc]\n",
    "aspect = betaindex[i]\n",
    "init = lambd[i]\n",
    "beta_i = beta['beta'][aspect][:,[words]]\n",
    "eta=init\n",
    "mu_i=mu\n",
    "siginv = siginv\n",
    "doc=doc\n",
    "\n",
    "#set document specs\n",
    "doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "Ndoc = np.sum(doc_ct)\n",
    "\n",
    "# initial values\n",
    "eta_long = np.insert(eta,-1,0)\n",
    "beta = beta_i.reshape(10,beta_i.shape[2])\n",
    "neta = len(eta)\n",
    "theta = softmax(eta_long)\n",
    "phi = softmax_weights(eta_long, beta)\n",
    "mu = mu_i.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1dda45fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def logisticnormalcpp(\n",
    "    eta,\n",
    "    mu_i,\n",
    "    siginv,\n",
    "    beta_i,\n",
    "    doc_ct,\n",
    "    Ndoc):\n",
    "    \n",
    "    # initial values\n",
    "    eta_long = np.insert(eta,-1,0)\n",
    "    beta = beta_i.reshape(10,beta_i.shape[2])\n",
    "    neta = len(eta)\n",
    "    theta = softmax(eta_long)\n",
    "    phi = softmax_weights(eta_long, beta)\n",
    "    \n",
    "    optim_out = optimize.fmin_bfgs(lhood,\n",
    "                                   x0=eta,\n",
    "                                   args=(beta_i, doc_ct, mu, siginv, Ndoc, eta_long),\n",
    "                                   fprime=grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "56977f34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [87]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlogisticnormalcpp\u001b[49m\u001b[43m(\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mmu_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbeta_i\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mdoc_ct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoc_ct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msiginv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msiginv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mNdoc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNdoc\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mlogisticnormalcpp\u001b[0;34m(eta, mu_i, siginv, beta_i, doc_ct, Ndoc)\u001b[0m\n\u001b[1;32m     13\u001b[0m theta \u001b[38;5;241m=\u001b[39m softmax(eta_long)\n\u001b[1;32m     14\u001b[0m phi \u001b[38;5;241m=\u001b[39m softmax_weights(eta_long, beta)\n\u001b[0;32m---> 16\u001b[0m optim_out \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlhood\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                               \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbeta_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc_ct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msiginv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_long\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mfprime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_optimize.py:1238\u001b[0m, in \u001b[0;36mfmin_bfgs\u001b[0;34m(f, x0, fprime, args, gtol, norm, epsilon, maxiter, full_output, disp, retall, callback)\u001b[0m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;124;03mMinimize a function using the BFGS algorithm.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1229\u001b[0m \n\u001b[1;32m   1230\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgtol\u001b[39m\u001b[38;5;124m'\u001b[39m: gtol,\n\u001b[1;32m   1232\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnorm\u001b[39m\u001b[38;5;124m'\u001b[39m: norm,\n\u001b[1;32m   1233\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m: epsilon,\n\u001b[1;32m   1234\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[1;32m   1235\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter,\n\u001b[1;32m   1236\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreturn_all\u001b[39m\u001b[38;5;124m'\u001b[39m: retall}\n\u001b[0;32m-> 1238\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_bfgs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfprime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[1;32m   1241\u001b[0m     retlist \u001b[38;5;241m=\u001b[39m (res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhess_inv\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   1242\u001b[0m                res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnjev\u001b[39m\u001b[38;5;124m'\u001b[39m], res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_optimize.py:1296\u001b[0m, in \u001b[0;36m_minimize_bfgs\u001b[0;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxiter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     maxiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x0) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[0;32m-> 1296\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43m_prepare_scalar_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1297\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1299\u001b[0m f \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mfun\n\u001b[1;32m   1300\u001b[0m myfprime \u001b[38;5;241m=\u001b[39m sf\u001b[38;5;241m.\u001b[39mgrad\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_optimize.py:263\u001b[0m, in \u001b[0;36m_prepare_scalar_function\u001b[0;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001b[0m\n\u001b[1;32m    259\u001b[0m     bounds \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf, np\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# calculation reduces overall function evaluations.\u001b[39;00m\n\u001b[0;32m--> 263\u001b[0m sf \u001b[38;5;241m=\u001b[39m \u001b[43mScalarFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mfinite_diff_rel_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sf\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:158\u001b[0m, in \u001b[0;36mScalarFunction.__init__\u001b[0;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m fun_wrapped(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mx)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun_impl \u001b[38;5;241m=\u001b[39m update_fun\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# Gradient evaluation\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callable(grad):\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_fun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated:\n\u001b[0;32m--> 251\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_fun_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate_fun\u001b[39m():\n\u001b[0;32m--> 155\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf \u001b[38;5;241m=\u001b[39m \u001b[43mfun_wrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "Input \u001b[0;32mIn [84]\u001b[0m, in \u001b[0;36mlhood\u001b[0;34m(eta, eta_long, beta, doc_ct, mu, siginv, Ndoc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlhood\u001b[39m(eta, eta_long, beta, doc_ct, mu, siginv, Ndoc):\n\u001b[1;32m      2\u001b[0m     \n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#formula\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     part1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241;43m.5\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;129;43m@siginv\u001b[39;49m\u001b[38;5;241m@\u001b[39m(eta\u001b[38;5;241m-\u001b[39mmu)\n\u001b[1;32m      5\u001b[0m     part2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(doc_ct \u001b[38;5;241m*\u001b[39m (eta_long\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(np\u001b[38;5;241m.\u001b[39mexp(eta_long \u001b[38;5;241m-\u001b[39m eta_long\u001b[38;5;241m.\u001b[39mmax())\u001b[38;5;129m@beta\u001b[39m)))\u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(doc_ct)\u001b[38;5;241m*\u001b[39mscipy\u001b[38;5;241m.\u001b[39mspecial\u001b[38;5;241m.\u001b[39mlogsumexp(eta)\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Move to trash eventually\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m#part1 = np.log(expeta@beta@doc_ct) - Ndoc * np.log(np.sum(expeta)) #alternative based on c++ code\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m#part1 = np.sum(doc_ct)*np.log(np.sum(expeta@beta)) - Ndoc * np.log(np.sum(expeta)) #alternative (based on paper)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m#part2 = - .5*(eta-mu)@siginv@(eta-mu)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m#out = .5*(eta-mu)@siginv@(eta-mu) + (np.sum(doc_ct)*np.log(np.sum(expeta@beta)) - Ndoc * np.log(np.sum(expeta)))\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "logisticnormalcpp(eta=eta,\n",
    "                  mu_i=mu,\n",
    "                  beta_i=beta_i,\n",
    "                  doc_ct=doc_ct, siginv=siginv,\n",
    "                  Ndoc=Ndoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d2526",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b0c8ad02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2442f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_weights(x, weight):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = weight*np.exp(x - np.max(x))[:,None]\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914f3b",
   "metadata": {},
   "source": [
    "## E-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1258d49",
   "metadata": {},
   "source": [
    "### Likelihood Function "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9a08a",
   "metadata": {},
   "source": [
    "$f(\\hat{\\eta_{d}}) \\propto  - \\frac{1}{2} (\\eta_d-\\mu_d)^T \\sum^{-1}(\\eta_d-\\mu_d)+\\big(\\sum_v c_{d,v} log\\sum_k \\beta_{k,v} e^{\\eta_{d,k}}- W_d log \\sum_k e^{\\eta_{d,k}}\\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "611169ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lhood(eta, beta, doc_ct, mu, siginv, Ndoc):\n",
    "    \n",
    "    #formula\n",
    "    part1 = .5*(eta-mu)@siginv@(eta-mu)\n",
    "    part2 = np.sum(doc_ct * (eta_long.max() + np.log(np.exp(eta_long - eta_long.max())@beta)))-np.sum(doc_ct)*scipy.special.logsumexp(eta)\n",
    "    \n",
    "    # Move to trash eventually\n",
    "    #part1 = np.log(expeta@beta@doc_ct) - Ndoc * np.log(np.sum(expeta)) #alternative based on c++ code\n",
    "    #part1 = np.sum(doc_ct)*np.log(np.sum(expeta@beta)) - Ndoc * np.log(np.sum(expeta)) #alternative (based on paper)\n",
    "    #part2 = - .5*(eta-mu)@siginv@(eta-mu)\n",
    "    #out = .5*(eta-mu)@siginv@(eta-mu) + (np.sum(doc_ct)*np.log(np.sum(expeta@beta)) - Ndoc * np.log(np.sum(expeta)))\n",
    "    \n",
    "    out = part1 - part2\n",
    "    \n",
    "    return -out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "302ffed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1859.5082847176204"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lhood(eta, beta, doc_ct, mu, siginv, Ndoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6676a518",
   "metadata": {},
   "source": [
    "### Gradient of the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d16528",
   "metadata": {},
   "source": [
    "$\\nabla f(\\eta_d)_k = (\\sum c_{d,v} \\langle \\phi_{d,v,k} \\rangle) - W_d\\theta_{d,k}-\\big(\\sum^{-1}(\\eta_d-\\mu_d)\\big)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b897de",
   "metadata": {},
   "source": [
    "$\\theta_{d,k} = \\frac{exp(\\eta)}{\\sum exp(\\eta)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58e4bc",
   "metadata": {},
   "source": [
    "$\\langle\\phi_{d,k,v}\\rangle = \\frac{exp(\\eta_{d,k}) \\beta_{d,v,k}}{\\sum_k exp(\\eta_{d,k}) \\beta_{d,v,k}} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "80d7f6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(eta, beta, doc_ct, mu, siginv, Ndoc):\n",
    "    \n",
    "    #formula\n",
    "    part1 = np.delete(np.sum(phi * doc_ct,axis=1) - np.sum(doc_ct)*theta, neta)\n",
    "    part2 = siginv@(eta-mu)\n",
    "\n",
    "    return part2-part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ea1be066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.03875613,  3.42609557, -4.22852552,  6.04017365,  3.03373851,\n",
       "       -6.22652965,  3.05837804, -2.50537515,  2.87384212])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(eta,beta, doc_ct, mu, siginv, Ndoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f0ac5e",
   "metadata": {},
   "source": [
    "# We minimize the negative log-likelihood which is equivalent to maximizing the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a0cda5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59.723380536159084"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import check_grad\n",
    "\n",
    "optimize.check_grad(lhood, grad, eta, beta, doc_ct, mu, siginv, Ndoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519686d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3bbc2",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f0f6254",
   "metadata": {},
   "source": [
    "### Optimize multiple arguments at the same time using scipy requires de-vectorization of the arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bca62f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toVector(eta,beta_i, doc_ct, mu, siginv, Ndoc):\n",
    "    assert eta.shape == (9,)\n",
    "    assert beta_i.shape == (10, 1, 127)\n",
    "    assert doc_ct.shape == (127,)\n",
    "    assert mu.shape == (9,1)\n",
    "    assert siginv.shape == (9,9)\n",
    "    assert Ndoc.shape == ()\n",
    "    return np.hstack([eta.flatten(),beta_i.flatten(), doc_ct.flatten(), mu.flatten(), siginv.flatten(),Ndoc.flatten()])\n",
    "\n",
    "vec = toVector(eta,beta_i, doc_ct, mu, siginv, Ndoc)\n",
    "\n",
    "def toObject(vec):\n",
    "    eta = vec[0:9].reshape(9,)\n",
    "    beta_i = vec[9:9+1270].reshape(10,1,127)\n",
    "    doc_ct = vec[1279:1279+127].reshape(127,)\n",
    "    mu = vec[1279+127:1279+127+9].reshape(9,1)\n",
    "    siginv = vec[1279+136:1279+136+9*9].reshape(9,9)\n",
    "    Ndoc = vec[1279+136+9*9:].reshape(())\n",
    "    return eta,beta_i, doc_ct, mu, siginv, Ndoc\n",
    "\n",
    "eta,beta_i, doc_ct, mu, siginv, Ndoc = toObject(vec)\n",
    "\n",
    "def doOptimization(lhood, eta,beta_i, doc_ct, mu, siginv, Ndoc):\n",
    "    def objective(vec): \n",
    "        eta,beta_i, doc_ct, mu, siginv, Ndoc = toObject(vec)\n",
    "        return lhood(eta,beta_i, doc_ct, mu, siginv, Ndoc)\n",
    "    def gradient(vec):\n",
    "        eta,beta_i, doc_ct, mu, siginv, Ndoc = toObject(vec)\n",
    "        return grad(eta,beta_i, doc_ct, mu, siginv, Ndoc)\n",
    "\n",
    "    result = optimize.minimize(objective, toVector(eta,beta_i, doc_ct, mu, siginv, Ndoc))\n",
    "    result.x = toObject(result.x) \n",
    "    return result\n",
    "\n",
    "result = doOptimization(lhood, eta,beta_i, doc_ct, mu, siginv, Ndoc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
