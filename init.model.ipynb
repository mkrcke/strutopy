{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0e8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba91356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "documents = corpora.MmCorpus('corpus.mm')\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8522ba",
   "metadata": {},
   "source": [
    "### Ingest corpus to create documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d46df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stm(documents, settings): \n",
    "      \n",
    "    K = settings['dim']['K']\n",
    "    V = settings['dim']['V']\n",
    "    A = settings['dim']['A']\n",
    "    N = settings['dim']['N']\n",
    "    \n",
    "    #Random initialization\n",
    "    mu = np.array([0]*(K-1))[:,None]\n",
    "    sigma = np.zeros(((K-1),(K-1)))\n",
    "    diag = np.diagonal(sigma, 0)\n",
    "    diag.setflags(write=True)\n",
    "    diag.fill(20)\n",
    "    beta = random.gamma(.1,1, V*K).reshape(K,V)\n",
    "    beta = (beta / beta.sum(axis=1)[:,None])\n",
    "    lambd = np.zeros((N, (K-1)))\n",
    "    \n",
    "    #turn beta into a list and assign it for each aspect\n",
    "    beta = [beta, beta] # FOR A=2\n",
    "    kappa_initialized = init_kappa(documents, K, V, A, interactions=settings['kappa']['interactions'])\n",
    "    \n",
    "    #create model object\n",
    "    model = {'mu':mu, 'sigma':sigma, 'beta': beta, 'lambda': lambd, 'kappa':kappa_initialized}\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def init_kappa(documents, K, V, A, interactions): \n",
    "    # read in documents and vocab\n",
    "    flat_documents = [item for sublist in documents for item in sublist]\n",
    "    m = []\n",
    "\n",
    "    total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "    for elem in flat_documents: \n",
    "        m.append(elem[1] / total_sum)\n",
    "\n",
    "    m = np.log(m) - np.log(np.mean(m)) #logit of m\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    aspectmod = A > 1 # if there is more than one topical content variable\n",
    "    if(aspectmod):\n",
    "        interact = interactions # allow for the choice to interact\n",
    "    else:\n",
    "        interact = FALSE\n",
    "\n",
    "    #Create the parameters object\n",
    "    parLength = K + A * aspectmod + (K*A)*interact\n",
    "\n",
    "    #create covariates. one element per item in parameter list.\n",
    "    #generation by type because its conceptually simpler\n",
    "    if not aspectmod & interact:\n",
    "        covar = {'k': np.arange(K),\n",
    "             'a': np.repeat(np.nan, parLength), #why parLength? \n",
    "             'type': np.repeat(1, K)}\n",
    "\n",
    "    if(aspectmod & interact == False):\n",
    "        covar = {'k': np.append(np.arange(K), np.repeat(np.nan, A)),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.arange(A)), \n",
    "                 'type': np.append(np.repeat(1, K), np.repeat(2, A))}      \n",
    "    if(interact):\n",
    "        covar = {'k': np.append(np.arange(K), np.append(np.repeat(np.nan, A), np.repeat(np.arange(K), A))),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.append(np.arange(A), np.repeat(np.arange(A), K))), \n",
    "                 'type': np.append(np.repeat(1, K), np.append(np.repeat(2, A),  np.repeat(3,K*A)))}\n",
    "\n",
    "    kappa = {'out': {'m':m,\n",
    "                     'params' : np.tile(np.repeat(0,V), (parLength, 1)),\n",
    "                     'covar' : covar\n",
    "                     #'kappasum':, why rolling sum?\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    return(kappa['out'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895cb66b",
   "metadata": {},
   "source": [
    "### Make Topic Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03643f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = 'blog'\n",
    "content = 'rating'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61fb823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopMatrix(x, data=None):\n",
    "    return(data.loc[:,x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1af31a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "xmat = makeTopMatrix(content, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46bed10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yvar = makeTopMatrix(content, data)\n",
    "yvar = yvar.astype('category')\n",
    "yvarlevels = set(yvar)\n",
    "betaindex = yvar.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = len(set(betaindex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400a1b1",
   "metadata": {},
   "source": [
    "# Setting control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16b2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K = 10 #settings.dim\n",
    "V = len(dictionary) #settings.dim\n",
    "N = len(documents) #settings.dim\n",
    "\n",
    "interactions = True #settings.kappa\n",
    "verbose = True\n",
    "\n",
    "init_type = \"Random\" #settings.init\n",
    "ngroups = 1 #settings.ngroups\n",
    "max_em_its = 15 #settings.convergence\n",
    "emtol = 1e-5 #settings.convergence\n",
    "\n",
    "#gamma_prior=(\"Pooled\",\"L1\") # settings.gamma.prior\n",
    "#sigma_prior=0 #settings.sigma.prior\n",
    "#kappa_prior=(\"L1\",\"Jeffreys\") # settings.kappa.prior\n",
    "\n",
    "#Initialize parameters\n",
    "\n",
    "settings = {\n",
    "    'dim':{\n",
    "        'K': K, #number of topics\n",
    "        'V' :V, #number of words\n",
    "        'A' : A, #dimension of topical content\n",
    "        'N' : N,\n",
    "        'wcounts':V\n",
    "    },\n",
    "    'kappa':{\n",
    "        'interactions':True,\n",
    "        'fixedintercept': True,\n",
    "        'contrats': False,\n",
    "        'mstep': {'tol':0.01, 'maxit':5}},\n",
    "    'tau':{\n",
    "        'mode': np.nan,\n",
    "        'tol': 1e-5,\n",
    "        'enet':1,\n",
    "        'nlambda':250,\n",
    "        'lambda.min.ratio':.001,\n",
    "        'ic.k':2,\n",
    "        'maxit':1e4},\n",
    "    'init':{\n",
    "        'mode':init_type, \n",
    "        'nits':20,\n",
    "        'burnin':25,\n",
    "        'alpha':50/K,\n",
    "        'eta':.01,\n",
    "        's':.05,\n",
    "        'p':3000},\n",
    "    'convergence':{\n",
    "        'max.em.its':max_em_its,\n",
    "        'em.converge.thresh':emtol,\n",
    "        'allow.neg.change':True,},\n",
    "    'covariates':{\n",
    "        'X':xmat,\n",
    "        'betaindex':betaindex,\n",
    "        'yvarlevels':yvarlevels,\n",
    "        'formula': prevalence,},\n",
    "    'gamma':{\n",
    "        'mode':np.nan,\n",
    "        'prior':np.nan,\n",
    "        'enet':1, \n",
    "        'ic.k':2,\n",
    "        'maxits':1000,},\n",
    "    'sigma':{\n",
    "        #'prior':sigma_prior,\n",
    "        'ngroups':ngroups,},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11ab119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stm_control(documents, vocab, settings, model=None):\n",
    "    \n",
    "    ##########\n",
    "    #Step 1: Initialize Parameters\n",
    "    ##########\n",
    "    \n",
    "    #ngroups = settings$ngroups\n",
    "    \n",
    "    if model == None:\n",
    "        print('Call init_stm()')\n",
    "        model = init_stm(documents, settings) #initialize\n",
    "    else: \n",
    "        model = model\n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    ntokens = settings['dim']['wcounts']\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    stopits = True\n",
    "    \n",
    "    ############\n",
    "    #Step 2: Run EM\n",
    "    ############\n",
    "    \n",
    "    while stopits == True:\n",
    "        #####\n",
    "        # Non-Blocked Updates\n",
    "        #####\n",
    "        t1 = time.process_time()\n",
    "\n",
    "        #run the model\n",
    "        #suffstats = estep(documents=documents, beta_index=betaindex,\n",
    "        #                 update_mu=(!is.null(mu$gamma)),\n",
    "        #                 beta$beta, lambda, mu$mu, sigma,\n",
    "        #                 verbose)\n",
    "        \n",
    "        print(\"Completed E-Step ({} seconds). \\n\".format(math.floor((time.process_time()-t1))))\n",
    "\n",
    "\n",
    "        #unpack variables \n",
    "        \n",
    "        #t1 = process_time()\n",
    "        #sigma_ss = suffstats['sigma']\n",
    "        #lambd <- suffstats['lambd']\n",
    "        #beta_ss <- suffstats['beta']\n",
    "        #bound_ss <- suffstats['bound']\n",
    "        #do the m-step\n",
    "        #mu = opt_mu(lambd=lambd,\n",
    "        #            mode=settings['gamma']['mode'],\n",
    "        #            covar=settings['covariates']['X'],\n",
    "        #            enet=settings['gamma']['enet'],ic.k=settings$gamma$ic.k,\n",
    "        #            maxits=settings['gamma']['maxits'])\n",
    "        #sigma = opt_sigma(nu=sigma_ss, lambd=lambd,\n",
    "        #                     mu=mu['mu'], sigprior=settings['sigma']['prior'])\n",
    "        #beta = opt_beta(beta_ss, beta['kappa'], settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6634ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stm_control(documents, vocab, settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afd20d",
   "metadata": {},
   "source": [
    "From **A Model of Text for Experimentation in the Social Sciences (Roberts et al.)**:\n",
    "- $\\beta_k$ is a V-dimensional probability mass function that controls the frequency according to which terms are generated from that topic.\n",
    "\n",
    "From **The Structural Topic Model and Applied Social Science (Roberts et al.)**:\n",
    "- 'For the nonconjugate logistic normal variables in the E-step we use a Laplace approximation.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82e5e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    model = init_stm(documents, settings) #initialize\n",
    "    \n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    ntokens = settings['dim']['wcounts']\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    stopits = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845496b3",
   "metadata": {},
   "source": [
    "#Let's start by assuming its one beta and we may have arbitrarily subset the number of docs.\n",
    "```python\n",
    "> def estep(documents=documents, beta_index=betaindex,\n",
    "          update_mu=True,\n",
    "          beta['beta'], lambd, mu, sigma,\n",
    "          verbose):\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162859f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step():\n",
    "    #quickly define useful constants\n",
    "    V = beta['beta'][0].shape[1] # ncol\n",
    "    K = beta['beta'][0].shape[0] # nrow\n",
    "    N = len(documents)\n",
    "    A = len(beta['beta'])\n",
    "    \n",
    "    # 1) Initialize Sufficient Statistics \n",
    "    sigma_ss = np.zeros(((K-1),(K-1)))\n",
    "    if A == 2: \n",
    "        beta_ss = [np.zeros((K,V)), np.zeros((K,V))]\n",
    "    else:\n",
    "        print('Error: Only two metadata columns allowed.')\n",
    "    bound = np.repeat(0,N)\n",
    "    lambd = np.repeat(0,N)\n",
    "    \n",
    "    # 2) Precalculate common components\n",
    "    sigobj = np.linalg.cholesky(sigma)\n",
    "    sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "    siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "    \n",
    "    # 3) Document Scheduling\n",
    "    # For right now we are just doing everything in serial.\n",
    "    # the challenge with multicore is efficient scheduling while\n",
    "    # maintaining a small dimension for the sufficient statistics.\n",
    "    for i in range(N): \n",
    "        # update components\n",
    "        doc = documents[i]\n",
    "        words = [x for x,y in doc]\n",
    "        aspect = betaindex[i]\n",
    "        init = lambd[i]\n",
    "        if update_mu: mu_i = mu[i]\n",
    "        beta_i = beta['beta'][aspect][:,[words]]\n",
    "        \n",
    "        #infer the document with the logistic normal distribution\n",
    "        doc_results = \n",
    "        \n",
    "        # update sufficient statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "1dda45fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (3686581265.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/var/folders/mj/mqt0qs7x0njf2_k3_mr5dcg80000gn/T/ipykernel_30185/3686581265.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    siginv,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def logisticnormalcpp(eta=init,\n",
    "                      mu=mu_i,\n",
    "                      siginv,\n",
    "                      beta=beta_i,\n",
    "                      doc,\n",
    "                      sigmaentropy,\n",
    "                      method=\"BFGS\",\n",
    "                      control=list(maxit=500),\n",
    "                      hpbcpp=True):\n",
    "    \n",
    "    doc_ct = [y for x,y in doc] #count of words in document\n",
    "    Ndoc = np.sum(doc_ct)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934d2526",
   "metadata": {},
   "source": [
    "## Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9cc9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4fcb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67510454",
   "metadata": {},
   "outputs": [],
   "source": [
    "for(i in 1:N) {\n",
    "    #update components\n",
    "    doc <- documents[[i]]\n",
    "    words <- doc[1,]\n",
    "    aspect <- beta.index[i]\n",
    "    init <- lambda.old[i,]\n",
    "    if(update.mu) mu.i <- mu[,i]\n",
    "    beta.i <- beta[[aspect]][,words,drop=FALSE]\n",
    "    \n",
    "    #infer the document\n",
    "    doc.results <- logisticnormalcpp(eta=init, mu=mu.i, siginv=siginv, beta=beta.i, \n",
    "                                  doc=doc, sigmaentropy=sigmaentropy)\n",
    "    \n",
    "    # update sufficient statistics \n",
    "    sigma.ss <- sigma.ss + doc.results$eta$nu\n",
    "    beta.ss[[aspect]][,words] <- doc.results$phis + beta.ss[[aspect]][,words]\n",
    "    bound[i] <- doc.results$bound\n",
    "    lambda[[i]] <- c(doc.results$eta$lambda)\n",
    "    if(verbose && i%%ctevery==0) cat(\".\")\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bdfee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
