{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0e8aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy.random as random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from gensim import corpora\n",
    "from scipy import optimize\n",
    "import scipy\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fb9818f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8522ba",
   "metadata": {},
   "source": [
    "### Ingest corpus to create documents and vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec3e5d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load corpus\n",
    "documents = corpora.MmCorpus('corpus.mm')\n",
    "dictionary = corpora.Dictionary.load('dictionary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6c49c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d46df0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_stm(documents, settings): \n",
    "      \n",
    "    K = settings['dim']['K']\n",
    "    V = settings['dim']['V']\n",
    "    A = settings['dim']['A']\n",
    "    N = settings['dim']['N']\n",
    "    \n",
    "    #Random initialization\n",
    "    mu = np.array([0]*(K-1))[:,None]\n",
    "    sigma = np.zeros(((K-1),(K-1)))\n",
    "    diag = np.diagonal(sigma, 0)\n",
    "    diag.setflags(write=True)\n",
    "    diag.fill(20)\n",
    "    beta = random.gamma(.1,1, V*K).reshape(K,V)\n",
    "    beta = (beta / beta.sum(axis=1)[:,None])\n",
    "    lambd = np.zeros((N, (K-1)))\n",
    "    \n",
    "    #turn beta into a list and assign it for each aspect\n",
    "    beta = np.repeat(beta,A).reshape(K,V,A) \n",
    "    kappa_initialized = init_kappa(documents, K, V, A, interactions=settings['kappa']['interactions'])\n",
    "      \n",
    "    #create model object\n",
    "    model = {'mu':mu, 'sigma':sigma, 'beta': beta, 'lambda': lambd, 'kappa':kappa_initialized}\n",
    "    \n",
    "    return(model)\n",
    "\n",
    "def init_kappa(documents, K, V, A, interactions): \n",
    "    # read in documents and vocab\n",
    "    flat_documents = [item for sublist in documents for item in sublist]\n",
    "    m = []\n",
    "\n",
    "    total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "    for elem in flat_documents: \n",
    "        m.append(elem[1] / total_sum)\n",
    "\n",
    "    m = np.log(m) - np.log(np.mean(m)) #logit of m\n",
    "\n",
    "\n",
    "    #Defining parameters\n",
    "    aspectmod = A > 1 # if there is more than one level for the topical content\n",
    "    if(aspectmod):\n",
    "        interact = interactions # allow for the choice to interact\n",
    "    else:\n",
    "        interact = FALSE\n",
    "\n",
    "    #Create the parameters object\n",
    "    parLength = K + A * aspectmod + (K*A)*interact\n",
    "\n",
    "    #create covariates. one element per item in parameter list.\n",
    "    #generation by type because its conceptually simpler\n",
    "    if not aspectmod & interact:\n",
    "        covar = {'k': np.arange(K),\n",
    "             'a': np.repeat(np.nan, parLength), #why parLength? \n",
    "             'type': np.repeat(1, K)}\n",
    "\n",
    "    if(aspectmod & interact == False):\n",
    "        covar = {'k': np.append(np.arange(K), np.repeat(np.nan, A)),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.arange(A)), \n",
    "                 'type': np.append(np.repeat(1, K), np.repeat(2, A))}      \n",
    "    if(interact):\n",
    "        covar = {'k': np.append(np.arange(K), np.append(np.repeat(np.nan, A), np.repeat(np.arange(K), A))),\n",
    "                 'a': np.append(np.repeat(np.nan, K), np.append(np.arange(A), np.repeat(np.arange(A), K))), \n",
    "                 'type': np.append(np.repeat(1, K), np.append(np.repeat(2, A),  np.repeat(3,K*A)))}\n",
    "\n",
    "    kappa = {'out': {'m':m,\n",
    "                     'params' : np.tile(np.repeat(0,V), (parLength, 1)),\n",
    "                     'covar' : covar\n",
    "                     #'kappasum':, why rolling sum?\n",
    "                    }\n",
    "            }\n",
    "\n",
    "    return(kappa['out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c486270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def softmax_weights(x, weight):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = weight*np.exp(x - np.max(x))[:,None]\n",
    "    return e_x / e_x.sum(axis=0)\n",
    "\n",
    "def lhood(eta, mu, siginv, doc_ct, Ndoc, eta_long, beta_tuple, phi, theta, neta):\n",
    "    \n",
    "    #formula \n",
    "    #rewrite LSE to prevent overflow\n",
    "    part1 = np.sum(doc_ct * (eta_long.max() + np.log(np.exp(eta_long - eta_long.max())@beta_tuple)))-np.sum(doc_ct)*scipy.special.logsumexp(eta)\n",
    "    part2 = .5*(eta-mu)@siginv@(eta-mu)\n",
    "    \n",
    "    out = part2 - part1\n",
    "    \n",
    "    return -out\n",
    "\n",
    "def grad(eta, mu, siginv, doc_ct,  Ndoc, eta_long, beta_tuple, phi, theta, neta):\n",
    "\n",
    "    #formula\n",
    "    part1 = np.delete(np.sum(phi * doc_ct,axis=1) - np.sum(doc_ct)*theta, neta)\n",
    "    part2 = siginv@(eta-mu)\n",
    "\n",
    "    return part2 - part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "56f4b482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_step(documents, mu, sigma, lambd, beta):\n",
    "    #quickly define useful constants\n",
    "    V = beta['beta'][0].shape[1] # ncol\n",
    "    K = beta['beta'][0].shape[0] # nrow\n",
    "    N = len(documents)\n",
    "    A = len(beta['beta'])\n",
    "    \n",
    "    # 1) Initialize Sufficient Statistics \n",
    "    sigma_ss = np.zeros(((K-1),(K-1)))\n",
    "    beta_ss_i = np.zeros((K,V))\n",
    "    beta_ss = np.repeat(beta_ss_i, A).reshape(K,V,A)\n",
    "    bound = np.repeat(0,N)\n",
    "    #lambd = np.repeat(0,N)\n",
    "    \n",
    "    # 2) Precalculate common components\n",
    "    sigobj = np.linalg.cholesky(sigma) #initialization of sigma not positive definite\n",
    "    sigmaentropy = np.sum(np.log(np.diag(sigobj)))\n",
    "    siginv = np.linalg.inv(sigobj).T*np.linalg.inv(sigobj)\n",
    "    \n",
    "    # 3) Document Scheduling\n",
    "    # For right now we are just doing everything in serial.\n",
    "    # the challenge with multicore is efficient scheduling while\n",
    "    # maintaining a small dimension for the sufficient statistics.\n",
    "    \n",
    "    mu = mu.flatten()\n",
    "    \n",
    "    #set parameters for one document (i)\n",
    "    for i in range(N):\n",
    "\n",
    "        eta=lambd[i]\n",
    "        neta = len(eta)\n",
    "        eta_long = np.insert(eta,-1,0)\n",
    "\n",
    "        doc = documents[i]\n",
    "        words = [x for x,y in doc]\n",
    "        aspect = betaindex[i]\n",
    "        #beta_i = beta['beta'][aspect][:,[words]] # replace with beta_ss[aspect][:,np.array(words)]\n",
    "        beta_tuple = beta['beta'][aspect][:,np.array(words)]\n",
    "\n",
    "        #set document specs\n",
    "        doc_ct = np.array([y for x,y in doc]) #count of words in document\n",
    "        Ndoc = np.sum(doc_ct)\n",
    "\n",
    "        # initial values\n",
    "        #beta_tuple = beta_i.reshape(K,beta_i.shape[2])\n",
    "        theta = softmax(eta_long)\n",
    "        phi = softmax_weights(eta_long, beta_tuple)\n",
    "        # optimize variational posterior\n",
    "        result = optimize.fmin_bfgs(lhood,x0=eta,\n",
    "                           args=(mu, siginv, Ndoc, doc_ct, eta_long, beta_tuple, phi, theta, neta),\n",
    "                           fprime=grad)\n",
    "        #solve hpb\n",
    "        doc_results = hpb(eta=result,\n",
    "                          doc_ct=doc_ct,\n",
    "                          mu=mu,\n",
    "                          siginv=siginv,\n",
    "                          beta_tuple=beta_tuple,\n",
    "                          sigmaentropy=sigmaentropy,\n",
    "                          theta=theta)\n",
    "        \n",
    "        # update sufficient statistics        \n",
    "        #print(f\"Input:eta: {doc_results['eta'].get('nu').shape}\\nphi:{doc_results['phi'].shape}\")\n",
    "        print(f\"\\nbound:{doc_results['bound']}\")\n",
    "        sigma_ss = sigma_ss + doc_results['eta'].get('nu')\n",
    "\n",
    "\n",
    "        #beta_ss[aspect][:,[words]] = beta_ss[aspect][:,[words]].reshape(K,beta_ss[aspect][:,[words]].shape[2]) \n",
    "        \n",
    "        beta_ss[aspect][:,np.array(words)] = doc_results.get('phi') + np.take(beta_ss[aspect], words, 1)\n",
    "        bound[i] = doc_results.get('bound')\n",
    "        lambd[i] = doc_results['eta'].get('lambd')\n",
    "        \n",
    "        #4) Combine and Return Sufficient Statistics\n",
    "        results = {'sigma':sigma_ss, 'beta':beta_ss, 'bound': bound, 'lambd': lambd}\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4f5028",
   "metadata": {},
   "source": [
    "# Solve for Hessian/Phi/Bound returning the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5de93314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpb(eta, doc_ct, mu,siginv, beta_tuple,sigmaentropy, theta):\n",
    "    #print(f'Input for the Hessian Phi Bound\\neta:{eta.shape}\\ndoc_ct:{doc_ct.shape}\\nmu:{mu.shape}\\nsiginv:{siginv.shape}\\nbeta_tuple:{beta_tuple.shape}\\nsigmaentropy:{sigmaentropy.shape}\\ntheta:{theta.shape}')\n",
    "    eta_long = np.insert(eta,-1,0)\n",
    "    # copy to mess with \n",
    "    beta_temp = beta_tuple\n",
    "    #column-wise multiplication of beta and expeta\n",
    "    beta_temp = beta_temp*eta_long[:,None]\n",
    "    \n",
    "    beta_temp = (np.sqrt(doc_ct)[:,None] / np.sum(beta_temp, axis=0)[:,None]) * beta_temp.T # with shape (VxK)\n",
    "    hess = beta_temp.T@beta_temp-np.sum(doc_ct)*(theta*theta.T) # hessian with shape KxK\n",
    "    #print(f'hess.shape:{hess.shape}')\n",
    "    #we don't need beta_temp any more so we turn it into phi \n",
    "    beta_temp = beta_temp.T * np.sqrt(doc_ct) # should equal phi ?! \n",
    "\n",
    "    np.fill_diagonal(hess, np.diag(hess)-np.sum(beta_temp, axis=1)-np.sum(doc_ct)*theta) #altered diagonal of h\n",
    "    \n",
    "    # drop last row and columns\n",
    "    hess = np.delete(hess,eta.size,0)\n",
    "    hess = np.delete(hess,eta.size,1)\n",
    "    hess = hess + siginv # at this point, the hessian is complete\n",
    "\n",
    "    # Invert hessian via cholesky decomposition \n",
    "    #np.linalg.cholesky(hess)\n",
    "    # error -> not properly converged: make the matrix positive definite\n",
    "    \n",
    "    dvec = hess.diagonal()\n",
    "    magnitudes = sum(abs(hess), 1) - abs(dvec)\n",
    "    # cholesky decomposition works only for symmetric and positive definite matrices\n",
    "    dvec = np.where(dvec < magnitudes, magnitudes, dvec)\n",
    "    # A Hermitian diagonally dominant matrix A with real non-negative diagonal entries is positive semidefinite. \n",
    "    np.fill_diagonal(hess, dvec)\n",
    "    #that was sufficient to ensure positive definiteness so no we can do cholesky \n",
    "    nu = np.linalg.cholesky(hess)\n",
    "\n",
    "    #compute 1/2 the determinant from the cholesky decomposition\n",
    "    detTerm = -np.sum(np.log(nu.diagonal()))\n",
    "    #print(f'detTerm {detTerm}')\n",
    "\n",
    "    #Finish constructing nu\n",
    "    nu = np.linalg.inv(np.triu(nu))\n",
    "    nu = nu@nu.T\n",
    "    # precompute the difference since we use it twice\n",
    "    diff = eta-mu\n",
    "    #print(f'diff (shape):{diff, diff.shape}')\n",
    "    ############## generate the bound and make it a scalar ##################\n",
    "    #print('Compute the lower bound...')\n",
    "    #print(f'shape of theta[:,None]: {theta[None,:].shape}')\n",
    "    #print(f'shape of beta_temp: {beta_temp.shape}')\n",
    "    #print(f'shape of the first summand: {(np.log(theta[None:,]@beta_temp)[None:,]@doc_ct).shape}')\n",
    "    #print(f'first summand: {np.log(theta[None:,]@beta_temp)@doc_ct}')\n",
    "    #print(f'shape of the second summand: {(detTerm - .5*diff.T@siginv@diff).shape}')\n",
    "    #print(f'second summand: {detTerm - .5*diff.T@siginv@diff}')\n",
    "    bound = np.log(theta[None:,]@beta_temp)@doc_ct + detTerm - .5*diff.T@siginv@diff - sigmaentropy \n",
    "    ###################### return values as dictionary ######################\n",
    "    phi = beta_temp\n",
    "    eta = {'lambd' : eta, 'nu':nu}\n",
    "    \n",
    "    result = {'phi':phi,'eta': eta,'bound': bound}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e71cded",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = 'rating'\n",
    "content = 'blog'\n",
    "num_topics = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400a1b1",
   "metadata": {},
   "source": [
    "# Setting control variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a16b2c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTopMatrix(x, data=None):\n",
    "    return(data.loc[:,x]) # add intercept! \n",
    "\n",
    "xmat = makeTopMatrix(prevalence, data) # replace: prevalence\n",
    "\n",
    "#replace: content\n",
    "yvar = makeTopMatrix(content, data).astype('category')\n",
    "yvarlevels = set(yvar)\n",
    "betaindex = yvar.cat.codes\n",
    "A = len(set(betaindex))\n",
    "\n",
    "interactions = True #settings.kappa\n",
    "verbose = True\n",
    "\n",
    "init_type = \"Random\" #settings.init\n",
    "ngroups = 1 #settings.ngroups\n",
    "max_em_its = 15 #settings.convergence\n",
    "emtol = 1e-5 #settings.convergence\n",
    "\n",
    "#gamma_prior=(\"Pooled\",\"L1\") # settings.gamma.prior\n",
    "#sigma_prior=0 #settings.sigma.prior\n",
    "#kappa_prior=(\"L1\",\"Jeffreys\") # settings.kappa.prior\n",
    "\n",
    "#Initialize parameters\n",
    "\n",
    "settings = {\n",
    "    'dim':{\n",
    "        'K': num_topics, #number of topics\n",
    "        'V' : len(dictionary), #number of words\n",
    "        'A' : A, #dimension of topical content\n",
    "        'N' : len(documents),\n",
    "    },\n",
    "    'kappa':{\n",
    "        'interactions':True,\n",
    "        'fixedintercept': True,\n",
    "        'contrats': False,\n",
    "        'mstep': {'tol':0.01, 'maxit':5}},\n",
    "    'tau':{\n",
    "        'mode': np.nan,\n",
    "        'tol': 1e-5,\n",
    "        'enet':1,\n",
    "        'nlambda':250,\n",
    "        'lambda.min.ratio':.001,\n",
    "        'ic.k':2,\n",
    "        'maxit':1e4},\n",
    "    'init':{\n",
    "        'mode':init_type, \n",
    "        'nits':20,\n",
    "        'burnin':25,\n",
    "        'alpha':50/num_topics,\n",
    "        'eta':.01,\n",
    "        's':.05,\n",
    "        'p':3000},\n",
    "    'convergence':{\n",
    "        'max.em.its':max_em_its,\n",
    "        'em.converge.thresh':emtol,\n",
    "        'allow.neg.change':True,},\n",
    "    'covariates':{\n",
    "        'X':xmat,\n",
    "        'betaindex':betaindex,\n",
    "        'yvarlevels':yvarlevels,\n",
    "        'formula': prevalence,},\n",
    "    'gamma':{\n",
    "        'mode':'L1', #needs to be set for the m-step (update mu in the topical prevalence model)\n",
    "        'prior':np.nan, #sigma in the topical prevalence model\n",
    "        'enet':1, #regularization term\n",
    "        'ic.k':2,#information criterion\n",
    "        'maxits':1000,},\n",
    "    'sigma':{\n",
    "        #'prior':sigma_prior,\n",
    "        'ngroups':ngroups,},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11ab119e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stm_control(documents, vocab, settings, model=None):\n",
    "    \n",
    "    ##########\n",
    "    #Step 1: Initialize Parameters\n",
    "    ##########\n",
    "    \n",
    "    #ngroups = settings$ngroups\n",
    "    \n",
    "    if model == None:\n",
    "        print('Call init_stm()')\n",
    "        model = init_stm(documents, settings) #initialize\n",
    "    else: \n",
    "        model = model\n",
    "        \n",
    "    # unpack initialized model\n",
    "    \n",
    "    mu = model['mu']\n",
    "    sigma = model['sigma']\n",
    "    lambd = model['lambda'] \n",
    "    beta = {'beta': model['beta'],\n",
    "            'kappa': model['kappa']}\n",
    "    \n",
    "    convergence = None\n",
    "    \n",
    "    #discard the old object\n",
    "    del model\n",
    "    \n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    #Pull out some book keeping elements\n",
    "    betaindex = settings['covariates']['betaindex']\n",
    "    \n",
    "    ############\n",
    "    #Step 2: Run EM\n",
    "    ############\n",
    "    \n",
    "    t1 = time.process_time()\n",
    "\n",
    "    #run the model (so far: one iteration)\n",
    "    #suffstats = [] \n",
    "\n",
    "    ############\n",
    "    # Run E-Step\n",
    "\n",
    "    suffstats = (e_step(documents, mu, sigma, lambd, beta))\n",
    "    \n",
    "    sigma_ss = suffstats.get('sigma')\n",
    "    lambd = suffstats.get('lambd')\n",
    "    beta_ss = suffstats.get('beta')\n",
    "    bound_ss = suffstats.get('bound')\n",
    "    \n",
    "    print(lambd.shape)\n",
    "    \n",
    "    ############\n",
    "    # Run M-Step \n",
    "    \n",
    "    # mu = opt_mu(lambd,\n",
    "    #  covar=settings['covariates']['X'],\n",
    "    #   enet=settings['gamma']['enet'],\n",
    "    #   ic_k=settings['gamma']['ic.k'],\n",
    "    #   maxits = settings['gamma']['maxits'],\n",
    "    #   mode = settings['gamma']['mode'])\n",
    "       \n",
    "\n",
    "    \n",
    "    print(\"Completed E-Step ({} seconds). \\n\".format(math.floor((time.process_time()-t1))))\n",
    "\n",
    "    return lambd, beta_ss, sigma_ss, bound_ss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afd20d",
   "metadata": {},
   "source": [
    "From **A Model of Text for Experimentation in the Social Sciences (Roberts et al.)**:\n",
    "- $\\beta_k$ is a V-dimensional probability mass function that controls the frequency according to which terms are generated from that topic.\n",
    "\n",
    "From **The Structural Topic Model and Applied Social Science (Roberts et al.)**:\n",
    "- 'For the nonconjugate logistic normal variables in the E-step we use a Laplace approximation.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c914f3b",
   "metadata": {},
   "source": [
    "## E-Step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1258d49",
   "metadata": {},
   "source": [
    "### Likelihood Function (for the nonconjugate variable) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b9a08a",
   "metadata": {},
   "source": [
    "$f(\\hat{\\eta}_{d}) \\propto  - \\frac{1}{2} (\\eta_d-\\mu_d)^T \\sum^{-1}(\\eta_d-\\mu_d)+\\big(\\sum_v c_{d,v} log\\sum_k \\beta_{k,v} e^{\\eta_{d,k}}- W_d log \\sum_k e^{\\eta_{d,k}}\\big)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a577fc2",
   "metadata": {},
   "source": [
    "### Gradient of the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51899b26",
   "metadata": {},
   "source": [
    "$\\nabla f(\\eta_d)_k = (\\sum c_{d,v} \\langle \\phi_{d,v,k} \\rangle) - W_d\\theta_{d,k}-\\big(\\sum^{-1}(\\eta_d-\\mu_d)\\big)_k$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9264fc4",
   "metadata": {},
   "source": [
    "with $\\theta_{d,k} = \\frac{exp(\\eta)}{\\sum exp(\\eta)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b888695d",
   "metadata": {},
   "source": [
    "and $\\langle\\phi_{d,k,v}\\rangle = \\frac{exp(\\eta_{d,k}) \\beta_{d,v,k}}{\\sum_k exp(\\eta_{d,k}) \\beta_{d,v,k}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2480c9c",
   "metadata": {},
   "source": [
    "### Variational posterior: \n",
    "$q(\\eta_d) \\sim N(\\hat{\\eta}_d, -\\nabla^2 f(\\hat{\\eta}_d)^{-1})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645eb2e",
   "metadata": {},
   "source": [
    "# Sigmaentropy\n",
    "\n",
    "- Logarithmic sum of the diagonals of the variance-covariance matrix of topics -> integer ?!\n",
    "- ```np.sum(np.log(np.diag(sigobj))```, where ```sigobj``` is the lower diagonal of the cholesky factor L\n",
    "    - c.f. https://numpy.org/doc/stable/reference/generated/numpy.linalg.cholesky.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f1d08",
   "metadata": {},
   "source": [
    "## M-Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87b2f1",
   "metadata": {},
   "source": [
    "Perform a penalized regression on $\\lambda_k$ to get the updates for $\\gamma_k$ based on the prevalence covariates (family: \"mgaussian\"). The prior on document-topic proportions maximizes the approximate ELBO with respect to the document-specific mean $\\mu_{d,k} = X_d \\gamma_k$ and the topic covariance matrix $\\Sigma$.\n",
    "\n",
    "Updates for $\\gamma_k$ correspond to linear regression for each topic under the user specified prior with $\\lambda_k$ as the outcome variable.\n",
    "\n",
    "**NOTE**: Cross-Validation for the Penalized Regression?!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29165bd3",
   "metadata": {},
   "source": [
    "### Topical Prevalence Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ecfd40a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_mu(lambd, covar, enet, ic_k, maxits, mode = \"L1\"):\n",
    "    #prepare covariate matrix for modeling \n",
    "    covar = covar.astype('category')\n",
    "    covar2D = covar[:,None] #prepares 1D array for one-hot encoding by making it 2D\n",
    "    #create and fit OHE\n",
    "    enc = OneHotEncoder(handle_unknown='ignore') \n",
    "    covarOHE = enc.fit_transform(covar2D).toarray()\n",
    "    \n",
    "    # TO-DO: mode = CTM if there are no covariates \n",
    "    # TO-DO: mode = Pooled if there are covariates requires variational linear regression with Half-Cauchy hyperprior\n",
    "    \n",
    "    # mode = L1 simplest method requires only glmnet (https://cran.r-project.org/web/packages/glmnet/index.html)\n",
    "    if mode == \"L1\":\n",
    "        model = linear_model.Lasso(alpha=enet)\n",
    "        fitted_model = model.fit(covarOHE,lambd)\n",
    "    else: \n",
    "        raise ValueError('Optimizing the parameter mu requires a mode. Choose from \"CTM\", \"Pooled\" or \"L1\" (default).')\n",
    "\n",
    "    gamma = np.insert(fitted_model.coef_, 0, fitted_model.intercept_)\n",
    "    # insert an intercept column into the covariate matrix\n",
    "    design_matrix = np.c_[ np.ones(covarOHE.shape[0]), covarOHE]\n",
    "    #compute mu as a linear combination of X and gamma\n",
    "    print(gamma.shape)\n",
    "    print(design_matrix.shape)\n",
    "    mu = design_matrix@gamma\n",
    "\n",
    "    return {\n",
    "        #'mu':mu,\n",
    "        'gamma':gamma\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d9fba736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Call init_stm()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb Cell 29'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000037?line=0'>1</a>\u001b[0m \u001b[39m#e-step iteration for all N documents: 52 seconds \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000037?line=2'>3</a>\u001b[0m out \u001b[39m=\u001b[39m stm_control(documents, vocab, settings)\n",
      "\u001b[1;32m/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb Cell 14'\u001b[0m in \u001b[0;36mstm_control\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=36'>37</a>\u001b[0m t1 \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=38'>39</a>\u001b[0m \u001b[39m#run the model (so far: one iteration)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=39'>40</a>\u001b[0m \u001b[39m#suffstats = [] \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=40'>41</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=41'>42</a>\u001b[0m \u001b[39m############\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=42'>43</a>\u001b[0m \u001b[39m# Run E-Step\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=44'>45</a>\u001b[0m suffstats \u001b[39m=\u001b[39m (e_step(documents, mu, sigma, lambd, beta))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=46'>47</a>\u001b[0m sigma_ss \u001b[39m=\u001b[39m suffstats\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39msigma\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000013?line=47'>48</a>\u001b[0m lambd \u001b[39m=\u001b[39m suffstats\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mlambd\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32m/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb Cell 8'\u001b[0m in \u001b[0;36me_step\u001b[0;34m(documents, mu, sigma, lambd, beta)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000007?line=35'>36</a>\u001b[0m aspect \u001b[39m=\u001b[39m betaindex[i]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000007?line=36'>37</a>\u001b[0m \u001b[39m#beta_i = beta['beta'][aspect][:,[words]] # replace with beta_ss[aspect][:,np.array(words)]\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000007?line=37'>38</a>\u001b[0m beta_tuple \u001b[39m=\u001b[39m beta[\u001b[39m'\u001b[39;49m\u001b[39mbeta\u001b[39;49m\u001b[39m'\u001b[39;49m][aspect][:,np\u001b[39m.\u001b[39;49marray(words)]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000007?line=39'>40</a>\u001b[0m \u001b[39m#set document specs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000007?line=40'>41</a>\u001b[0m doc_ct \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([y \u001b[39mfor\u001b[39;00m x,y \u001b[39min\u001b[39;00m doc]) \u001b[39m#count of words in document\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "#e-step iteration for all N documents: 52 seconds \n",
    "\n",
    "out = stm_control(documents, vocab, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "4235e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/mqt0qs7x0njf2_k3_mr5dcg80000gn/T/ipykernel_39814/3769805511.py:4: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  covar2D = covar[:,None] #prepares 1D array for one-hot encoding by making it 2D\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [13246, 9]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb Cell 32'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=0'>1</a>\u001b[0m opt_mu(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=1'>2</a>\u001b[0m     lambd[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=2'>3</a>\u001b[0m     covar\u001b[39m=\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mcovariates\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=3'>4</a>\u001b[0m     enet\u001b[39m=\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39menet\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=4'>5</a>\u001b[0m     ic_k\u001b[39m=\u001b[39;49msettings[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mic.k\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=5'>6</a>\u001b[0m     maxits \u001b[39m=\u001b[39;49m settings[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mmaxits\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=6'>7</a>\u001b[0m     mode \u001b[39m=\u001b[39;49m settings[\u001b[39m'\u001b[39;49m\u001b[39mgamma\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m'\u001b[39;49m\u001b[39mmode\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000056?line=7'>8</a>\u001b[0m     )\n",
      "\u001b[1;32m/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb Cell 29'\u001b[0m in \u001b[0;36mopt_mu\u001b[0;34m(lambd, covar, enet, ic_k, maxits, mode)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000030?line=11'>12</a>\u001b[0m \u001b[39mif\u001b[39;00m mode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mL1\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000030?line=12'>13</a>\u001b[0m     model \u001b[39m=\u001b[39m linear_model\u001b[39m.\u001b[39mLasso(alpha\u001b[39m=\u001b[39menet)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000030?line=13'>14</a>\u001b[0m     fitted_model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(covarOHE,lambd)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000030?line=14'>15</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/maxkricke/Desktop/Projects/masterthesis/strutopy/expectation_maximization.ipynb#ch0000030?line=15'>16</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mOptimizing the parameter mu requires a mode. Choose from \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCTM\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPooled\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m or \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL1\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m (default).\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py:949\u001b[0m, in \u001b[0;36mElasticNet.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=946'>947</a>\u001b[0m \u001b[39mif\u001b[39;00m check_input:\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=947'>948</a>\u001b[0m     X_copied \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy_X \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=948'>949</a>\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=949'>950</a>\u001b[0m         X,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=950'>951</a>\u001b[0m         y,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=951'>952</a>\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsc\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=952'>953</a>\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mF\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=953'>954</a>\u001b[0m         dtype\u001b[39m=\u001b[39;49m[np\u001b[39m.\u001b[39;49mfloat64, np\u001b[39m.\u001b[39;49mfloat32],\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=954'>955</a>\u001b[0m         copy\u001b[39m=\u001b[39;49mX_copied,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=955'>956</a>\u001b[0m         multi_output\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=956'>957</a>\u001b[0m         y_numeric\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=957'>958</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=958'>959</a>\u001b[0m     y \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=959'>960</a>\u001b[0m         y, order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mF\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mtype, ensure_2d\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=960'>961</a>\u001b[0m     )\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/linear_model/_coordinate_descent.py?line=962'>963</a>\u001b[0m n_samples, n_features \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py?line=593'>594</a>\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py?line=594'>595</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py?line=595'>596</a>\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py?line=596'>597</a>\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/base.py?line=598'>599</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py:1088\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1069'>1070</a>\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1070'>1071</a>\u001b[0m     X,\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1071'>1072</a>\u001b[0m     accept_sparse\u001b[39m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1082'>1083</a>\u001b[0m     input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1083'>1084</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1085'>1086</a>\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[0;32m-> <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1087'>1088</a>\u001b[0m check_consistent_length(X, y)\n\u001b[1;32m   <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=1089'>1090</a>\u001b[0m \u001b[39mreturn\u001b[39;00m X, y\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py:383\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=380'>381</a>\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=381'>382</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=382'>383</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=383'>384</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=384'>385</a>\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    <a href='file:///usr/local/Caskroom/miniconda/base/envs/master/lib/python3.10/site-packages/sklearn/utils/validation.py?line=385'>386</a>\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [13246, 9]"
     ]
    }
   ],
   "source": [
    "opt_mu(\n",
    "    lambd,\n",
    "    covar=settings['covariates']['X'],\n",
    "    enet=settings['gamma']['enet'],\n",
    "    ic_k=settings['gamma']['ic.k'],\n",
    "    maxits = settings['gamma']['maxits'],\n",
    "    mode = settings['gamma']['mode']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "13ff6409",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/mqt0qs7x0njf2_k3_mr5dcg80000gn/T/ipykernel_39814/1733353521.py:2: FutureWarning: Support for multi-dimensional indexing (e.g. `obj[:, None]`) is deprecated and will be removed in a future version.  Convert to a numpy array before indexing instead.\n",
      "  covar2D = covar[:,None] #prepares 1D array for one-hot encoding by making it 2D\n"
     ]
    }
   ],
   "source": [
    "covar = covar.astype('category')\n",
    "covar2D = covar[:,None] #prepares 1D array for one-hot encoding by making it 2D\n",
    "enc = OneHotEncoder(handle_unknown='ignore') #create OHE\n",
    "covarOHE = enc.fit_transform(covar2D).toarray() #fit OHE\n",
    "design_matrix = np.c_[ np.ones(covarOHE.shape[0]), covarOHE]\n",
    "test = design_matrix@gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67af8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_sigma():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d24d68",
   "metadata": {},
   "source": [
    "### Topical Content Model Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "90a55f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_beta(beta_ss, kappa, settings):\n",
    "    #if its standard lda just row normalize\n",
    "    if kappa is None: \n",
    "        return list(beta=list(beta_ss[[1]]/np.sum(beta_ss[[1]])))\n",
    "    #if its a SAGE model (Eisenstein et al., 2013) use the distributed poissons\n",
    "    # if settings['tau']['mode'] == \"L1\":\n",
    "    #     out = mnreg(beta_ss, settings) \n",
    "    # else: \n",
    "    #     out = jeffreysKappa(beta_ss, kappa, settings)\n",
    "    # return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ee6c0",
   "metadata": {},
   "source": [
    "### Global Covariance Update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
