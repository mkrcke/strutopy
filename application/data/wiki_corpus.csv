,pageid,text,title,statistics,ml
0,1776354,"Anthony William Fairbank Edwards, FRS (born 1935) is a British statistician, geneticist and evolutionary biologist. He is the son of the surgeon Harold C. Edwards, and brother of medical geneticist John H. Edwards. He has sometimes been called ""Fisher's Edwards"" to distinguish him from his brother, because he was mentored by Ronald Fisher. Edwards has always had a high regard for Fisher's scientific contributions and has written extensively on them. To mark the Fisher centenary in 1990, Edwards proposed a commemorative Sir Ronald Fisher window be installed in the Dining Hall of Gonville & Caius College. When the window was removed in 2020, he vigorously opposed the move.",A. W. F. Edwards,1,0
1,19616384,"In algebra, which is a broad division of mathematics, abstract algebra (occasionally called modern algebra) is the study of algebraic structures. Algebraic structures include groups, rings, fields, modules, vector spaces, lattices, and algebras. The term abstract algebra was coined in the early 20th century to distinguish this area of study from older parts of algebra, and more specifically from elementary algebra, the use of variables to represent numbers in computation and reasoning.
Algebraic structures, with their associated homomorphisms, form mathematical categories. Category theory is a formalism that allows a unified  way for expressing properties and constructions that are similar for various structures.
Universal algebra is a related subject that studies types of algebraic structures as single objects. For example, the structure of groups is a single object in universal algebra, which is called the variety of groups.",Abstract algebra,1,0
2,42122213,"Abundance estimation comprises all statistical methods for estimating the number of individuals in a population. In ecology, this may be anything from estimating the number of daisies in a field to estimating the number of blue whales in the ocean.",Abundance estimation,1,0
3,14924067,"An academic discipline or academic field is a subdivision of knowledge that is taught and researched at the college or university level. Disciplines are defined (in part) and recognized by the academic journals in which research is published, and the learned societies and academic departments or faculties within colleges and universities to which their practitioners belong. Academic disciplines are conventionally divided into the humanities, including language, art and cultural studies, and the scientific disciplines, such as physics, chemistry, and biology; the social sciences are sometimes considered a third category.
Individuals associated with academic disciplines are commonly referred to as experts or specialists. Others, who may have studied liberal arts or systems theory rather than concentrating in a specific academic discipline, are classified as generalists.
While academic disciplines in and of themselves are more or less focused practices, scholarly approaches such as multidisciplinarity/interdisciplinarity, transdisciplinarity, and cross-disciplinarity integrate aspects from multiple academic disciplines, therefore addressing any problems that may arise from narrow concentration within specialized fields of study. For example, professionals may encounter trouble communicating across academic disciplines because of differences in language, specified concepts, or methodology.
Some researchers believe that academic disciplines may, in the future, be replaced by what is known as Mode 2 or ""post-academic science"", which involves the acquisition of cross-disciplinary knowledge through the collaboration of specialists from various academic disciplines.
It is also known as a field of study, field of inquiry, research field and branch of knowledge. The different terms are used in different countries and fields.",Academic discipline,1,1
4,15704862,"In the statistical area of survival analysis, an accelerated failure time model  (AFT model) is a parametric model that provides an alternative to the commonly used proportional hazards models. Whereas a proportional hazards model assumes that the effect of a covariate is to multiply the hazard by some constant, an AFT model assumes that the effect of a covariate is to accelerate or decelerate the life course of a disease by some constant. This is especially appealing in a technical context where the 'disease' is a result of some mechanical process with a known sequence of intermediary stages.",Accelerated failure time model,1,0
5,189373,"Actuarial science is the discipline that applies mathematical and statistical methods to assess risk in insurance, pension, finance, investment and other industries and professions. More generally, actuaries apply rigorous mathematics to model matters of uncertainty.
Actuaries are professionals trained in this discipline. In many countries, actuaries must demonstrate their competence by passing a series of rigorous professional examinations.
Actuarial science includes a number of interrelated subjects, including mathematics, probability theory, statistics, finance, economics, financial accounting and computer science. Historically, actuarial science used deterministic models in the construction of tables and premiums. The science has gone through revolutionary changes since the 1980s due to the proliferation of high speed computers and the union of stochastic actuarial models with modern financial theory.Many universities have undergraduate and graduate degree programs in actuarial science. In 2010, a study published by job search website CareerCast ranked actuary as the #1 job in the United States. The study used five key criteria to rank jobs: environment, income, employment outlook, physical demands, and stress. A similar study by U.S. News & World Report in 2006 included actuaries among the 25 Best Professions that it expects will be in great demand in the future.",Actuarial science,1,0
6,65297106,"In an adaptive design of a clinical trial, the parameters and conduct of the trial for a candidate drug or vaccine may be changed based on an interim analysis. Adaptive design typically involves advanced statistics to interpret a clinical trial endpoint. This is in contrast to traditional randomized clinical trials (RCTs) that are static in their protocol and do not modify any parameters until the trial is completed. The adaptation process generally continues throughout the trial, as prescribed in the trial protocol. Adaptions may include modifications to: dosage, sample size, drug undergoing trial, patient selection criteria and/or ""cocktail"" mix. The PANDA (A Practical Adaptive & Novel Designs and Analysis toolkit) provides comprehensive summary of different adaptive designs. In some cases, trials have become an ongoing process that regularly adds and drops therapies and patient groups as more information is gained. Importantly, the trial protocol is set before the trial begins which pre-specifies the adaptation schedule and processes.

 

",Adaptive design (medicine),1,0
7,49695,Adrien-Marie Legendre (; French: [adʁiɛ̃ maʁi ləʒɑ̃dʁ]; 18 September 1752 – 9 January 1833) was a French mathematician who made numerous contributions to mathematics. Well-known and important concepts such as the Legendre polynomials and Legendre transformation are named after him.,Adrien-Marie Legendre,1,0
8,690512,"The Akaike information criterion (AIC) is an estimator of prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection.
AIC is founded on information theory. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.
In estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.
The Akaike information criterion is named after the Japanese statistician Hirotugu Akaike, who formulated it. It now forms the basis of a paradigm for the foundations of statistics and is also widely used for statistical inference.",Akaike information criterion,1,0
9,2747204,"Abu ‘Abd ar-Raḥmān al-Khalīl ibn Aḥmad ibn ‘Amr ibn Tammām al-Farāhīdī al-Azdī al-Yaḥmadī (Arabic: أبو عبدالرحمن الخليل بن أحمد الفراهيدي; 718 – 786 CE), known as Al-Farāhīdī,  or Al-Khalīl, was an Arab philologist, lexicographer and leading grammarian of Basra based on Iraq. He made the first dictionary of the Arabic language – and the oldest extant dictionary – Kitab al-'Ayn (Arabic: كتاب العين ""The Source"") – introduced the now standard harakat (vowel marks in Arabic script) system, and was instrumental in the early development of ʿArūḍ (study of prosody), musicology and poetic metre. His linguistic theories influenced the development of Persian, Turkish, Kurdish and Urdu prosody. The ""Shining Star"" of the Basran school of Arabic grammar, a polymath and scholar, he was a man of genuinely original thought.Al-Farahidi was the first scholar to subject the prosody of Classical Arabic poetry to a detailed phonological analysis. The primary data he listed and categorized in meticulous detail was extremely complex to master and utilize, and later theorists have developed simpler formulations with greater coherence and general utility. He was also a pioneer in the field of cryptography, and influenced the work of Al-Kindi.",Al-Khalil ibn Ahmad al-Farahidi,1,0
10,358,"Algeria, officially the People's Democratic Republic of Algeria, is a country in North Africa. Algeria is bordered to the northeast by Tunisia; to the east by Libya; to the southeast by Niger; to the southwest by Mali, Mauritania, and Western Sahara; to the west by Morocco; and to the north by the Mediterranean Sea. It is considered to be a part of the Maghreb region of North Africa. It has a semi-arid geography, with most of the population living in the fertile north and the Sahara dominating the geography of the south. Algeria covers an area of 2,381,741 square kilometres (919,595 sq mi), making it the world's tenth largest nation by area, and the largest nation in Africa. With a population of 44 million, Algeria is the ninth-most populous country in Africa, and the 32nd-most populous country in the world. The capital and largest city is Algiers, located in the far north on the Mediterranean coast.
Algeria produced and is linked to many civilizations, empires and dynasties, including ancient Numidians, Phoenicians, Carthaginians, Romans, Vandals, Byzantines, Umayyads, Abbasids, Rustamids, Idrisids, Aghlabids, Fatimids, Zirids, Hammadids, Almoravids, Almohads, Zayyanids, Spaniards, Ottomans and the French colonial empire. The vast majority of Algeria's population is Arab-Berber, practicing Islam, and using the official languages of Arabic and Berber. However, French serves as an administrative and educational language in some contexts. The main spoken language is Algerian Arabic.
Algeria is a semi-presidential republic, with local constituencies consisting of 58 provinces and 1,541 communes. Algeria is a regional power in North Africa, and a middle power in global affairs. It has the highest Human Development Index of all non-island African countries and one of the largest economies on the continent, based largely on energy exports. Algeria has the world's sixteenth-largest oil reserves and the ninth-largest reserves of natural gas. Sonatrach, the national oil company, is the largest company in Africa, supplying large amounts of natural gas to Europe. Algeria's military is one of the largest in Africa, and has the largest defence budget on the continent. It is a member of the African Union, the Arab League, the OIC, OPEC, the United Nations, and the Arab Maghreb Union, of which it is a founding member.

",Algeria,1,0
11,1997,"Algebraic geometry is a branch of mathematics, classically studying zeros of multivariate polynomials. Modern algebraic geometry is based on the use of abstract algebraic techniques, mainly from commutative algebra, for solving  geometrical problems about these sets of zeros.
The fundamental objects of study in algebraic geometry are algebraic varieties, which are geometric manifestations of solutions of systems of polynomial equations. Examples of the most studied classes of algebraic varieties are: plane algebraic curves, which include lines, circles, parabolas, ellipses, hyperbolas, cubic curves like elliptic curves, and quartic curves like lemniscates and Cassini ovals. A point of the plane belongs to an algebraic curve if its coordinates satisfy a given polynomial equation. Basic questions involve the study of the points of special interest like the singular points, the inflection points and the points at infinity. More advanced questions involve the topology of the curve and relations between the curves given by different equations.
Algebraic geometry occupies a central place in modern mathematics and has multiple conceptual connections with such diverse fields as complex analysis, topology and number theory. Initially a study of systems of polynomial equations in several variables, the subject of algebraic geometry starts where equation solving leaves off, and it becomes even more important to understand the intrinsic properties of the totality of solutions of a system of equations, than to find a specific solution; this leads into some of the deepest areas in all of mathematics, both conceptually and in terms of technique.
In the 20th century, algebraic geometry split into several subareas.

The mainstream of algebraic geometry is devoted to the study of the complex points of the algebraic varieties and more generally to the points with coordinates in an algebraically closed field.
Real algebraic geometry is the study of the real points of an algebraic variety.
Diophantine geometry and, more generally, arithmetic geometry is the study of the points of an algebraic variety with coordinates in fields that are not algebraically closed and occur in algebraic number theory, such as the field of rational numbers, number fields, finite fields, function fields, and p-adic fields.
A large part of singularity theory is devoted to the singularities of algebraic varieties.
Computational algebraic geometry is an area that has emerged at the intersection of algebraic geometry and computer algebra, with the rise of computers. It consists mainly of algorithm design and software development for the study of properties of explicitly given algebraic varieties.Much of the development of the mainstream of algebraic geometry in the 20th century occurred within an abstract algebraic framework, with increasing emphasis being placed on ""intrinsic"" properties of algebraic varieties not dependent on any particular way of embedding the variety in an ambient coordinate space; this parallels developments in topology, differential and complex geometry. One key achievement of this abstract algebraic geometry is Grothendieck's scheme theory which allows one to use sheaf theory to study algebraic varieties in a way which is very similar to its use in the study of differential and analytic manifolds. This is obtained by extending the notion of point: In classical algebraic geometry, a point of an affine variety may be identified, through Hilbert's Nullstellensatz, with a maximal ideal of the coordinate ring, while the points of the corresponding affine scheme are all prime ideals of this ring. This means that a point of such a scheme may be either a usual point or a subvariety. This approach also enables a unification of the language and the tools of classical algebraic geometry, mainly concerned with complex points, and of algebraic number theory. Wiles' proof of the longstanding conjecture called Fermat's Last Theorem is an example of the power of this approach.",Algebraic geometry,1,0
12,174705,"Algebraic number theory is a branch of number theory that uses the techniques of abstract algebra to study the integers, rational numbers, and their generalizations. Number-theoretic questions are expressed in terms of properties of algebraic objects such as algebraic number fields and their rings of integers, finite fields, and function fields. These properties, such as whether a ring admits unique factorization, the behavior of ideals, and the Galois groups of fields, can resolve questions of primary importance in number theory, like the existence of solutions to Diophantine equations.",Algebraic number theory,1,0
13,38801,"Algebraic topology is a branch of mathematics that uses tools from abstract algebra to study topological spaces. The basic goal is to find algebraic invariants that classify topological spaces up to homeomorphism, though usually most classify up to homotopy equivalence.
Although algebraic topology primarily uses algebra to study topological problems, using topology to solve algebraic problems is sometimes also possible. Algebraic topology, for example, allows for a convenient proof that any subgroup of a free group is again a free group.

",Algebraic topology,1,0
14,775,"In mathematics and computer science, an algorithm ( (listen)) is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation. Algorithms are used as specifications for performing calculations and data processing. By making use of artificial intelligence, algorithms can perform automated deductions (referred to as automated reasoning) and use mathematical and logical tests to divert the code execution through various routes (referred to as automated decision-making). Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as ""memory"", ""search"" and ""stimulus"".In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, especially in problem domains where there is no well-defined correct or optimal result.As an effective method, an algorithm can be expressed within a finite amount of space and time, and in a well-defined formal language for calculating a function. Starting from an initial state and initial input (perhaps empty), the instructions describe a computation that, when executed, proceeds through a finite number of well-defined successive states, eventually producing ""output"" and terminating at a final ending state. The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

",Algorithm,1,1
15,645892,"In statistical hypothesis testing, the alternative hypothesis is one of the proposed proposition in the hypothesis test. In general the goal of hypothesis test is to demonstrate that in the given condition, there is sufficient evidence supporting the credibility of alternative hypothesis instead of the exclusive proposition in the test  (null hypothesis). It is usually consistent with the research hypothesis because it is constructed from literature review, previous studies, etc.  However, the research hypothesis is sometimes consistent with the null hypothesis.
In statistics, alternative hypothesis is often denoted as Ha or H1. Hypotheses are formulated to compare in a statistical hypothesis test.
In the domain of inferential statistics two rival hypotheses can be compared by explanatory power and predictive power.

",Alternative hypothesis,1,0
16,634,"Analysis of variance (ANOVA) is a collection of statistical models and their associated estimation procedures (such as the ""variation"" among and between groups) used to analyze the differences among means. ANOVA was developed by the statistician Ronald Fisher. ANOVA is based on the law of total variance, where the observed variance in a particular variable is partitioned into components attributable to different sources of variation. In its simplest form, ANOVA provides a statistical test of whether two or more population means are equal, and therefore generalizes the t-test beyond two means. In other words, the ANOVA is used to test the difference between two or more means.",Analysis of variance,1,1
17,404926,"Analysis of covariance (ANCOVA) is a general linear model which blends ANOVA and regression. ANCOVA evaluates whether the means of a dependent variable (DV) are equal across levels of a categorical independent variable (IV) often called a treatment, while statistically controlling for the effects of other continuous variables that are not of primary interest, known as covariates (CV) or nuisance variables. Mathematically, ANCOVA decomposes the variance in the DV into variance explained by the CV(s), variance explained by the categorical IV, and residual variance. Intuitively, ANCOVA can be thought of as 'adjusting' the DV by the group means of the CV(s).The ANCOVA model assumes a linear relationship between the response (DV) and covariate (CV):

  
    
      
        
          y
          
            i
            j
          
        
        =
        μ
        +
        
          τ
          
            i
          
        
        +
        
          B
        
        (
        
          x
          
            i
            j
          
        
        −
        
          
            x
            ¯
          
        
        )
        +
        
          ϵ
          
            i
            j
          
        
        .
      
    
    {\displaystyle y_{ij}=\mu +\tau _{i}+\mathrm {B} (x_{ij}-{\overline {x}})+\epsilon _{ij}.}
  
In this equation, the DV, 
  
    
      
        
          y
          
            i
            j
          
        
      
    
    {\displaystyle y_{ij}}
   is the jth observation under the ith categorical group; the CV, 
  
    
      
        
          x
          
            i
            j
          
        
      
    
    {\displaystyle x_{ij}}
   is the jth observation of the covariate under the ith group. Variables in the model that are derived from the observed data are 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   (the grand mean) and 
  
    
      
        
          
            x
            ¯
          
        
      
    
    {\displaystyle {\overline {x}}}
   (the global mean for covariate 
  
    
      
        x
      
    
    {\displaystyle x}
  ). The variables to be fitted are 
  
    
      
        
          τ
          
            i
          
        
      
    
    {\displaystyle \tau _{i}}
   (the effect of the ith level of the IV), 
  
    
      
        B
      
    
    {\displaystyle B}
   (the slope of the line) and 
  
    
      
        
          ϵ
          
            i
            j
          
        
      
    
    {\displaystyle \epsilon _{ij}}
   (the associated unobserved error term for the jth observation in the ith group).
Under this specification, the categorical treatment effects sum to zero 
  
    
      
        
          (
          
            
              ∑
              
                i
              
              
                a
              
            
            
              τ
              
                i
              
            
            =
            0
          
          )
        
        .
      
    
    {\displaystyle \left(\sum _{i}^{a}\tau _{i}=0\right).}
   The standard assumptions of the linear regression model are also assumed to hold, as discussed below.

",Analysis of covariance,1,0
19,2202,"In classical mathematics, analytic geometry, also known as coordinate geometry or Cartesian geometry, is the study of geometry using a coordinate system. This contrasts with synthetic geometry.
Analytic geometry is used in physics and engineering, and also in aviation, rocketry, space science, and spaceflight. It is the foundation of most modern fields of geometry, including algebraic, differential, discrete and computational geometry.
Usually the Cartesian coordinate system is applied to manipulate equations for planes, straight lines, and circles, often in two and sometimes three dimensions. Geometrically, one studies the Euclidean plane (two dimensions) and Euclidean space. As taught in school books, analytic geometry can be explained more simply: it is concerned with defining and representing geometric shapes in a numerical way and extracting numerical information from shapes' numerical definitions and representations. That the algebra of the real numbers can be employed to yield results about the linear continuum of geometry relies on the Cantor–Dedekind axiom.",Analytic geometry,1,0
20,251513,"In mathematics, analytic number theory is a branch of number theory that uses methods from mathematical analysis to solve problems about the integers. It is often said to have begun with Peter Gustav Lejeune Dirichlet's 1837 introduction of Dirichlet L-functions to give the first proof of Dirichlet's theorem on arithmetic progressions. It is well known for its results on prime numbers (involving the Prime Number Theorem and Riemann zeta function) and additive number theory (such as the Goldbach conjecture and Waring's problem).",Analytic number theory,1,0
21,670398,"An ancillary statistic is a measure of a sample whose distribution (or whose pmf or pdf) does not depend on the parameters of the model. An ancillary statistic is a pivotal quantity that is also a statistic. Ancillary statistics can be used to construct prediction intervals.
This concept was introduced by Ronald Fisher in the 1920s.",Ancillary statistic,1,0
22,3479881,"The Anderson–Darling test is a statistical test of whether a given sample of data is drawn from a given probability distribution. In its basic form, the test assumes that there are no parameters to be estimated in the distribution being tested, in which case the test and its set of critical values is distribution-free. However, the test is most often used in contexts where a family of distributions is being tested, in which case the parameters of that family need to be estimated and account must be taken of this in adjusting either the test-statistic or its critical values.  When applied to testing whether a normal distribution adequately describes a set of data, it is one of the most powerful statistical tools for detecting most departures from normality.K-sample Anderson–Darling tests are available for testing whether several collections of observations can be modelled as coming from a single population, where the distribution function does not have to be specified.
In addition to its use as a test of fit for distributions, it can be used in parameter estimation as the basis for a form of minimum distance estimation procedure.
The test is named after Theodore Wilbur Anderson (1918–2016) and Donald A. Darling (1915–2014), who invented it in 1952.",Anderson–Darling test,1,0
23,24295969,"Applied mathematics is the application of mathematical methods by different fields such as physics, engineering, medicine, biology, finance, business, computer science, and industry. Thus, applied mathematics is a combination of mathematical science and specialized knowledge. The term ""applied mathematics"" also describes the professional specialty in which mathematicians work on practical problems by formulating and studying mathematical models.
In the past, practical applications have motivated the development of mathematical theories, which then became the subject of study in pure mathematics where abstract concepts are studied for their own sake. The activity of applied mathematics is thus intimately connected with research in pure mathematics.",Applied mathematics,1,0
24,803,"Arabic (اَلْعَرَبِيَّةُ, al-ʿarabiyyah [al ʕaraˈbijːa] (listen) or عَرَبِيّ, ʿarabīy [ˈʕarabiː] (listen) or [ʕaraˈbij]) is a Semitic language that first emerged in the 1st to 4th centuries CE. It is the lingua franca of the Arab world and the liturgical language of Islam. It is named after the Arabs, a term initially used to describe peoples living in the Arabian Peninsula bounded by eastern Egypt in the west, Mesopotamia in the east, and the Anti-Lebanon mountains and northern Syria in the north, as perceived by ancient Greek geographers. The ISO assigns language codes to 32 varieties of Arabic, including its standard form, Modern Standard Arabic, also referred to as Literary Arabic, which is modernized Classical Arabic. This distinction exists primarily among Western linguists; Arabic speakers themselves generally do not distinguish between Modern Standard Arabic and Classical Arabic, but rather refer to both as al-ʿarabiyyatu l-fuṣḥā (اَلعَرَبِيَّةُ ٱلْفُصْحَىٰ ""the eloquent Arabic"") or simply al-fuṣḥā (اَلْفُصْحَىٰ).
Arabic is widely taught in schools and universities around the world and is used to varying degrees in workplaces, governments and the media. Arabic, in its Modern Standard Arabic form, is an official language of 26 states and 1 disputed territory, the third most after English and French;
it is also the liturgical language of the religion of Islam, since the Quran and the Hadiths were written in Classical Arabic.During the early Middle Ages, Arabic was a major vehicle of culture in the Mediterranean region, especially in science, mathematics and philosophy. As a result, many European languages have also borrowed many words from it. Arabic influence, mainly in vocabulary, is seen in European languages—mainly Spanish and to a lesser extent Portuguese, Catalan, and Sicilian—owing to both the proximity of Christian European and Muslim Arabized civilizations and the long-lasting Muslim culture and Arabic language presence, mainly in Southern Iberia, during the Al-Andalus era. For example, ""Algebra"" comes from the Arabic word ""al-jabr"", which was then transferred to Middle English. The Maltese language is a Semitic language developed from a dialect of Arabic and written in the Latin alphabet. The Balkan languages, including Greek and Bulgarian, have also acquired a significant number of words of Arabic origin through contact with Ottoman Turkish.
Arabic has influenced many other languages around the globe throughout its history especially languages of Muslim cultures and countries that were conquered by Muslims. Some of the most influenced languages are Persian, Turkish, Hindustani (Hindi and Urdu), Kashmiri, Kurdish, Bosnian, Kazakh, Bengali, Malay (Indonesian and Malaysian), Maldivian, Pashto, Punjabi, Albanian, Armenian, Azerbaijani, Sicilian, Spanish, Greek, Bulgarian, Tagalog, Sindhi, Odia Hebrew and Hausa and some languages in parts of Africa (e.g. Swahili, Somali). Conversely, Arabic has borrowed words from other languages, including Aramaic as well as Hebrew, Latin, Greek, Persian and to a lesser extent Turkish (due to the Ottoman Empire), English and French (due to their colonization of the Levant) and other Semitic languages such as Abyssinian.
Arabic is the liturgical language of more than 2 billion Muslims, and Arabic is one of six official languages of the United Nations. All varieties of Arabic combined are spoken by perhaps as many as 422 million speakers (native and non-native) in the Arab world, making it the fifth most spoken language in the world, and the fourth most used language on the internet in terms of users. In 2011, Bloomberg Businessweek ranked Arabic the fourth most useful language for business, after English, Standard Mandarin Chinese, and French. Arabic is written with the Arabic alphabet, which is an abjad script and is written from right to left, although the spoken varieties are sometimes written in ASCII Latin from left to right with no standardized orthography.",Arabic,1,0
25,3118,"Arithmetic (from Ancient Greek  ἀριθμός (arithmós) 'number', and  τική [τέχνη] (tikḗ [tékhnē]) 'art, craft') is an elementary part of mathematics that consists of the study of the properties of the traditional operations on numbers—addition, subtraction, multiplication, division, exponentiation, and extraction of roots. In the 19th century, Italian mathematician Giuseppe Peano formalized arithmetic with his Peano axioms, which are highly important to the field of mathematical logic today. 

",Arithmetic,1,0
26,1973177,"In mathematics, arithmetic geometry is roughly the application of techniques from algebraic geometry to problems in number theory. Arithmetic geometry is centered around Diophantine geometry, the study of rational points of algebraic varieties.In more abstract terms, arithmetic geometry can be defined as the study of schemes of finite type over the spectrum of the ring of integers.

",Arithmetic geometry,1,0
27,612,"In mathematics and statistics, the arithmetic mean (  air-ith-MET-ik) or arithmetic average, or just the mean or the average (when the context is clear), is the sum of a collection of numbers divided by the count of numbers in the collection. The collection is often a set of results of an experiment or an observational study, or frequently a set of results from a survey. The term ""arithmetic mean"" is preferred in some contexts in mathematics and statistics, because it helps distinguish it from other means, such as the geometric mean and the harmonic mean.
In addition to mathematics and statistics, the arithmetic mean is used frequently in many diverse fields such as economics, anthropology and history, and it is used in almost every academic field to some extent. For example, per capita income is the arithmetic average income of a nation's population.
While the arithmetic mean is often used to report central tendencies, it is not a robust statistic, meaning that it is greatly influenced by outliers (values that are very much larger or smaller than most of the values). For skewed distributions, such as the distribution of income for which a few people's incomes are substantially greater than most people's, the arithmetic mean may not coincide with one's notion of ""middle"", and robust statistics, such as the median, may provide better description of central tendency.",Arithmetic mean,1,0
28,21523,"Artificial neural networks (ANNs), usually simply called neural networks (NNs) or, more simply yet, neural nets, are computing systems inspired by the biological neural networks that constitute animal brains.

An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. An artificial neuron receives signals then processes them and can signal neurons connected to it. The ""signal"" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. The connections are called edges. Neurons and edges typically have a weight that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.",Artificial neural network,1,1
29,1146,"An assembly line is a manufacturing process (often called a progressive assembly) in which parts (usually interchangeable parts) are added as the semi-finished assembly moves from workstation to workstation where the parts are added in sequence until the final assembly is produced. By mechanically moving the parts to the assembly work and moving the semi-finished assembly from work station to work station, a finished product can be assembled faster and with less labor than by having workers carry parts to a stationary piece for assembly.
Assembly lines are common methods of assembling complex items such as automobiles and other transportation equipment, household appliances and electronic goods.
Workers in charge of the works of assembly line are called assemblers.",Assembly line,1,0
30,26685,"Statistics is the discipline that concerns the collection, organization, analysis, interpretation, and presentation of data. In applying statistics to a scientific, industrial, or social problem, it is conventional to begin with a statistical population or a statistical model to be studied. Populations can be diverse groups of people or objects such as ""all people living in a country"" or ""every atom composing a crystal"". Statistics deals with every aspect of data, including the planning of data collection in terms of the design of surveys and experiments.When census data cannot be collected, statisticians collect data by developing specific experiment designs and survey samples. Representative sampling assures that inferences and conclusions can reasonably extend from the sample to the population as a whole. An experimental study involves taking measurements of the system under study, manipulating the system, and then taking additional measurements using the same procedure to determine if the manipulation has modified the values of the measurements. In contrast, an observational study does not involve experimental manipulation.
Two main statistical methods are used in data analysis: descriptive statistics, which summarize data from a sample using indexes such as the mean or standard deviation, and inferential statistics, which draw conclusions from data that are subject to random variation (e.g., observational errors, sampling variation). Descriptive statistics are most often concerned with two sets of properties of a distribution (sample or population): central tendency (or location) seeks to characterize the distribution's central or typical value, while dispersion (or variability) characterizes the extent to which members of the distribution depart from its center and each other. Inferences on mathematical statistics are made under the framework of probability theory, which deals with the analysis of random phenomena.
A standard statistical procedure involves the collection of data leading to test of the relationship between two statistical data sets, or a data set and synthetic data drawn from an idealized model. A hypothesis is proposed for the statistical relationship between the two data sets, and this is compared as an alternative to an idealized null hypothesis of no relationship between two data sets. Rejecting or disproving the null hypothesis is done using statistical tests that quantify the sense in which the null can be proven false, given the data that are used in the test. Working from a null hypothesis, two basic forms of error are recognized: Type I errors (null hypothesis is falsely rejected giving a ""false positive"") and Type II errors (null hypothesis fails to be rejected and an actual relationship between populations is missed giving a ""false negative""). Multiple problems have come to be associated with this framework, ranging from obtaining a sufficient sample size to specifying an adequate null hypothesis.Measurement processes that generate statistical data are also subject to error. Many of these errors are classified as random (noise) or systematic (bias), but other types of errors (e.g., blunder, such as when an analyst reports incorrect units) can also occur. The presence of missing data or censoring may result in biased estimates and specific techniques have been developed to address these problems.",Statistics,1,1
31,37815827,"Astrostatistics is a discipline which spans astrophysics, statistical analysis and data mining. It is used to process the vast amount of data produced by automated scanning of the cosmos, to characterize complex datasets, and to link astronomical data to astrophysical theory.  Many branches of statistics are involved in astronomical analysis including nonparametrics, multivariate regression and multivariate classification, time series analysis, and especially Bayesian inference. The field is closely related to Astroinformatics.",Astrostatistics,1,0
32,26024794,"In statistics, asymptotic theory, or large sample theory, is a framework for assessing properties of estimators and statistical tests. Within this framework, it is often assumed that the sample size n may grow indefinitely; the properties of estimators and tests are then evaluated under the limit of n → ∞. In practice, a limit evaluation is considered to be approximately valid for large finite sample sizes too.",Asymptotic theory (statistics),1,0
33,167897,"Editing is the process of selecting and preparing written, photographic, visual, audible, or cinematic material used by a person or an entity to convey a message or information. The editing process can involve correction, condensation, organisation, and many other modifications performed with an intention of producing a correct, consistent, accurate and complete piece of work.The editing process often begins with the author's idea for the work itself, continuing as a collaboration between the author and the editor as the work is created. Editing can involve creative skills, human relations and a precise set of methods.

There are various editorial positions in publishing. Typically, one finds editorial assistants reporting to the senior-level editorial staff and directors who report to senior executive editors. Senior executive editors are responsible for developing a product for its final release. The smaller the publication, the more these roles overlap.
The top editor at many publications may be known as the chief editor, executive editor, or simply the editor. A frequent and highly regarded contributor to a magazine may acquire the title of editor-at-large or contributing editor. Mid-level newspaper editors often manage or help to manage sections, such as business, sports and features. In U.S. newspapers, the level below the top editor is usually the managing editor.
In the book publishing industry, editors may organize anthologies and other compilations, produce definitive editions of a classic author's works (scholarly editor), and organize and manage contributions to a multi-author book (symposium editor or volume editor). Obtaining manuscripts or recruiting authors is the role of an acquisitions editor or a commissioning editor in a publishing house. Finding marketable ideas and presenting them to appropriate authors are the responsibilities of a sponsoring editor.
Copy editors correct spelling, grammar and align writings to house style. Changes to the publishing industry since the 1980s have resulted in nearly all copy editing of book manuscripts being outsourced to freelance copy editors.At newspapers and wire services, press or copy editors write headlines and work on more substantive issues, such as ensuring accuracy, fairness, and taste. In some positions, they design pages and select news stories for inclusion. At U.K. and Australian newspapers, the term is sub-editor. They may choose the layout of the publication and communicate with the printer. These editors may have the title of layout or design editor or (more so in the past) makeup editor.

",Editing,1,0
34,2724,"Autocorrelation, sometimes known as serial correlation in the discrete time case, is the correlation of a signal with a delayed copy of itself as a function of delay. Informally, it is the similarity between observations as a function of the time lag between them. The analysis of autocorrelation is a mathematical tool for finding repeating patterns, such as the presence of a periodic signal obscured by noise, or identifying the missing fundamental frequency  in a signal implied by its harmonic frequencies. It is often used in signal processing for analyzing functions or series of values, such as time domain signals.
Different fields of study define autocorrelation differently, and not all of these definitions are equivalent. In some fields, the term is used interchangeably with autocovariance.
Unit root processes, trend-stationary processes, autoregressive processes, and moving average processes are specific forms of processes with autocorrelation.",Autocorrelation,1,0
35,610752,"In econometrics, the autoregressive conditional heteroskedasticity (ARCH) model is a statistical model for time series data that describes the variance of the current error term or innovation as a function of the actual sizes of the previous time periods' error terms; often the variance is related to the squares of the previous innovations. The ARCH model is appropriate when the error variance in a time series follows an autoregressive (AR) model; if an autoregressive moving average (ARMA) model is assumed for the error variance, the model is a generalized autoregressive conditional heteroskedasticity (GARCH) model.ARCH models are commonly employed in modeling financial time series that exhibit time-varying volatility and volatility clustering, i.e. periods of swings interspersed with periods of relative calm. ARCH-type models are sometimes considered to be in the family of stochastic volatility models, although this is strictly incorrect since at time t the volatility is completely pre-determined (deterministic) given previous values.

",Autoregressive conditional heteroskedasticity,1,0
36,764848,"In the statistical analysis of time series, autoregressive–moving-average (ARMA) models provide a parsimonious description of a (weakly) stationary stochastic process in terms of two polynomials, one for the autoregression (AR) and the second for the moving average (MA). The general ARMA model was described in the 1951 thesis of Peter Whittle, Hypothesis testing in time series analysis, and it was popularized in the 1970 book by George E. P. Box and Gwilym Jenkins.
Given a time series of data Xt , the ARMA model is a tool for understanding and, perhaps, predicting future values in this series. The AR part involves regressing the variable on its own lagged (i.e., past) values. The MA part involves modeling the error term as a linear combination of error terms occurring contemporaneously and at various times in the past. The model is usually referred to as the ARMA(p,q) model where p is the order of the AR part and q is the order of the MA part (as defined below).
ARMA models can be estimated by using the Box–Jenkins method.

",Autoregressive–moving-average model,1,0
37,344242,"The average absolute deviation (AAD) of a data set is the average of the absolute deviations from a central point. It is a summary statistic of statistical dispersion or variability. In the general form, the central point can be a mean, median, mode, or the result of any other measure of central tendency or any reference value related to the given data set. 
AAD includes the mean absolute deviation and the median absolute deviation (both abbreviated as MAD).",Average absolute deviation,1,0
38,14527587,"The average treatment effect (ATE) is a measure used to compare treatments (or interventions) in randomized experiments, evaluation of policy interventions, and medical trials. The ATE measures the difference in mean (average) outcomes between units assigned to the treatment and units assigned to the control.  In a randomized trial (i.e., an experimental study), the average treatment effect can be estimated from a sample using a comparison in mean outcomes for treated and untreated units. However, the ATE is generally understood as a causal parameter (i.e., an estimate or property of a population) that a researcher desires to know, defined without reference to the study design or estimation procedure. Both observational studies and experimental study designs with random assignment may enable one to estimate an ATE in a variety of ways.",Average treatment effect,1,0
39,393311,"A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally.  A vertical bar chart is sometimes called a column chart.
A bar graph shows comparisons among discrete categories. One axis of the chart shows the specific categories being compared, and the other axis represents a measured value. Some bar graphs present bars clustered in groups of more than one, showing the values of more than one measured variable.",Bar chart,1,0
40,3797,"Baseball statistics play an important role in evaluating the progress of a player or team.
Since the flow of a baseball game has natural breaks to it, and normally players act individually rather than performing in clusters, the sport lends itself to easy record-keeping and statistics. Statistics have been kept for professional baseball since the creation of the National League and American League, now part of Major League Baseball.
Many statistics are also available from outside Major League Baseball, from leagues such as the National Association of Professional Base Ball Players and the Negro leagues, although the consistency of whether these records were kept, of the standards with respect to which they were calculated, and of their accuracy has varied.",Baseball statistics,1,0
41,6978815,"In estimation theory and decision theory, a Bayes estimator or a Bayes action is an estimator or decision rule that minimizes the posterior expected value of a loss function (i.e., the posterior expected loss). Equivalently, it maximizes the posterior expectation of a utility function. An alternative way of formulating an estimator within Bayesian statistics is maximum a posteriori estimation.",Bayes estimator,1,0
42,824552,"The Bayes factor is a ratio of the marginal likelihood of two competing statistical models, usually a null and an alternative, with the aim of quantifying the support for one model over the other. Oftentimes the models in question have a common set of parameters, but this is not necessary. The Bayes factor can be thought of as a Bayesian analog to the likelihood-ratio test, but since it uses the (integrated) marginal likelihood instead of the maximized likelihood, both tests only coincide under simple hypotheses (e.g., two specific parameter values). Also, in contrast with null hypothesis significance testing, Bayes factors support evaluation of evidence in favor of a null hypothesis, rather than only allowing the null to be rejected or not rejected.Although conceptually simple, the computation of the Bayes factor can be challenging depending on the complexity of the model and the hypotheses. Since closed-form expressions of the marginal likelihood are generally not available, numerical approximations based on MCMC samples have been suggested. For certain special cases, simplified algebraic expressions can be derived; for instance, the Savage–Dickey density ratio in the case of a precise (equality constrained) hypothesis against an unrestricted alternative. Another approximation, derived by applying Laplace's method to the integrated likelihoods, is known as the Bayesian information criterion (BIC); in large data sets the Bayes factor will approach the BIC as the influence of the priors wanes. In small data sets, priors generally matter and may not be improper since the Bayes factor will be undefined if either of the two integrals in its ratio is not finite.

",Bayes factor,1,0
43,49571,"Bayesian inference is a method of statistical inference in which Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. Bayesian inference is an important technique in statistics, and especially in mathematical statistics. Bayesian updating is particularly important in the dynamic analysis of a sequence of data. Bayesian inference has found application in a wide range of activities, including science, engineering, philosophy, medicine, sport, and law. In the philosophy of decision theory, Bayesian inference is closely related to subjective probability, often called ""Bayesian probability"".",Bayesian inference,1,0
44,2473272,"In statistics, the Bayesian information criterion (BIC) or Schwarz information criterion (also SIC, SBC, SBIC) is a criterion for model selection among a finite set of models; models with lower BIC are generally preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).
When fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.The BIC was developed by Gideon E. Schwarz and published in a 1978 paper, where he gave a Bayesian argument for adopting it.",Bayesian information criterion,1,0
45,7519917,"Bayesian linear regression is a type of conditional modeling in which the mean of one variable (the regressand, generally labeled 
  
    
      
        y
      
    
    {\displaystyle y}
  ) is described by a linear combination of a set of other variables (the regressors, usually 
  
    
      
        X
      
    
    {\displaystyle X}
  ). After obtaining the posterior probability of the coefficients of this linear function, as well as other parameters describing the distribution of the regressand, the model then allows prediction of 
  
    
      
        y
      
    
    {\displaystyle y}
   conditional on observed values of 
  
    
      
        X
      
    
    {\displaystyle X}
  . The simplest and most widely used version of this model is the normal linear model, in which the distribution of 
  
    
      
        y
      
    
    {\displaystyle y}
   given 
  
    
      
        X
      
    
    {\displaystyle X}
   follows a normal distribution. In this model, and under a particular choice of prior probabilities for the parameters—so-called conjugate priors—the posterior can be found analytically, but generally posteriors have to be approximated.",Bayesian linear regression,1,0
46,203996,"A Bayesian network (also known as a Bayes network, Bayes net, belief network, or decision network) is a probabilistic graphical model that represents a set of variables and their conditional dependencies via a directed acyclic graph (DAG). Bayesian networks are ideal for taking an event that occurred and predicting the likelihood that any one of several possible known causes was the contributing factor.  For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases.
Efficient algorithms can perform inference and learning in Bayesian networks. Bayesian networks that model sequences of variables (e.g. speech signals or protein sequences) are called dynamic Bayesian networks. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called influence diagrams.",Bayesian network,1,1
47,4890,"Bayesian probability is an interpretation of the concept of probability, in which, instead of frequency or propensity of some phenomenon, probability is interpreted as reasonable expectation representing a state of knowledge or as quantification of a personal belief.The Bayesian interpretation of probability can be seen as an extension of propositional logic that enables reasoning with hypotheses; that is, with propositions whose truth or falsity is unknown. In the Bayesian view, a probability is assigned to a hypothesis, whereas under frequentist inference, a hypothesis is typically tested without being assigned a probability.
Bayesian probability belongs to the category of evidential probabilities; to evaluate the probability of a hypothesis, the Bayesian probabilist specifies a prior probability. This, in turn, is then updated to a posterior probability in the light of new, relevant data (evidence). The Bayesian interpretation provides a standard set of procedures and formulae to perform this calculation.
The term Bayesian derives from the 18th-century mathematician and theologian Thomas Bayes, who provided the first mathematical treatment of a non-trivial problem of statistical data analysis using what is now known as Bayesian inference.: 131  Mathematician Pierre-Simon Laplace pioneered and popularized what is now called Bayesian probability.: 97–98 ",Bayesian probability,1,0
48,404412,"Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability where probability expresses a degree of belief in an event. The degree of belief may be based on prior knowledge about the event, such as the results of previous experiments, or on personal beliefs about the event. This differs from a number of other interpretations of probability, such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials.Bayesian statistical methods use Bayes' theorem to compute and update probabilities after obtaining new data. Bayes' theorem describes the conditional probability of an event based on data as well as prior information or beliefs about the event or conditions related to the event. For example, in Bayesian inference, Bayes' theorem can be used to estimate the parameters of a probability distribution or statistical model. Since Bayesian statistics treats probability as a degree of belief, Bayes' theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters.Bayesian statistics is named after Thomas Bayes, who formulated a specific case of Bayes' theorem in a paper published in 1763. In several papers spanning from the late 18th to the early 19th centuries, Pierre-Simon Laplace developed the Bayesian interpretation of probability. Laplace used methods that would now be considered Bayesian to solve a number of statistical problems. Many Bayesian methods were developed by later authors, but the term was not commonly used to describe such methods until the 1950s. During much of the 20th century, Bayesian methods were viewed unfavorably by many statisticians due to philosophical and practical considerations. Many Bayesian methods required much computation to complete, and most methods that were widely used during the century were based on the frequentist interpretation. However, with the advent of powerful computers and new algorithms like Markov chain Monte Carlo, Bayesian methods have seen increasing use within statistics in the 21st century.",Bayesian statistics,1,0
49,64307014,Beth L. Chance (born 1968) is an American statistics educator. She is a professor of statistics at the California Polytechnic State University.,Beth Chance,1,0
50,184099,"Statistical bias is a systematic tendency which causes differences between results and facts. The bias exists in numbers of the process of data analysis, including the source of the data, the estimator chosen, and the ways the data was analyzed. Bias may have a serious impact on results, for example, to investigate people's buying habits. If the sample size is not large enough, the results may not be representative of the buying habits of all the people. That is, there may be discrepancies between the survey results and the actual results. Therefore, understanding the source of statistical bias can help to assess whether the observed results are close to the real results.
Bias can be differentiated from other mistakes such as accuracy (instrument failure/inadequacy), lack of data, or mistakes in transcription (typos). Bias implies that the data selection may have been skewed by the collection criteria.
Bias does not preclude the existence of any other mistakes. One may have a poorly designed sample, an inaccurate measurement device, and typos in recording data simultaneously. 
Also it is useful to recognize that the term “error” specifically refers to the outcome rather than the process (errors of rejection or acceptance of the hypothesis being tested). Use of flaw or mistake to differentiate procedural errors from these specifically defined outcome-based terms is recommended.",Bias (statistics),1,0
51,8450479,"In statistics, the bias (or bias function) of an estimator is the difference between this estimator's expected value and the true value of the parameter being estimated. An estimator or decision rule with zero bias is called unbiased. In statistics, ""bias"" is an objective property of an estimator. Bias can also be measured with respect to the median, rather than the mean (expected value), in which case one distinguishes median-unbiased from the usual mean-unbiasedness property. Bias is a distinct concept from consistency: consistent estimators converge in probability to the true value of the parameter, but may be biased or unbiased; see bias versus consistency for more.
All else being equal, an unbiased estimator is preferable to a biased estimator, although in practice, biased estimators (with generally small bias) are frequently used. When a biased estimator is used, bounds of the bias are calculated. A biased estimator may be used for various reasons: because an unbiased estimator does not exist without further assumptions about a population; because an estimator is difficult to compute (as in unbiased estimation of standard deviation); because an estimator is median-unbiased but not mean-unbiased (or the reverse); because a biased estimator gives a lower value of some loss function (particularly mean squared error) compared with unbiased estimators (notably in shrinkage estimators); or because in some cases being unbiased is too strong a condition, and the only unbiased estimators are not useful. 
Further, mean-unbiasedness is not preserved under non-linear transformations, though median-unbiasedness is (see § Effect of transformations); for example, the sample variance is a biased estimator for the population variance. These are all illustrated below.",Bias of an estimator,1,1
52,14092434,"The bibcode (also known as the refcode) is a compact identifier used by several astronomical data systems to uniquely specify literature references.

",Bibcode,1,1
53,27051151,"Big data refers to data sets that are too large or complex to be dealt with by traditional data-processing application software. Data with many fields (rows) offer greater statistical power, while data with higher complexity (more attributes or columns) may lead to a higher false discovery rate. Big data analysis challenges include capturing data, data storage, data analysis, search, sharing, transfer, visualization, querying, updating, information privacy, and data source. Big data was originally associated with three key concepts: volume, variety, and velocity. The analysis of big data presents challenges in sampling, and thus previously allowing for only observations and sampling. Thus a fourth concept, veracity, refers to the quality or insightfulness of the data. Without sufficient investment in expertise for big data veracity, then the volume and variety of data can produce costs and risks that exceed an organization's capacity to create and capture value from big data.Current usage of the term big data tends to refer to the use of predictive analytics, user behavior analytics, or certain other advanced data analytics methods that extract value from big data, and seldom to a particular size of data set. ""There is little doubt that the quantities of data now available are indeed large, but that's not the most relevant characteristic of this new data ecosystem.""
Analysis of data sets can find new correlations to ""spot business trends, prevent diseases, combat crime and so on"". Scientists, business executives, medical practitioners, advertising and governments alike regularly meet difficulties with large data-sets in areas including Internet searches, fintech, healthcare analytics, geographic information systems, urban informatics, and business informatics. Scientists encounter limitations in e-Science work, including meteorology, genomics, connectomics, complex physics simulations, biology, and environmental research.The size and number of available data sets have grown rapidly as data is collected by devices such as mobile devices, cheap and numerous information-sensing Internet of things devices, aerial (remote sensing), software logs, cameras, microphones, radio-frequency identification (RFID) readers and wireless sensor networks. The world's technological per-capita capacity to store information has roughly doubled every 40 months since the 1980s; as of 2012, every day 2.5 exabytes (2.5×260 bytes) of data are generated. Based on an IDC report prediction, the global data volume was predicted to grow exponentially from 4.4 zettabytes to 44 zettabytes between 2013 and 2020. By 2025, IDC predicts there will be 163 zettabytes of data. One question for large enterprises is determining who should own big-data initiatives that affect the entire organization.Relational database management systems and desktop statistical software packages used to visualize data often have difficulty processing and analyzing big data. The processing and analysis of big data may require ""massively parallel software running on tens, hundreds, or even thousands of servers"". What qualifies as ""big data"" varies depending on the capabilities of those analyzing it and their tools. Furthermore, expanding capabilities make big data a moving target. ""For some organizations, facing hundreds of gigabytes of data for the first time may trigger a need to reconsider data management options. For others, it may take tens or hundreds of terabytes before data size becomes a significant consideration.""

",Big data,1,1
54,3876,"In probability theory and statistics, the binomial distribution with parameters n and p is the discrete probability distribution of the number of successes in a sequence of n independent experiments, each asking a yes–no question, and each with its own Boolean-valued outcome: success (with probability p) or failure (with probability q = 1 − p). A single success/failure experiment is also called a Bernoulli trial or Bernoulli experiment, and a sequence of outcomes is called a Bernoulli process; for a single trial, i.e., n = 1, the binomial distribution is a Bernoulli distribution. The binomial distribution is the basis for the popular binomial test of statistical significance.The binomial distribution is frequently used to model the number of successes in a sample of size n drawn with replacement from a population of size N. If the sampling is carried out without replacement, the draws are not independent and so the resulting distribution is a hypergeometric distribution, not a binomial one.  However, for N much larger than n, the binomial distribution remains a good approximation, and is widely used.",Binomial distribution,1,0
55,4916857,"In statistics, binomial regression is a regression analysis technique in which the response (often referred to as Y) has a binomial distribution: it is the number of successes in a series of 
  
    
      
        n
      
    
    {\displaystyle n}
   independent Bernoulli trials, where each trial has probability of success 
  
    
      
        p
      
    
    {\displaystyle p}
  . In binomial regression, the probability of a success is related to explanatory variables: the corresponding concept in ordinary regression is to relate the mean value of the unobserved response to explanatory variables.
Binomial regression is closely related to binary regression: a binary regression can be considered a binomial regression with 
  
    
      
        n
        =
        1
      
    
    {\displaystyle n=1}
  , or a regression on ungrouped binary data, while a binomial regression can be considered a regression on grouped binary data (see comparison). Binomial regression models are essentially the same as binary choice models, one type of discrete choice model: the primary difference is in the theoretical motivation (see comparison). In machine learning, binomial regression is considered a special case of probabilistic classification, and thus a generalization of binary classification.",Binomial regression,1,0
56,4214,"Bioinformatics ( (listen)) is an interdisciplinary field that develops methods and software tools for understanding biological data, in particular when the data sets are large and complex. As an interdisciplinary field of science, bioinformatics combines biology, chemistry, physics, computer science, information engineering, mathematics and statistics to analyze and interpret the biological data. Bioinformatics has been used for in silico analyses of biological queries using computational and statistical techniques.
Bioinformatics includes biological studies that use computer programming as part of their methodology, as well as specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidates genes and single nucleotide polymorphisms (SNPs). Often, such identification is made with the aim to better understand the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organizational principles within nucleic acid and protein sequences, called proteomics.Image and signal processing allow extraction of useful results from large amounts of raw data. In the field of genetics, it aids in sequencing and annotating genomes and their observed mutations. It plays a role in the text mining of biological literature and the development of biological and gene ontologies to organize and query biological data. It also plays a role in the analysis of gene and protein expression and regulation. Bioinformatics tools aid in comparing, analyzing and interpreting genetic and genomic data and more generally in the understanding of evolutionary aspects of molecular biology. At a more integrative level, it helps analyze and catalogue the biological pathways and networks that are an important part of systems biology. In structural biology, it aids in the simulation and modeling of DNA, RNA, proteins as well as biomolecular interactions.",Bioinformatics,1,1
57,12207,"Geology (from Ancient Greek  γῆ (gê) 'earth', and  -λoγία (-logía) 'study of, discourse') is a branch of natural science concerned with Earth and other astronomical objects, the features or rocks of which it is composed, and the processes by which they change over time. Modern geology significantly overlaps all other Earth sciences, including hydrology and the atmospheric sciences, and so is treated as one major aspect of integrated Earth system science and planetary science.
Geology describes the structure of the Earth on and beneath its surface, and the processes that have shaped that structure. It also provides tools to determine the relative and absolute ages of rocks found in a given location, and also to describe the histories of those rocks. By combining these tools, geologists are able to chronicle the geological history of the Earth as a whole, and also to demonstrate the age of the Earth. Geology provides the primary evidence for plate tectonics, the evolutionary history of life, and the Earth's past climates.
Geologists use a wide variety of methods to understand the Earth's structure and evolution, including field work, rock description, geophysical techniques, chemical analysis, physical experiments, and numerical modelling. In practical terms, geology is important for mineral and hydrocarbon exploration and exploitation, evaluating water resources, understanding of natural hazards, the remediation of environmental problems, and providing insights into past climate change. Geology is a major academic discipline, and it is central to geological engineering and plays an important role in geotechnical engineering.

",Geology,1,1
58,11885692,"Biometrics is a journal that publishes articles on the application of statistics and mathematics to the biological sciences. It is published by the International Biometric Society (IBS).
Originally published in 1945 under the title Biometrics Bulletin, the journal adopted the shorter title in 1947. A notable contributor to the journal was R.A. Fisher, for whom a memorial edition was published in 1964. In a survey of statistics researchers' opinions, it was ranked fifth overall among 40 statistics journals, and it was second only to the Journal of the American Statistical Association in the ranking provided by biometrics specialists.",Biometrics (journal),1,0
59,290622,"Biometrics are body measurements and calculations related to human characteristics. Biometric authentication (or realistic authentication) is used in computer science as a form of identification and access control. It is also used to identify individuals in groups that are under surveillance.
Biometric identifiers are the distinctive, measurable characteristics used to label and describe individuals. Biometric identifiers are often categorized as physiological characteristics, which are related to the shape of the body. Examples include, but are not limited to fingerprint, palm veins, face recognition, DNA, palm print, hand geometry, iris recognition, retina and odor/scent. Behavioral characteristics are related to the pattern of behavior of a person, including but not limited to mouse movement, typing rhythm, gait, signature, behavioral profiling, and Credentials. Some researchers have coined the term 'behaviometrics' to describe the latter class of biometrics.More traditional means of access control include token-based identification systems, such as a driver's license or passport, and knowledge-based identification systems, such as a password or personal identification number. Since biometric identifiers are unique to individuals, they are more reliable in verifying identity than token and knowledge-based methods; however, the collection of biometric identifiers raises privacy concerns about the ultimate use of this information.",Biometrics,1,0
60,3878,"Biostatistics (also known as biometry) are the development and application of statistical methods to a wide range of topics in biology. It encompasses the design of biological experiments, the collection and analysis of data from those experiments and the interpretation of the results.",Biostatistics,1,0
61,14306166,"Biplots are a type of exploratory graph used in statistics, a generalization of the simple two-variable scatterplot.
A biplot overlays a score plot with a loading plot.
A biplot allows information on both samples and variables of a data matrix to be displayed graphically. Samples are displayed as points while variables are displayed either as vectors, linear axes or nonlinear trajectories. In the case of categorical variables, category level points may be used to represent the levels of a categorical variable. A generalised biplot displays information on both continuous and categorical variables.",Biplot,1,0
62,4068,"Blaise Pascal ( pass-KAL, also UK:  -⁠KAHL, PASS-kəl, -⁠kal, US:  pahs-KAHL; French: [blɛz paskal]; 19 June 1623 – 19 August 1662) was a French mathematician, physicist, inventor, philosopher, writer, and Catholic theologian.
He was a child prodigy who was educated by his father, a tax collector in Rouen. Pascal's earliest mathematical work was on conic sections; he wrote a significant treatise on the subject of projective geometry at the age of 16. He later corresponded with Pierre de Fermat on probability theory, strongly influencing the development of modern economics and social science. In 1642, while still a teenager, he started some pioneering work on calculating machines (called Pascal's calculators and later Pascalines), establishing him as one of the first two inventors of the mechanical calculator.Like his contemporary René Descartes, Pascal was also a pioneer in the natural and applied sciences. Pascal wrote in defense of the scientific method and produced several controversial results. He made important contributions to the study of fluids, and clarified the concepts of pressure and vacuum by generalising the work of Evangelista Torricelli. Following Torricelli and Galileo Galilei, he rebutted the likes of Aristotle and Descartes who insisted that nature abhors a vacuum in 1647.
In 1646, he and his sister Jacqueline identified with the religious movement within Catholicism known by its detractors as Jansenism. Following a religious experience in late 1654, he began writing influential works on philosophy and theology. His two most famous works date from this period: the Lettres provinciales and the Pensées, the former set in the conflict between Jansenists and Jesuits. The latter contains Pascal's Wager, known in the original as the Discourse on the Machine, a fideistic probabilistic argument for God's existence. In that year, he also wrote an important treatise on the arithmetical triangle. Between 1658 and 1659, he wrote on the cycloid and its use in calculating the volume of solids.
Throughout his life, Pascal was in frail health, especially after the age of 18; he died just two months after his 39th birthday.",Blaise Pascal,1,0
63,1822339,"In the  statistical theory of the design of experiments, blocking is the arranging of experimental units in groups (blocks) that are similar to one another. Blocking can be used to tackle the problem of pseudoreplication.",Blocking (statistics),1,0
64,1162065,"In computer science, the Boolean (sometimes shortened to Bool) is a data type that has one of two possible values (usually denoted true and false) which is intended to represent the two truth values of logic and Boolean algebra. It is named after George Boole, who first defined an algebraic system of logic in the mid 19th century. The Boolean data type is primarily associated with conditional statements, which allow different actions by changing control flow depending on whether a programmer-specified Boolean condition evaluates to true or false. It is a special case of a more general logical data type (see probabilistic logic)—logic doesn't always need to be Boolean.

",Boolean data type,1,0
65,6885770,"Bootstrapping is any test or metric that uses random sampling with replacement (e.g. mimicking the sampling process), and falls under the broader class of resampling methods. Bootstrapping assigns measures of accuracy (bias, variance, confidence intervals, prediction error, etc.) to sample estimates. This technique allows estimation of the sampling distribution of almost any statistic using random sampling methods.Bootstrapping estimates the properties of an estimator (such as its variance) by measuring those properties when sampling from an approximating distribution. One standard choice for an approximating distribution is the empirical distribution function of the observed data. In the case where a set of observations can be assumed to be from an independent and identically distributed population, this can be implemented by constructing a number of  resamples with replacement, of the observed data set (and of equal size to the observed data set).
It may also be used for constructing hypothesis tests. It is often used as an alternative to statistical inference based on the assumption of a parametric model when that assumption is in doubt, or where parametric inference is impossible or requires complicated formulas for the calculation of standard errors.",Bootstrapping (statistics),1,0
66,160960,"In descriptive statistics, a box plot or boxplot is a method for graphically demonstrating the locality, spread and skewness groups of numerical data through their quartiles. In addition to the box on a box plot, there can be lines (which are called whiskers) extending from the box indicating variability outside the upper and lower quartiles, thus, the plot is also termed as the box-and-whisker plot and the box-and-whisker diagram. Outliers that differ significantly from the rest of the dataset may be plotted as individual points beyond the whiskers on the box-plot.
Box plots are non-parametric: they display variation in samples of a statistical population without making any assumptions of the underlying statistical distribution (though Tukey's boxplot assumes symmetry for the whiskers and normality for their length). The spacings in each subsection of the box-plot indicate the degree of dispersion (spread) and skewness of the data, which are usually described using the five-number summary. In addition, the box-plot allows one to visually estimate various L-estimators, notably the interquartile range, midhinge, range, mid-range, and trimean. Box plots can be drawn either horizontally or vertically.",Box plot,1,0
67,2399066,"In time series analysis, the Box–Jenkins method, named after the statisticians George Box and Gwilym Jenkins, applies autoregressive moving average (ARMA) or autoregressive integrated moving average (ARIMA) models to find the best fit of a time-series model to past values of a time series.

",Box–Jenkins method,1,0
68,17097662,"In statistics, the Breusch–Godfrey test is used to assess the validity of some of the modelling assumptions inherent in applying regression-like models to observed data series.  In particular, it tests for the presence of serial correlation that has not been included in a proposed model structure and which, if present, would mean that incorrect conclusions would be drawn from other tests or that sub-optimal estimates of model parameters would be obtained.
The regression models to which the test can be applied include cases where lagged values of the dependent variables are used as independent variables in the model's representation for later observations. This type of structure is common in econometric models.
The test is named after Trevor S. Breusch and Leslie G. Godfrey.",Breusch–Godfrey test,1,0
69,2371482,"Business analysis is a professional discipline of identifying business needs and determining solutions to business problems. Solutions often include a software-systems development component, but may also consist of process improvements, organizational change or strategic planning and policy development. The person who carries out this task is called a business analyst or BA.Business analysts do not work solely on developing software systems. But work across the organisation, solving business problems in consultation with business stakeholders. Whilst most of the work that business analysts do today relate to software development/solutions, this derives from the ongoing massive changes businesses all over the world are experiencing in their attempts to digitise.Although there are different role definitions, depending upon the organization, there does seem to be an area of common ground where most
business analysts work. The responsibilities appear to be:

To investigate business systems, taking a holistic view of the situation. This may include examining elements of the organisation structures and staff development issues as well as current processes and IT systems.
To evaluate actions to improve the operation of a business system. Again, this may require an examination of organisational structure and staff development needs, to ensure that they are in line with any proposed process redesign and IT system development.
To document the business requirements for the IT system support using appropriate documentation standards.In line with this, the core business analyst role could be defined as an internal consultancy role that has the responsibility for investigating business situations, identifying and evaluating options for improving business systems, defining requirements and ensuring the effective use of information systems in meeting the needs of the business.",Business analysis,1,0
70,1990966,"Business education is a branch of education that involves teaching the skills and operations of the business industry. This field of education occurs at multiple levels, including secondary and higher education

",Business education,1,0
71,5176,"Calculus, originally called infinitesimal calculus or ""the calculus of infinitesimals"", is the mathematical study of continuous change, in the same way that geometry is the study of shape, and algebra is the study of generalizations of arithmetic operations.
It has two major branches, differential calculus and integral calculus; differential calculus concerns instantaneous rates of change, and the slopes of curves, while integral calculus concerns accumulation of quantities, and areas under or between curves. These two branches are related to each other by the fundamental theorem of calculus, and they make use of the fundamental notions of convergence of infinite sequences and infinite series to a well-defined limit.Infinitesimal calculus was developed independently in the late 17th century by Isaac Newton and Gottfried Wilhelm Leibniz. Later work, including codifying the idea of limits, put these developments on a more solid conceptual footing. Today, calculus has widespread uses in science, engineering, and social science.In mathematics education, calculus denotes courses of elementary mathematical analysis, which are mainly devoted to the study of functions and limits. The word calculus is Latin for ""small pebble"" (the diminutive of calx, meaning ""stone""), a meaning which still persists in medicine. Because such pebbles were used for counting out distances, tallying votes, and doing abacus arithmetic, the word came to mean a method of computation. In this sense, it was used in English at least as early as 1672, several years prior to the publications of Leibniz and Newton.In addition to the differential calculus and integral calculus, the term is also used for naming specific methods of calculation and related theories which seek to model a particular concept in terms of mathematics. Examples of this convention include propositional calculus, Ricci calculus, calculus of variations, lambda calculus, and process calculus. Furthermore, the term ""calculus"" has variously been applied in ethics and philosophy, for such systems as Bentham's felicific calculus, and the ethical calculus.",Calculus,1,0
72,363900,"In statistics, canonical-correlation analysis (CCA), also called canonical variates analysis, is a way of inferring information from cross-covariance matrices. If we have two vectors X = (X1, ..., Xn) and Y = (Y1, ..., Ym)  of random variables, and there are correlations among the variables, then canonical-correlation analysis will find linear combinations of X and Y which have maximum correlation with each other. T. R. Knapp notes that ""virtually all of the commonly encountered parametric tests of significance can be treated as special cases of canonical-correlation analysis, which is the general procedure for investigating the relationships between two sets of variables."" The method was first introduced by Harold Hotelling in 1936, although in the context of angles between flats the mathematical concept was published by Jordan in 1875.

",Canonical correlation,1,1
73,7294,"Cartography (; from Greek χάρτης chartēs, ""papyrus, sheet of paper, map""; and γράφειν graphein, ""write"") is the study and practice of making and using maps. Combining science, aesthetics and technique, cartography builds on the premise that reality (or an imagined reality) can be modeled in ways that communicate spatial information effectively.
The fundamental objectives of traditional cartography are to:

Set the map's agenda and select traits of the object to be mapped. This is the concern of map editing. Traits may be physical, such as roads or land masses, or may be abstract, such as toponyms or political boundaries.
Represent the terrain of the mapped object on flat media. This is the concern of map projections.
Eliminate characteristics of the mapped object that are not relevant to the map's purpose. This is the concern of generalization.
Reduce the complexity of the characteristics that will be mapped. This is also the concern of generalization.
Orchestrate the elements of the map to best convey its message to its audience. This is the concern of map design.Modern cartography constitutes many theoretical and practical foundations of geographic information systems (GIS) and geographic information science (GISc).",Cartography,1,0
74,788661,"A case–control study (also known as case–referent study) is a type of observational study in which two existing groups differing in outcome are identified and compared on the basis of some supposed causal attribute. Case–control studies are often used to identify factors that may contribute to a medical condition by comparing subjects who have that condition/disease (the ""cases"") with patients who do not have the condition/disease but are otherwise similar (the ""controls""). They require fewer resources but provide less evidence for causal inference than a randomized controlled trial. A case–control study produces only an odds ratio, which is an inferior measure of strength of association compared to relative risk.

",Case–control study,1,0
75,821165,"In statistics, a categorical variable (also called qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property. In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution.
Categorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from observations made of qualitative data that are summarised as counts or cross tabulations, or from observations of quantitative data grouped within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term ""categorical data"" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables.
A categorical variable that can take on exactly two values is termed a binary variable or a dichotomous variable; an important special case is the Bernoulli variable. Categorical variables with more than two possible values are called polytomous variables; categorical variables are often assumed to be polytomous unless otherwise specified. Discretization is treating continuous data as if it were categorical. Dichotomization is treating continuous data or polytomous variables as if they were binary variables. Regression analysis often treats category membership with one or more quantitative dummy variables.

",Categorical variable,1,1
77,5869,"Category theory is a general theory of mathematical structures and their relations that was introduced by Samuel Eilenberg and Saunders Mac Lane in the middle of the 20th century in their foundational work on algebraic topology. Nowadays, category theory is used in almost all areas of mathematics, and in some areas of computer science. In particular, many constructions of new mathematical objects from previous ones, that appear similarly in several contexts are conveniently expressed and unified in terms of categories. Examples include quotient spaces, direct products, completion, and duality.
A category is formed by two sorts of objects, the objects of the category, and the morphisms, which relate two objects called the source and the target of the morphism. One often says that a morphism is an arrow that maps its source to its target. Morphisms can be composed if the target of the first morphism equals the source of the second one, and morphism composition has similar properties as function composition (associativity and existence of identity morphisms). Morphisms are often some sort of function, but this is not always the case. For example, a monoid may be viewed as a category with a single object, whose morphisms are the elements of the monoid.
The second fundamental concept of category is the concept of a functor, which plays the role of a morphism between two categories 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
   and 
  
    
      
        
          C
          
            2
          
        
        :
      
    
    {\displaystyle C_{2}:}
   it maps objects of 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
   to objects of 
  
    
      
        
          C
          
            2
          
        
      
    
    {\displaystyle C_{2}}
   and morphisms of 
  
    
      
        
          C
          
            1
          
        
      
    
    {\displaystyle C_{1}}
   to morphisms of 
  
    
      
        
          C
          
            2
          
        
      
    
    {\displaystyle C_{2}}
   in such a way that sources are mapped to sources and targets are mapped to targets (or, in the case of a contravariant functor, sources are mapped to targets and vice-versa). A third fundamental concept is a natural transformation that may be viewed as a morphism of functors.

",Category theory,1,0
78,37196,"Causality (also referred to as causation, or cause and effect) is influence by which one event, process, state, or object (a cause) contributes to the production of another event, process, state, or object (an effect) where the cause is partly responsible for the effect, and the effect is partly dependent on the cause. In general, a process has many causes, which are also said to be causal factors for it, and all lie in its past. An effect can in turn be a cause of, or causal factor for, many other effects, which all lie in its future. Some writers have held that causality is metaphysically prior to notions of time and space.Causality is an abstraction that indicates how the world progresses, so basic a concept that it is more apt as an explanation of other concepts of progression than as something to be explained by others more basic. The concept is like those of agency and efficacy. For this reason, a leap of intuition may be needed to grasp it. Accordingly, causality is implicit in the logic and structure of ordinary language.In English studies of Aristotelian philosophy, the word ""cause"" is used as a specialized technical term, the translation of Aristotle's term αἰτία, by which Aristotle meant ""explanation"" or ""answer to a 'why' question"". Aristotle categorized the four types of answers as material, formal, efficient, and final ""causes"". In this case, the ""cause"" is the explanans for the explanandum, and failure to recognize that different kinds of ""cause"" are being considered can lead to futile debate. Of Aristotle's four explanatory modes, the one nearest to the concerns of the present article is the ""efficient"" one.
David Hume, as part of his opposition to rationalism, argued that pure reason alone cannot prove the reality of efficient causality; instead, he appealed to custom and mental habit, observing that all human knowledge derives solely from experience.
The topic of causality remains a staple in contemporary philosophy.",Causality,1,0
79,19593040,"The degree Celsius is a unit of temperature on the Celsius scale, a temperature scale originally known as the centigrade scale. The degree Celsius (symbol: °C) can refer to a specific temperature on the Celsius scale or a unit to indicate a difference or range between two temperatures. It is named after the Swedish astronomer Anders Celsius (1701–1744), who developed a similar temperature scale in 1742. Before being renamed to honour Anders Celsius in 1948, the unit was called centigrade, from the Latin centum, which means 100, and gradus, which means steps. Most major countries use this scale; the other major scale, Fahrenheit, is still used in the United States, some island territories, and Liberia. The Kelvin scale is of use in the sciences, with 0 K (-273.15 °C) representing absolute zero.
Since 1743 the Celsius scale has been based on 0 °C for the freezing point of water and 100 °C for the boiling point of water at 1 atm pressure. Prior to 1743 the values were reversed (i.e. the boiling point was 0 degrees and the freezing point was 100 degrees). The 1743 scale reversal was proposed by Jean-Pierre Christin.
By international agreement, between 1954 and 2019 the unit degree Celsius and the Celsius scale were defined by absolute zero and the triple point of water. After 2007, it was clarified that this definition referred to Vienna Standard Mean Ocean Water (VSMOW), a precisely defined water standard. This definition also precisely related the Celsius scale to the Kelvin scale, which defines the SI base unit of thermodynamic temperature with symbol K. Absolute zero, the lowest temperature possible, is defined as being exactly 0 K and −273.15 °C. Until 19 May 2019, the temperature of the triple point of water was defined as exactly 273.16 K (0.01 °C). This means that a temperature difference of one degree Celsius and that of one kelvin are exactly the same.On 20 May 2019, the kelvin was redefined so that its value is now determined by the definition of the Boltzmann constant rather than being defined by the triple point of VSMOW. This means that the triple point is now a measured value, not a defined value. The newly-defined exact value of the Boltzmann constant was selected so that the measured value of the VSMOW triple point is exactly the same as the older defined value to within the limits of accuracy of contemporary metrology. The degree Celsius remains exactly equal to the kelvin, and 0 K remains exactly −273.15 °C.

",Celsius,1,0
80,11548952,"In statistics, censoring is a condition in which the value of a measurement or observation is only partially known.
For example, suppose a study is conducted to measure the impact of a drug on mortality rate.  In such a study, it may be known that an individual's age at death is at least 75 years (but may be more).  Such a situation could occur if the individual withdrew from the study at age 75, or if the individual is currently alive at the age of 75.
Censoring also occurs when a value occurs outside the range of a measuring instrument.  For example, a bathroom scale might only measure up to 140 kg.  If a 160-kg individual is weighed using the scale, the observer would only know that the individual's weight is at least 140 kg.
The problem of censored data, in which the observed value of some variable is partially known, is related to the problem of missing data, where the observed value of some variable is unknown.
Censoring should not be confused with the related idea truncation. With censoring, observations result either in knowing the exact value that applies, or in knowing that the value lies within an interval. With truncation, observations never result in values outside a given range: values in the population outside the range are never seen or never recorded if they are seen. Note that in statistics, truncation is not the same as rounding.",Censoring (statistics),1,0
81,6889,"A census is the procedure of systematically calculating, acquiring and recording information about the members of a given population. This term is used mostly in connection with national population and housing censuses; other common censuses include the census of agriculture, and other censuses such as the traditional culture, business, supplies, and traffic censuses. The United Nations defines the essential features of population and housing censuses as ""individual enumeration, universality within a defined territory, simultaneity and defined periodicity"", and recommends that population censuses be taken at least every ten years. United Nations recommendations also cover census topics to be collected, official definitions, classifications and other useful information to co-ordinate international practices.The Food and Agriculture Organization of the United Nations (FAO), in turn, defines the census of agriculture as “a statistical operation for collecting, processing and disseminating data on the structure of agriculture, covering the whole or a significant part of a country.” “In a census of agriculture, data are collected at the holding level.The word is of Latin origin: during the Roman Republic, the census was a list that kept track of all adult males fit for military service. The modern census is essential to international comparisons of any kind of statistics, and censuses collect data on many attributes of a population, not just how many people there are. Censuses typically began as the only method of collecting national demographic data, and are now part of a larger system of different surveys. Although population estimates remain an important function of a census, including exactly the geographic distribution of the population or the agricultural population, statistics can be produced about combinations of attributes e.g. education by age and sex in different regions. Current administrative data systems allow for other approaches to enumeration with the same level of detail but raise concerns about privacy and the possibility of biasing estimates.A census can be contrasted with sampling in which information is obtained only from a subset of a population; typically main population estimates are updated by such intercensal estimates. Modern census data are commonly used for research, business marketing, and planning, and as a baseline for designing sample surveys by providing a sampling frame such as an address register. Census counts are necessary to adjust samples to be representative of a population by weighting them as is common in opinion polling. Similarly, stratification requires knowledge of the relative sizes of different population strata, which can be derived from census enumerations. In some countries, the census provides the official counts used to apportion the number of elected representatives to regions (sometimes controversially – e.g., Utah v. Evans). In many cases, a carefully chosen random sample can provide more accurate information than attempts to get a population census.",Census,1,0
82,39406,"In probability theory, the central limit theorem (CLT) establishes that, in many situations, when independent random variables are summed up, their properly normalized sum tends toward a normal distribution even if the original variables themselves are not normally distributed. 
The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions. 
This theorem has seen many changes during the formal development of probability theory. Previous versions of the theorem date back to 1811, but in its modern general form, this fundamental result in probability theory was precisely stated as late as 1920, thereby serving as a bridge between classical and modern probability theory.
If 
  
    
      
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        ,
        …
      
    
    {\textstyle X_{1},X_{2},\dots ,X_{n},\dots }
   are  random samples drawn from a population with overall mean 
  
    
      
        μ
      
    
    {\textstyle \mu }
   and finite variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\textstyle \sigma ^{2}}
  , and if 
  
    
      
        
          
            
              
                X
                ¯
              
            
          
          
            n
          
        
      
    
    {\textstyle {\bar {X}}_{n}}
   is the sample mean of the first 
  
    
      
        n
      
    
    {\textstyle n}
   samples, then the limiting form of the distribution, 
  
    
      
        Z
        =
        
          lim
          
            n
            →
            ∞
          
        
        
          
            (
            
              
                
                  
                    
                      
                        
                          X
                          ¯
                        
                      
                    
                    
                      n
                    
                  
                  −
                  μ
                
                
                  σ
                  
                    
                      
                        X
                        ¯
                      
                    
                  
                
              
            
            )
          
        
      
    
    {\textstyle Z=\lim _{n\to \infty }{\left({\frac {{\bar {X}}_{n}-\mu }{\sigma _{\bar {X}}}}\right)}}
  , with 
  
    
      
        
          σ
          
            
              
                X
                ¯
              
            
          
        
        =
        
          
            σ
            
              n
            
          
        
      
    
    {\displaystyle \sigma _{\bar {X}}={\frac {\sigma }{\sqrt {n}}}}
  , is a standard normal distribution.For example, suppose that a sample is obtained containing many observations, each observation being randomly generated in a way that does not depend on the values of the other observations, and that the arithmetic mean of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the probability distribution of the average will closely approximate a normal distribution. A simple example of this is that if one flips a coin many times, the probability of getting a given number of heads will approach a normal distribution, with the mean equal to half the total number of flips. At the limit of an infinite number of flips, it will equal a normal distribution.
The central limit theorem has several variants. In its common form, the random variables must be identically distributed. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, if they comply with certain conditions.
The earliest version of this theorem, that the normal distribution may be used as an approximation to the binomial distribution, is the de Moivre–Laplace theorem.

",Central limit theorem,1,0
83,5794,"In statistics, a central tendency (or measure of central tendency) is a central or typical value for a probability distribution.Colloquially, measures of central tendency are often called averages. The term central tendency dates from the late 1920s.The most common measures of central tendency are the arithmetic mean, the median, and the mode.  A middle tendency can be calculated for either a finite set of values or for a theoretical distribution, such as the normal distribution. Occasionally authors use central tendency to denote ""the tendency of quantitative data to cluster around some central value.""The central tendency of a distribution is typically contrasted with its dispersion or variability; dispersion and central tendency are the often characterized properties of distributions. Analysis may judge whether data has a strong or a weak central tendency based on its dispersion.",Central tendency,1,0
84,6295,"Chaos theory is an interdisciplinary scientific theory and branch of mathematics focused on underlying patterns and deterministic laws, of dynamical systems, that are highly sensitive to initial conditions, that were once thought to have completely random states of disorder and irregularities. Chaos theory states that within the apparent randomness of chaotic complex systems, there are underlying patterns, interconnectedness, constant feedback loops, repetition, self-similarity, fractals, and self-organization. The butterfly effect, an underlying principle of chaos, describes how a small change in one state of a deterministic nonlinear system can result in large differences in a later state (meaning that there is sensitive dependence on initial conditions). A metaphor for this behavior is that a butterfly flapping its wings in Brazil can cause a tornado in Texas.Small differences in initial conditions, such as those due to errors in measurements or due to rounding errors in numerical computation, can yield widely diverging outcomes for such dynamical systems, rendering long-term prediction of their behavior impossible in general. This can happen even though these systems are deterministic, meaning that their future behavior follows a unique evolution and is fully determined by their initial conditions, with no random elements involved. In other words, the deterministic nature of these systems does not make them predictable. This behavior is known as deterministic chaos, or simply chaos. The theory was summarized by Edward Lorenz as:
Chaos: When the present determines the future, but the approximate present does not approximately determine the future.
Chaotic behavior exists in many natural systems, including fluid flow, heartbeat irregularities, weather, and climate. It also occurs spontaneously in some systems with artificial components, such as the stock market and road traffic. This behavior can be studied through the analysis of a chaotic mathematical model, or through analytical techniques such as recurrence plots and Poincaré maps. Chaos theory has applications in a variety of disciplines, including meteorology, anthropology, sociology, environmental science, computer science, engineering, economics, ecology, and pandemic crisis management. The theory formed the basis for such fields of study as complex dynamical systems, edge of chaos theory, and self-assembly processes.",Chaos theory,1,0
85,1012548,"The Chartered Financial Analyst (CFA) program is a postgraduate professional certification offered internationally by the American-based CFA Institute (formerly the Association for Investment Management and Research, or AIMR) to investment and financial professionals.
The program teaches a wide range of subjects relating to advanced investment analysis - including security analysis, statistics, probability theory, fixed income, derivatives, economics, financial analysis, corporate finance, alternative investments, portfolio management - and provides a generalist knowledge of other areas of finance. A candidate who successfully completes the program and meets other professional requirements is awarded the ""CFA charter"" and becomes a ""CFA charterholder"". As of March 2022, at least 175,000 people are charterholders globally, growing 3.5% annually since 2012 (including effects of the pandemic). Successful candidates take an average of four years to earn their CFA charter.The top employers of CFA charterholders globally include JP Morgan, UBS, Royal Bank of Canada, Morgan Stanley, BlackRock, and Goldman Sachs.",Chartered Financial Analyst,1,1
86,5180,"Chemistry is the scientific study of the properties and behavior of matter. It is a natural science that covers the elements that make up matter to the compounds composed of atoms, molecules and ions: their composition, structure, properties, behavior and the changes they undergo during a reaction with other substances.In the scope of its subject, chemistry occupies an intermediate position between physics and biology. It is sometimes called the central science because it provides a foundation for understanding both basic and applied scientific disciplines at a fundamental level. For example, chemistry explains aspects of plant growth (botany), the formation of igneous rocks (geology), how atmospheric ozone is formed and how environmental pollutants are degraded (ecology), the properties of the soil on the moon (cosmochemistry), how medications work (pharmacology), and how to collect DNA evidence at a crime scene (forensics).
Chemistry addresses topics such as how atoms and molecules interact via chemical bonds to form new chemical compounds. There are two types of chemical bonds: 1. primary chemical bonds—e.g., covalent bonds, in which atoms share one or more electron(s); ionic bonds, in which an atom donates one or more electrons to another atom to produce ions (cations and anions); metallic bonds—and 2. secondary chemical bonds—e.g., hydrogen bonds; Van der Waals force bonds; ion-ion interaction; ion-dipole interaction.",Chemistry,1,0
87,314204,"Chemometrics is the science of extracting information from chemical systems by data-driven means.  Chemometrics is inherently interdisciplinary, using methods frequently employed in core data-analytic disciplines such as multivariate statistics, applied mathematics, and computer science, in order to address problems in chemistry, biochemistry, medicine, biology and chemical engineering.  In this way, it mirrors other interdisciplinary fields, such as psychometrics and econometrics.",Chemometrics,1,0
88,226680,"A chi-squared test (also chi-square or χ2 test) is a statistical hypothesis test that is valid to perform when the test statistic is chi-squared distributed under the null hypothesis, specifically Pearson's chi-squared test and variants thereof. Pearson's chi-squared test is used to determine whether there is a statistically significant difference between the expected frequencies and the observed frequencies in one or more categories of a contingency table.
In the standard applications of this test, the observations are classified into mutually exclusive classes. If the null hypothesis that there are no differences between the classes in the population is true, the test statistic computed from the observations follows a χ2 frequency distribution. The purpose of the test is to evaluate how likely the observed frequencies would be assuming the null hypothesis is true.
Test statistics that follow a χ2 distribution occur when the observations are independent. There are also χ2 tests for testing the null hypothesis of independence of a pair of random variables based on observations of the pairs.
Chi-squared tests often refers to tests for which the distribution of the test statistic approaches the χ2 distribution asymptotically, meaning that the sampling distribution (if the null hypothesis is true) of the test statistic approximates a chi-squared distribution more and more closely as sample sizes increase.",Chi-squared test,1,1
89,113424,"In probability theory and statistics, the chi-squared distribution (also chi-square or χ2-distribution) with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables. The chi-squared distribution is a special case of the gamma distribution and is one of the most widely used probability distributions in inferential statistics, notably in hypothesis testing and in construction of confidence intervals. This distribution is sometimes called the central chi-squared distribution, a special case of the more general noncentral chi-squared distribution.
The chi-squared distribution is used in the common chi-squared tests for goodness of fit of an observed distribution to a theoretical one, the independence of two criteria of classification of qualitative data, and in confidence interval estimation for a population standard deviation of a normal distribution from a sample standard deviation. Many other statistical tests also use this distribution, such as Friedman's analysis of variance by ranks.",Chi-squared distribution,1,0
91,2944829,"Clinical study design is the formulation of trials and experiments, as well as observational studies in medical, clinical and other types of research (e.g., epidemiological) involving human beings.  The goal of a clinical study is to assess the safety, efficacy, and / or the mechanism of action of an investigational medicinal product (IMP) or procedure, or new drug or device that is in development, but potentially not yet approved by a health authority (e.g. Food and Drug Administration). It can also be to investigate a drug, device or procedure that has already been approved but is still in need of further investigation, typically with respect to long-term effects or cost-effectiveness.Some of the considerations here are shared under the more general topic of design of experiments but there can be others, in particular related to patient confidentiality and ethics.

",Clinical study design,1,0
92,241717,"Clinical trials are experiments or observations done in clinical research. Such prospective biomedical or behavioral research studies on human participants are designed to answer specific questions about biomedical or behavioral interventions, including new treatments (such as novel vaccines, drugs, dietary choices, dietary supplements, and medical devices) and known interventions that warrant further study and comparison. Clinical trials generate data on dosage, safety and efficacy. They are conducted only after they have received health authority/ethics committee approval in the country where approval of the therapy is sought. These authorities are responsible for vetting the risk/benefit ratio of the trial—their approval does not mean the therapy is 'safe' or effective, only that the trial may be conducted.
Depending on product type and development stage, investigators initially enroll volunteers or patients into small pilot studies, and subsequently conduct progressively larger scale comparative studies. Clinical trials can vary in size and cost, and they can involve a single research center or multiple centers, in one country or in multiple countries. Clinical study design aims to ensure the scientific validity and reproducibility of the results.
Costs for clinical trials can range into the billions of dollars per approved drug. The sponsor may be a governmental organization or a pharmaceutical, biotechnology or medical device company. Certain functions necessary to the trial, such as monitoring and lab work, may be managed by an outsourced partner, such as a contract research organization or a central laboratory. Only 10 percent of all drugs started in human clinical trials become approved drugs.",Clinical trial,1,0
93,669675,"Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery or interactive multi-objective optimization that involves trial and failure. It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, numerical taxonomy, botryology (from Greek βότρυς ""grape""), typological analysis, and community detection. The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest, in automatic classification the resulting discriminative power is of interest.
Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939 and famously used by Cattell beginning in 1943 for trait theory classification in personality psychology.",Cluster analysis,1,1
95,5797,"In statistics, cluster sampling is a sampling plan used when mutually homogeneous yet internally heterogeneous groupings are evident in a statistical population.  It is often used in marketing research. In this sampling plan, the total population is divided into these groups (known as clusters) and a simple random sample of the groups is selected. The elements in each cluster are then sampled. If all elements in each sampled cluster are sampled, then this is referred to as a ""one-stage"" cluster sampling plan. If a simple random subsample of elements is selected within each of these groups, this is referred to as a ""two-stage"" cluster sampling plan. A common motivation for cluster sampling is to reduce the total number of interviews and costs given the desired accuracy. For a fixed sample size, the expected random error is smaller when most of the variation in the population is present internally within the groups, and not between the groups.",Cluster sampling,1,0
96,29622014,"In statistics, the Cochran–Mantel–Haenszel test (CMH) is a test used in the analysis of stratified or matched categorical data. It allows an investigator to test the association between a binary predictor or treatment and a binary outcome such as case or control status while taking into account the stratification. Unlike the McNemar test which can only handle pairs, the CMH test handles arbitrary strata size. It is named after William G. Cochran, Nathan Mantel and William Haenszel. Extensions of this test to a categorical response and/or to several groups are commonly called Cochran–Mantel–Haenszel statistics. It is often used in observational studies where random assignment of subjects to different treatments cannot be controlled, but confounding covariates can be measured.",Cochran–Mantel–Haenszel statistics,1,0
97,1500869,"In statistics, the coefficient of determination, denoted R2 or r2 and pronounced ""R squared"", is the proportion of the variation in the dependent variable that is predictable from the independent variable(s).
It is a statistic used in the context of statistical models whose main purpose is either the prediction of future outcomes or the testing of hypotheses, on the basis of other related information. It provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model.There are several definitions of R2 that are only sometimes equivalent. One class of such cases includes that of simple linear regression where r2 is used instead of R2. When only an intercept is included, then r2 is simply the square of the sample correlation coefficient (i.e., r) between the observed outcomes and the observed predictor values. If additional regressors are included, R2 is the square of the coefficient of multiple correlation. In both such cases, the coefficient of determination normally ranges from 0 to 1.
There are cases where the computational definition of R2 can yield negative values, depending on the definition used. This can arise when the predictions that are being compared to the corresponding outcomes have not been derived from a model-fitting procedure using those data. Even if a model-fitting procedure has been used, R2 may still be negative, for example when linear regression is conducted without including an intercept, or when a non-linear function is used to fit the data. In cases where negative values arise, the mean of the data provides a better fit to the outcomes than do the fitted function values, according to this particular criterion.
The coefficient of determination can be more (intuitively) informative than MAE, MAPE, MSE, and RMSE in regression analysis evaluation, as the former can be expressed as a percentage, whereas the latter measures have arbitrary ranges. It also proved more robust for poor fits compared to SMAPE on the test datasets in the article.When evaluating the goodness-of-fit of simulated (Ypred) vs. measured (Yobs) values, it is not appropriate to base this on the R2 of the linear regression (i.e., Yobs= m·Ypred + b). The R2 quantifies the degree of any linear correlation between Yobs and Ypred, while for the goodness-of-fit evaluation only one specific linear correlation should be taken into consideration: Yobs = 1·Ypred + 0 (i.e., the 1:1 line).",Coefficient of determination,1,0
98,1012687,"In probability theory and statistics, the coefficient of variation (CV), also known as relative standard deviation (RSD), is a standardized measure of dispersion of a probability distribution or frequency distribution. It is often expressed as a percentage, and is defined as the ratio of the standard deviation  
  
    
      
         
        σ
      
    
    {\displaystyle \ \sigma }
   to the mean  
  
    
      
         
        μ
      
    
    {\displaystyle \ \mu }
   (or its absolute value, 
  
    
      
        
          |
        
        μ
        
          |
        
      
    
    {\displaystyle |\mu |}
  ).  The CV or RSD is widely used in analytical chemistry to express the precision and repeatability of an assay. It is also commonly used in fields such as engineering or physics when doing quality assurance studies and ANOVA gauge R&R. In addition, CV is utilized by economists and investors in economic models.

",Coefficient of variation,1,0
99,1701650,"Cohen's kappa coefficient (κ) is a statistic that is used to measure inter-rater reliability (and also intra-rater reliability) for qualitative (categorical) items. It is generally thought to be a more robust measure than simple percent agreement calculation, as κ takes into account the possibility of the agreement occurring by chance. There is controversy surrounding Cohen's kappa due to the difficulty in interpreting indices of agreement. Some researchers have suggested that it is conceptually simpler to evaluate disagreement between items.",Cohen's kappa,1,0
100,397271,"A cohort study is a particular form of longitudinal study that samples a cohort (a group of people who share a defining characteristic, typically those who experienced a common event in a selected period, such as birth or graduation), performing a cross-section at intervals through time. It is a type of panel study where the individuals in the panel share a common characteristic.
Cohort studies represent one of the fundamental designs of epidemiology which are used in research in the fields of medicine, pharmacy, nursing, psychology, social science, and in any field reliant on 'difficult to reach' answers that are based on evidence (statistics). In medicine for instance, while clinical trials are used primarily for assessing the safety of newly developed pharmaceuticals before they are approved for sale, epidemiological analysis on how risk factors affect the incidence of diseases is often used to identify the causes of diseases in the first place, and to help provide pre-clinical justification for the plausibility of protective factors (treatments).",Cohort study,1,0
101,1822603,"Cointegration is a statistical property of a collection (X1, X2, ..., Xk) of time series variables. First, all of the series must be integrated of order d (see Order of integration). Next, if a linear combination of this collection is integrated of order less than d, then the collection is said to be co-integrated. Formally, if (X,Y,Z) are each integrated of order d, and there exist coefficients a,b,c such that aX + bY + cZ is integrated of order less than d, then X, Y, and Z are cointegrated.  Cointegration has become an important property in contemporary time series analysis. Time series often have trends—either deterministic or stochastic. In an influential paper, Charles Nelson and Charles Plosser (1982) provided statistical evidence that many US macroeconomic time series (like GNP, wages, employment, etc.) have stochastic trends.

",Cointegration,1,0
102,149968,"In linear algebra, a column vector is a column of entries, for example,

  
    
      
        
          x
        
        =
        
          
            [
            
              
                
                  
                    x
                    
                      1
                    
                  
                
              
              
                
                  
                    x
                    
                      2
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    x
                    
                      m
                    
                  
                
              
            
            ]
          
        
        
        .
      
    
    {\displaystyle {\boldsymbol {x}}={\begin{bmatrix}x_{1}\\x_{2}\\\vdots \\x_{m}\end{bmatrix}}\,.}
  Similarly, a row vector is a row of entries

  
    
      
        
          a
        
        =
        
          
            [
            
              
                
                  
                    a
                    
                      1
                    
                  
                
                
                  
                    a
                    
                      2
                    
                  
                
                
                  …
                
                
                  
                    a
                    
                      n
                    
                  
                
              
            
            ]
          
        
        
        .
      
    
    {\displaystyle {\boldsymbol {a}}={\begin{bmatrix}a_{1}&a_{2}&\dots &a_{n}\end{bmatrix}}\,.}
  Throughout, boldface is used for both row and column vectors. The transpose (indicated by T) of a row vector is the column vector

  
    
      
        
          
            
              [
              
                
                  
                    
                      x
                      
                        1
                      
                    
                    
                    
                      x
                      
                        2
                      
                    
                    
                    …
                    
                    
                      x
                      
                        m
                      
                    
                  
                
              
              ]
            
          
          
            
              T
            
          
        
        =
        
          
            [
            
              
                
                  
                    x
                    
                      1
                    
                  
                
              
              
                
                  
                    x
                    
                      2
                    
                  
                
              
              
                
                  ⋮
                
              
              
                
                  
                    x
                    
                      m
                    
                  
                
              
            
            ]
          
        
        
        ,
      
    
    {\displaystyle {\begin{bmatrix}x_{1}\;x_{2}\;\dots \;x_{m}\end{bmatrix}}^{\rm {T}}={\begin{bmatrix}x_{1}\\x_{2}\\\vdots \\x_{m}\end{bmatrix}}\,,}
  and the transpose of a column vector is the row vector

  
    
      
        
          
            
              [
              
                
                  
                    
                      x
                      
                        1
                      
                    
                  
                
                
                  
                    
                      x
                      
                        2
                      
                    
                  
                
                
                  
                    ⋮
                  
                
                
                  
                    
                      x
                      
                        m
                      
                    
                  
                
              
              ]
            
          
          
            
              T
            
          
        
        =
        
          
            [
            
              
                
                  
                    x
                    
                      1
                    
                  
                  
                  
                    x
                    
                      2
                    
                  
                  
                  …
                  
                  
                    x
                    
                      m
                    
                  
                
              
            
            ]
          
        
        
        .
      
    
    {\displaystyle {\begin{bmatrix}x_{1}\\x_{2}\\\vdots \\x_{m}\end{bmatrix}}^{\rm {T}}={\begin{bmatrix}x_{1}\;x_{2}\;\dots \;x_{m}\end{bmatrix}}\,.}
  The set of all row vectors with n entries forms an n-dimensional vector space; similarly, the set of all column vectors with m entries forms an m-dimensional vector space.
The space of row vectors with n entries can be regarded as the dual space of the space of column vectors with n entries, since any linear functional on the space of column vectors can be represented as the left-multiplication of a unique row vector.",Row and column vectors,1,0
103,5170,"Combinatorics is an area of mathematics primarily concerned with counting, both as a means and an end in obtaining results, and certain properties of finite structures.  It is closely related to many other areas of mathematics and has many applications ranging from logic to statistical physics and from evolutionary biology to computer science.
The full scope of combinatorics is not universally agreed upon. According to H.J. Ryser, a definition of the subject is difficult because it crosses so many mathematical subdivisions. Insofar as an area can be described by the types of problems it addresses, combinatorics is involved with:

the enumeration (counting) of specified structures, sometimes referred to as arrangements or configurations in a very general sense, associated with finite systems,
the existence of such structures that satisfy certain given criteria,
the construction of these structures, perhaps in many ways, and
optimization: finding the ""best"" structure or solution among several possibilities, be it the ""largest"",  ""smallest"" or satisfying some other optimality criterion.Leon Mirsky has said: ""combinatorics is a range of linked studies which have something in common and yet diverge widely in their objectives, their methods, and the degree of coherence they have attained."" One way to define combinatorics is, perhaps, to describe its subdivisions with their problems and techniques. This is the approach that is used below. However, there are also purely historical reasons for including or not including some topics under the combinatorics umbrella. Although primarily concerned with finite systems, some combinatorial questions and techniques can be extended to an infinite (specifically, countable) but discrete setting.
Combinatorics is well known for the breadth of the problems it tackles. Combinatorial problems arise in many areas of pure mathematics, notably in algebra, probability theory, topology, and geometry, as well as in its many application areas. Many combinatorial questions have historically been considered in isolation, giving an ad hoc solution to a problem arising in some mathematical context. In the later twentieth century, however, powerful and general theoretical methods were developed, making combinatorics into an independent branch of mathematics in its own right. One of the oldest and most accessible parts of combinatorics is graph theory, which by itself has numerous natural connections to other areas. Combinatorics is used frequently in computer science to obtain formulas and estimates in the analysis of algorithms.
A mathematician who studies combinatorics is called a combinatorialist.",Combinatorics,1,0
104,245990,"Commutative algebra, first known as ideal theory, is the branch of algebra that studies commutative rings, their ideals, and modules over such rings. Both algebraic geometry and algebraic number theory build on commutative algebra. Prominent examples of commutative rings include polynomial rings; rings of algebraic integers, including the ordinary integers 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
  ; and p-adic integers.Commutative algebra is the main technical tool in the local study of schemes.
The study of rings that are not necessarily commutative is known as noncommutative algebra; it includes ring theory, representation theory, and the theory of Banach algebras.",Commutative algebra,1,0
105,194618,"In statistics, completeness is a property of a statistic in relation to a model for a set of observed data. In essence, it ensures that the distributions corresponding to different values of the parameters are distinct.
It is closely related to the idea of identifiability, but in statistical theory it is often found as a condition imposed on  a sufficient statistic from which certain optimality results are derived.",Completeness (statistics),1,0
106,5759,"Complex analysis, traditionally known as the theory of functions of a complex variable, is the branch of mathematical analysis that investigates functions of complex numbers. It is helpful in many branches of mathematics, including algebraic geometry, number theory, analytic combinatorics, applied mathematics; as well as in physics, including the branches of hydrodynamics, thermodynamics, and particularly quantum mechanics. By extension, use of complex analysis also has applications in engineering fields such as nuclear, aerospace, mechanical and electrical engineering.As a differentiable function of a complex variable is equal to its Taylor series (that is, it is analytic), complex analysis is particularly concerned with analytic functions of a complex variable (that is, holomorphic functions).

",Complex analysis,1,0
107,149353,"Computational biology refers to the use of data analysis, mathematical modelling and computational simulations to understand biological systems and relationships. An intersection of computer science, biology, and big data, the field also has foundations in applied mathematics, chemistry, and genetics. It differs from biological computing, a subfield of computer engineering which uses bioengineering to build computers.",Computational biology,1,1
108,7543,"In computer science and mathematics, computational complexity theory focuses on classifying computational problems according to their resource usage, and relating these classes to each other. A computational problem is a task solved by a computer. A computation problem is solvable by mechanical application of mathematical steps, such as an algorithm.
A problem is regarded as inherently difficult if its solution requires significant resources, whatever the algorithm used. The theory formalizes this intuition, by introducing mathematical models of computation to study these problems and quantifying their computational complexity, i.e., the amount of resources needed to solve them, such as time and storage. Other measures of complexity are also used, such as the amount of communication (used in communication complexity), the number of gates in a circuit (used in circuit complexity) and the number of processors (used in parallel computing). One of the roles of computational complexity theory is to determine the practical limits on what computers can and cannot do. The P versus NP problem, one of the seven Millennium Prize Problems, is dedicated to the field of computational complexity.Closely related fields in theoretical computer science are analysis of algorithms and computability theory. A key distinction between analysis of algorithms and computational complexity theory is that the former is devoted to analyzing the amount of resources needed by a particular algorithm to solve a problem, whereas the latter asks a more general question about all possible algorithms that could be used to solve the same problem. More precisely, computational complexity theory tries to classify problems that can or cannot be solved with appropriately restricted resources. In turn, imposing restrictions on the available resources is what distinguishes computational complexity from computability theory: the latter theory asks what kinds of problems can, in principle, be solved algorithmically.",Computational complexity theory,1,1
109,10433833,"Computational mathematics involves mathematical research in mathematics as well as in areas of science where computation plays a central and essential role, and emphasizes algorithms, numerical methods, and symbolic computations.Computational applied mathematics consists roughly of using mathematics for allowing and improving computer computation in applied mathematics. Computational mathematics may also refer to the use of computers for mathematics itself. This includes the use of computers for mathematical computations (computer algebra), the study of what can (and cannot) be computerized in mathematics (effective methods), which computations may be done with present technology (complexity theory), and which proofs can be done on computers (proof assistants).

",Computational mathematics,1,1
110,984692,"Computational sociology is a branch of sociology that uses computationally intensive methods to analyze and model social phenomena. Using computer simulations, artificial intelligence, complex statistical methods, and analytic approaches like social network analysis, computational sociology develops and tests theories of complex social processes through bottom-up modeling of social interactions.It involves the understanding of social agents, the interaction among these agents, and the effect of these interactions on the social aggregate. Although the subject matter and methodologies in social science differ from those in natural science or computer science, several of the approaches used in contemporary social simulation originated from fields such as physics and artificial intelligence. Some of the approaches that originated in this field have been imported into the natural sciences, such as measures of network centrality from the fields of social network analysis and network science.
In relevant literature, computational sociology is often related to the study of social complexity. Social complexity concepts such as complex systems, non-linear interconnection among macro and micro process, and emergence, have entered the vocabulary of computational sociology. A practical and well-known example is the construction of a computational model in the form of an ""artificial society"", by which researchers can analyze the structure of a social system.",Computational sociology,1,0
111,15832717,"Computational statistics, or statistical computing, is the bond between statistics and computer science. It means statistical methods that are enabled by using computational methods. It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. This area is also developing rapidly, leading to calls that a broader concept of computing should be taught as part of general statistical education.As in traditional statistics the goal is to transform raw data into knowledge, but the focus lies on computer intensive statistical methods, such as cases with very large sample size and non-homogeneous data sets.The terms 'computational statistics' and 'statistical computing' are often used interchangeably, although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, defining 'statistical computing' as ""the application of computer science to statistics"",
and 'computational statistics' as ""aiming at the design of algorithm for implementing
statistical methods on computers, including the ones unthinkable before the computer
age (e.g. bootstrap, simulation), as well as to cope with analytically intractable problems"" [sic].The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models.

",Computational statistics,1,1
112,23659805,"In mathematics and computer science, computer algebra, also called symbolic computation or algebraic computation, is a scientific area that refers to the study and development of algorithms and software for manipulating mathematical expressions and other mathematical objects. Although computer algebra could be considered a subfield of scientific computing, they are generally considered as distinct fields because scientific computing is usually based on numerical computation with approximate floating point numbers, while symbolic computation emphasizes exact computation with expressions containing variables that have no given value and are manipulated as symbols.
Software applications that perform symbolic calculations are called computer algebra systems, with the term system alluding to the complexity of the main applications  that include, at least, a method to represent mathematical data in a computer, a user programming language (usually different from the language used for the implementation), a dedicated memory manager, a user interface for the input/output of mathematical expressions, a large set of routines to perform usual operations, like simplification of expressions, differentiation using chain rule, polynomial factorization, indefinite integration, etc.
Computer algebra is widely used to experiment in mathematics and to design the formulas that are used in numerical programs. It is also used for complete scientific computations, when purely numerical methods fail, as in public key cryptography, or for some non-linear problems.

",Computer algebra,1,0
113,5323,"Computer science is the study of computation, automation, and information. Computer science spans theoretical disciplines (such as algorithms, theory of computation, information theory and automation) to practical disciplines (including the design and implementation of hardware and software). Computer science is generally considered an area of academic research and distinct from computer programming.Algorithms and data structures are central to computer science.
The theory of computation concerns abstract models of computation and general classes of problems that can be solved using them. The fields of cryptography and computer security involve studying the means for secure communication and for preventing security vulnerabilities. Computer graphics and computational geometry address the generation of images. Programming language theory considers different ways to describe computational processes, and database theory concerns the management of repositories of data. Human–computer interaction investigates the interfaces through which humans and computers interact, and software engineering focuses on the design and principles behind developing software. Areas such as operating systems, networks and embedded systems investigate the principles and design behind complex systems. Computer architecture describes the construction of computer components and computer-operated equipment. Artificial intelligence and machine learning aim to synthesize goal-orientated processes such as problem-solving, decision-making, environmental adaptation, planning and learning found in humans and animals. Within artificial intelligence, computer vision aims to understand and process image and video data, while natural language processing aims to understand and process textual and linguistic data.
The fundamental concern of computer science is determining what can and cannot be automated. The Turing Award is generally recognized as the highest distinction in computer science.",Computer science,1,1
114,280911,"In frequentist statistics, a confidence interval (CI) is a range of estimates for an unknown parameter. A confidence interval is computed at a designated confidence level; the 95% confidence level is most common, but other levels, such as 90% or 99%, are sometimes used. The confidence level represents the long-run proportion of corresponding CIs that contain the true value of the parameter. For example, out of all intervals computed at the 95% level, 95% of them should contain the parameter's true value.Factors affecting the width of the CI include the confidence level, the sample size, and the variability in the sample. All else being the same, a larger sample would produce a narrower confidence interval. Likewise, greater variability in the sample produces a wider confidence interval, and a higher confidence level would demand a wider confidence interval.",Confidence interval,1,1
116,3105999,"In statistics, a confounder (also confounding variable, confounding factor, extraneous determinant or lurking variable) is a variable that influences both the dependent variable and independent variable, causing a spurious association.  Confounding is a causal concept, and as such, cannot be described in terms of correlations or associations. The existence of confounders is an important quantitative explanation why correlation does not imply causation.",Confounding,1,1
118,296457,"Conjoint analysis is a survey-based statistical technique used in market research that helps determine how people value different attributes (feature, function, benefits) that make up an individual product or service.
The objective of conjoint analysis is to determine what combination of a limited number of attributes is most influential on respondent choice or decision making. A controlled set of potential products or services is shown to survey respondents and by analyzing how they make choices among these products, the implicit valuation of the individual elements making up the product or service can be determined. These implicit valuations (utilities or part-worths) can be used to create market models that estimate market share, revenue and even profitability of new designs.
Conjoint analysis originated in mathematical psychology and was developed by marketing professor Paul E. Green at the Wharton School of the University of Pennsylvania. Other prominent conjoint analysis pioneers include professor V. ""Seenu"" Srinivasan of Stanford University who developed a linear programming (LINMAP) procedure for rank ordered data as well as a self-explicated approach, and Jordan Louviere (University of Iowa) who invented and developed choice-based approaches to conjoint analysis and related techniques such as best–worst scaling.
Today it is used in many of the social sciences and applied sciences including marketing, product management, and operations research. It is used frequently in testing customer acceptance of new product designs, in assessing the appeal of advertisements and in service design. It has been used in product positioning, but there are some who raise problems with this application of conjoint analysis.
Conjoint analysis techniques may also be referred to as multiattribute compositional modelling, discrete choice modelling, or stated preference research, and are part of a broader set of trade-off analysis tools used for systematic analysis of decisions. These tools include Brand-Price Trade-Off, Simalto, and mathematical approaches such as AHP, PAPRIKA, evolutionary algorithms or rule-developing experimentation.",Conjoint analysis,1,0
119,1497569,"In statistics, a consistent estimator or asymptotically  consistent estimator is an estimator—a rule for computing estimates of a parameter θ0—having the property that as the number of data points used increases indefinitely, the resulting sequence of estimates converges in probability to θ0. This means that the distributions of the estimates become more and more concentrated near the true value of the parameter being estimated, so that the probability of the estimator being arbitrarily close to θ0 converges to one.
In practice one constructs an estimator as a function of an available sample of size n, and then imagines being able to keep collecting data and expanding the sample ad infinitum. In this way one would obtain a sequence of estimates indexed by n, and consistency is a property of what occurs as the sample size “grows to infinity”. If the sequence of estimates can be mathematically shown to converge in probability to the true value θ0, it is called a consistent estimator; otherwise the estimator is said to be inconsistent.
Consistency as defined here is sometimes referred to as weak consistency. When we replace convergence in probability with almost sure convergence, then the estimator is said to be strongly consistent. Consistency is related to bias; see bias versus consistency.",Consistent estimator,1,0
120,935515,"In statistics, a contingency table (also known as a cross tabulation or crosstab) is a type of table in a matrix format that displays the (multivariate) frequency distribution of the variables. They are heavily used in survey research, business intelligence, engineering, and scientific research. They provide a basic picture of the interrelation between two variables and can help find interactions between them. The term contingency table was first used by Karl Pearson in ""On the Theory of Contingency and Its Relation to Association and Normal Correlation"", part of the Drapers' Company Research Memoirs Biometric Series I published in 1904.
A crucial problem of multivariate statistics is finding the (direct-)dependence structure underlying the variables contained in high-dimensional contingency tables. If some of the conditional independences are revealed, then  even the storage of the data can be done in a smarter way (see Lauritzen (2002)). In order to do this one can use information theory concepts, which gain the information only from the distribution of probability, which can be expressed easily from the contingency table by the relative frequencies.
A pivot table is a way to create contingency tables using spreadsheet software.

",Contingency table,1,0
121,23543,"In probability theory and statistics, a probability distribution is the mathematical function that gives the probabilities of occurrence of different possible outcomes for an experiment. It is a mathematical description of a random phenomenon in terms of its sample space and the probabilities of events (subsets of the sample space).For instance, if X is used to denote the outcome of a coin toss (""the experiment""), then the probability distribution of X would take the value 0.5 (1 in 2 or 1/2) for X = heads, and 0.5 for X = tails (assuming that the coin is fair). Examples of random phenomena include the weather conditions at some future date, the height of a randomly selected person, the fraction of male students in a school, the results of a survey to be conducted, etc.",Probability distribution,1,1
122,435754,"Control charts, also known as Shewhart charts (after Walter A. Shewhart) or process-behavior charts, are a statistical process control tool used to determine if a manufacturing or business process is in a state of control.  It is more appropriate to say that the control charts are the graphical device for Statistical  Process Monitoring (SPM). Traditional control charts are mostly designed to monitor process parameters when underlying form of the process distributions are known. However, more advanced techniques are available in the 21st century where incoming data streaming can-be monitored even without any knowledge of the underlying process distributions.  Distribution-free control charts are becoming increasingly popular.",Control chart,1,0
123,1765232,"In the design of experiments, hypothesis are applied to experimental units in a treatment group. In comparative experiments, members of a control group receive a standard treatment, a placebo, or no treatment at all. There may be more than one treatment group, more than one control group, or both.
A placebo control group can be used to support a double-blind study, in which some subjects are given an ineffective treatment (in medical studies typically a sugar pill) to minimize differences in the experiences of subjects in the different groups; this is done in a way that ensures no participant in the experiment (subject or experimenter) knows to which group each subject belongs. In such cases, a third, non-treatment control group can be used to measure the placebo effect directly, as the difference between the responses of placebo subjects and untreated subjects, perhaps paired by age group or other factors (such as being twins).
For the conclusions drawn from the results of an experiment to have validity, it is essential that the items or patients assigned to treatment and control groups  be representative of the same population. In some experiments, such as many in agriculture or psychology, this can be achieved by randomly assigning items from a common population to one of the treatment and control groups. In studies of twins involving just one treatment group and a control group, it is statistically efficient to do this random assignment separately for each pair of twins, so that one is in the treatment group and one in the control group.In some medical studies, where it may be unethical not to treat patients who present with symptoms, controls may be given a standard treatment, rather than no treatment at all. An alternative is to select controls from a wider population, provided that this population is well-defined and that those presenting with symptoms at the clinic are representative of those in the wider population. Another method to reduce ethical concerns would be to test early-onset symptoms, with enough time later to offer real treatments to the control subjects, and let those subjects know the first treatments are ""experimental"" and might not be as effective as later treatments, again with the understanding there would be ample time to try other remedies.",Treatment and control groups,1,0
124,7039,"Control theory deals with the control of dynamical systems in engineered processes and machines. The objective is to develop a model or algorithm governing the application of system inputs to drive the system to a desired state, while minimizing any delay, overshoot, or steady-state error and ensuring a level of control stability; often with the aim to achieve a degree of optimality.
To do this, a controller with the requisite corrective behavior is required. This controller monitors the controlled process variable (PV), and compares it with the reference or set point (SP). The difference between actual and desired value of the process variable, called the error signal, or SP-PV error, is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point. Other aspects which are also studied are  controllability and observability. This is the basis for the advanced type of automation that revolutionized manufacturing, aircraft, communications and other industries. This is feedback control, which involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range by means of a ""final control element"", such as a control valve.Extensive use is usually made of a diagrammatic style known as the block diagram. In it the transfer function, also known as the system function or network function, is a mathematical model of the relation between the input and output based on the differential equations describing the system.
Control theory dates from the 19th century, when the theoretical basis for the operation of governors was first described by James Clerk Maxwell.  Control theory was further advanced by Edward Routh in 1874, Charles Sturm and in 1895, Adolf Hurwitz, who all contributed to the establishment of control stability criteria; and from 1922 onwards, the development of PID control theory by Nicolas Minorsky.
Although a major application of mathematical control theory is in control systems engineering, which deals with the design of process control systems for industry, other applications range far beyond this. As the general theory of feedback systems, control theory is useful wherever feedback occurs - thus control theory also has applications in life sciences, computer engineering, sociology and operations research.",Control theory,1,1
125,50723,"In probability theory, there exist several different notions of convergence of random variables.  The convergence of sequences of random variables to some limit random variable is an important concept in probability theory, and its applications to statistics and stochastic processes. The same concepts are known in more general mathematics as stochastic convergence and they formalize the idea that a sequence of essentially random or unpredictable events can sometimes be expected to settle down into a behavior that is essentially unchanging when items far enough into the sequence are studied. The different possible notions of convergence relate to how such a behavior can be characterized: two readily understood behaviors are that the sequence eventually takes a constant value, and that values in the sequence continue to change but can be described by an unchanging probability distribution.

",Convergence of random variables,1,0
126,157057,"In statistics, correlation  or dependence  is any statistical relationship, whether causal or not, between two random variables or bivariate data. Although in the broadest sense, ""correlation"" may indicate any type of association, in statistics it normally refers to the degree to which a pair of variables are linearly related.  
Familiar examples of dependent phenomena include the correlation between the height of parents and their offspring, and the correlation between the price of a good and the quantity the consumers are willing to purchase, as it is depicted in the so-called demand curve.
Correlations are useful because they can indicate a predictive relationship that can be exploited in practice.  For example, an electrical utility may produce less power on a mild day based on the correlation between electricity demand and weather. In this example, there is a causal relationship, because extreme weather causes people to use more electricity for heating or cooling. However, in general, the presence of a correlation is not sufficient to infer the presence of a causal relationship (i.e., correlation does not imply causation).
Formally, random variables are dependent if they do not satisfy a mathematical property of probabilistic independence.  In informal parlance, correlation is synonymous with dependence. However, when used in a technical sense, correlation refers to any of several specific types of mathematical operations between the tested variables and their respective expected values. Essentially, correlation is the measure of how two or more variables are related to one another.  There are several correlation coefficients, often denoted 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   or 
  
    
      
        r
      
    
    {\displaystyle r}
  , measuring the degree of correlation.  The most common of these is the Pearson correlation coefficient, which is sensitive only to a linear relationship between two variables (which may be present even when one variable is a nonlinear function of the other).  Other correlation coefficients – such as Spearman's rank correlation – have been developed to be more robust than Pearson's, that is, more sensitive to nonlinear relationships. Mutual information can also be applied to measure dependence between two variables.",Correlation,1,1
128,39834,"The phrase ""correlation does not imply causation"" refers to the inability to legitimately deduce a cause-and-effect relationship between two events or  variables solely on the basis of an observed association or correlation between them. The idea that ""correlation implies causation"" is an example of a questionable-cause logical fallacy, in which two events occurring together are taken to have established a cause-and-effect relationship. This fallacy is also known by the Latin phrase cum hoc ergo propter hoc ('with this, therefore because of this'). This differs from the fallacy known as post hoc ergo propter hoc (""after this, therefore because of this""), in which an event following another is seen as a necessary consequence of the former event, and from conflation, the errant merging of two events, ideas, databases, etc., into one.
As with any logical fallacy, identifying that the reasoning behind an argument is flawed does not necessarily imply that the resulting conclusion is false. Statistical methods have been proposed that use correlation as the basis for hypothesis tests for causality, including the Granger causality test and convergent cross mapping.

",Correlation does not imply causation,1,0
129,5483693,"In the analysis of data, a correlogram is a chart of correlation statistics.  
For example, in time series analysis, a plot of the sample autocorrelations 
  
    
      
        
          r
          
            h
          
        
        
      
    
    {\displaystyle r_{h}\,}
   versus 
  
    
      
        h
        
      
    
    {\displaystyle h\,}
   (the time lags) is an autocorrelogram. 
If cross-correlation is plotted, the result is called a cross-correlogram.
The correlogram is a commonly used tool for checking randomness in a data set. If random, autocorrelations should be near zero for any and all time-lag separations. If non-random, then one or more of the autocorrelations will be significantly non-zero.
In addition, correlograms are used in the model identification stage for Box–Jenkins autoregressive moving average time series models. Autocorrelations should be near-zero for randomness; if the analyst does not check for randomness, then the validity of many of the statistical conclusions becomes suspect. The correlogram is an excellent way of checking for such randomness.
Sometimes, corrgrams, color-mapped matrices of correlation strengths in multivariate analysis, are also called correlograms.",Correlogram,1,0
130,15690807,"In statistics, count data is a statistical data type describing countable quantities, data which can take only the counting numbers, non-negative integer values {0, 1, 2, 3, ...}, and where these integers arise from counting rather than ranking.  The statistical treatment of count data is distinct from that of binary data, in which the observations can take only two values, usually represented by 0 and 1, and from ordinal data, which may also consist of integers but where the individual values fall on an arbitrary scale and only the relative ranking is important",Count data,1,0
131,53458,"In linguistics, a  count noun (also countable noun) is a noun that can be modified by a numeral and that occurs in both singular and plural forms, and that can co-occur with quantificational determiners like every, each, several, etc. A mass noun has none of these properties, because it cannot be modified by a numeral, cannot occur in plural, and cannot co-occur with quantificational determiners.",Count noun,1,0
132,3408427,"In Bayesian statistics, a credible interval is an interval within which an unobserved parameter value falls with a particular probability. It is an interval in the domain of a posterior probability distribution or a predictive distribution. The generalisation to multivariate problems is the credible region. Credible intervals are analogous to confidence intervals in frequentist statistics, although they differ on a philosophical basis: Bayesian intervals treat their bounds as fixed and the estimated parameter as a random variable, whereas frequentist confidence intervals treat their bounds as random variables and the parameter as a fixed value.  Also, Bayesian credible intervals use (and indeed, require) knowledge of the situation-specific prior distribution, while the frequentist confidence intervals do not.
For example, in an experiment that determines the distribution of possible values of the parameter 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  , if the subjective probability that 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   lies between 35 and 45 is 0.95, then 
  
    
      
        35
        ≤
        μ
        ≤
        45
      
    
    {\displaystyle 35\leq \mu \leq 45}
   is a 95% credible interval.",Credible interval,1,0
133,648942,"Cricket is a sport that generates a variety of statistics.
Statistics are recorded for each player during a match, and aggregated over a career. At the professional level, statistics for Test cricket, one-day internationals, and first-class cricket are recorded separately. However, since Test matches are a form of first-class cricket, a player's first-class statistics will include their Test match statistics – but not vice versa. Nowadays records are also maintained for List A and Twenty20 limited over matches. These matches are normally limited over games played domestically at the national level by leading Test nations.  Since one-day internationals are a form of List A limited over matches, a player's List A statistics will include their ODI match statistics – but not vice versa.

",Cricket statistics,1,0
134,177487,"Crime statistics refer to systematic, quantitative results about crime, as opposed to crime news or anecdotes. Notably, crime statistics can be the result of two rather different processes:

scientific research, such as criminological studies, victimisation surveys;
official figures, such as published by the police, prosecution, courts, and prisons.However, in their research, criminologists often draw on official figures as well.",Crime statistics,1,0
135,30284,"A statistical hypothesis test is a method of statistical inference used to decide whether the data at hand sufficiently support a particular hypothesis.
Hypothesis testing allows us to make probabilistic statements about population parameters.",Statistical hypothesis testing,1,1
136,714163,"In signal processing, cross-correlation is a measure of similarity of two series as a function of the displacement of one relative to the other. This is also known as a sliding dot product or sliding inner-product. It is commonly used for searching a long signal for a shorter, known feature. It has applications in pattern recognition, single particle analysis, electron tomography, averaging, cryptanalysis, and neurophysiology. The cross-correlation is similar in nature to the convolution of two functions.  In an autocorrelation, which is the cross-correlation of a signal with itself, there will always be a peak at a lag of zero, and its size will be the signal energy.
In probability and statistics, the term cross-correlations refers to the correlations between the entries of two random vectors 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   and 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
  , while the correlations of a random vector 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   are the correlations between the entries of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   itself, those forming the correlation matrix of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  . If each of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   and 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
   is a scalar random variable which is realized repeatedly in a time series, then the correlations of the various temporal instances of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   are known as autocorrelations of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
  , and the cross-correlations of 
  
    
      
        
          X
        
      
    
    {\displaystyle \mathbf {X} }
   with 
  
    
      
        
          Y
        
      
    
    {\displaystyle \mathbf {Y} }
   across time are temporal cross-correlations. In probability and statistics, the definition of correlation always includes a standardising factor in such a way that correlations have values between −1 and +1.
If 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are two independent random variables with probability density functions 
  
    
      
        f
      
    
    {\displaystyle f}
   and 
  
    
      
        g
      
    
    {\displaystyle g}
  , respectively, then the probability density of the difference 
  
    
      
        Y
        −
        X
      
    
    {\displaystyle Y-X}
   is formally given by the cross-correlation (in the signal-processing sense) 
  
    
      
        f
        ⋆
        g
      
    
    {\displaystyle f\star g}
  ; however, this terminology is not used in probability and statistics. In contrast, the convolution 
  
    
      
        f
        ∗
        g
      
    
    {\displaystyle f*g}
   (equivalent to the cross-correlation of 
  
    
      
        f
        (
        t
        )
      
    
    {\displaystyle f(t)}
   and 
  
    
      
        g
        (
        −
        t
        )
      
    
    {\displaystyle g(-t)}
  ) gives the probability density function of the sum 
  
    
      
        X
        +
        Y
      
    
    {\displaystyle X+Y}
  .

",Cross-correlation,1,0
137,1557627,"In medical research, social science, and biology, a cross-sectional study (also known as a cross-sectional analysis, transverse study, prevalence study) is a type of observational study that analyzes data from a population, or a representative subset, at a specific point in time—that is, cross-sectional data.
In economics, cross-sectional studies typically involve the use of cross-sectional regression, in order to sort out the existence and magnitude of causal effects of one independent variable upon a dependent variable of interest at a given point in time. They differ from time series analysis, in which the behavior of one or more economic aggregates is traced through time.
In medical research, cross-sectional studies differ from case-control studies in that they aim to provide data on the entire population under study, whereas case-control studies typically include only individuals who have developed a specific condition and compare them with a matched sample, often a tiny minority, of the rest of the population. Cross-sectional studies are descriptive studies (neither longitudinal nor experimental). Unlike case-control studies, they can be used to describe, not only the odds ratio, but also absolute risks and relative risks from prevalences (sometimes called prevalence risk ratio, or PRR).  They may be used to describe some feature of the population, such as prevalence of an illness, but cannot prove cause and effect. Longitudinal studies differ from both in making a series of observations more than once on members of the study population over a period of time.",Cross-sectional study,1,0
138,416612,"Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set.
Cross-validation is a resampling method that uses different portions of the data to test and train a model on different iterations. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice.  In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem).
One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance.
In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.",Cross-validation (statistics),1,1
139,7088,This is a list of cryptographers. Cryptography is the practice and study of techniques for secure communication in the presence of third parties called adversaries.,List of cryptographers,1,0
140,45087831,"The cubic mean (written as 
  
    
      
        
          
            
              
                x
                ¯
              
            
          
          
            
              c
              u
              b
              i
              c
            
          
        
      
    
    {\displaystyle {\bar {x}}_{\mathrm {cubic} }}
  ) is a specific instance of the generalized mean with 
  
    
      
        p
        =
        3
      
    
    {\displaystyle p=3}
  .",Cubic mean,1,0
141,316593,"Darrell Huff (July 15, 1913 – June 27, 2001) was an American writer, and is best known as the author of How to Lie with Statistics (1954), the best-selling statistics book of the second half of the twentieth century, and for his use of statistics as a tobacco lobbyist.",Darrell Huff,1,0
142,2720954,"Data analysis  is a process of inspecting, cleansing, transforming, and modelling data with the goal of discovering useful information, informing conclusions, and supporting decision-making. Data analysis has multiple facets and approaches, encompassing diverse techniques under a variety of names, and is used in different business, science, and social science domains. In today's business world, data analysis plays a role in making decisions more scientific and helping businesses operate more effectively.Data mining is a particular data analysis technique that focuses on statistical modelling and knowledge discovery for predictive rather than purely descriptive purposes, while business intelligence covers data analysis that relies heavily on aggregation, focusing mainly on business information. In statistical applications, data analysis can be divided into descriptive statistics, exploratory data analysis (EDA), and confirmatory data analysis (CDA). EDA focuses on discovering new features in the data while CDA focuses on confirming or falsifying existing hypotheses. Predictive analytics focuses on the application of statistical models for predictive forecasting or classification, while text analytics applies statistical, linguistic, and structural techniques to extract and classify information from textual sources, a species of unstructured data. All of the above are varieties of data analysis.Data integration is a precursor to data analysis, and data analysis is closely linked to data visualization and data dissemination.",Data analysis,1,0
143,11501746,"Data collection is the process of gathering and measuring information on targeted variables in an established system, which then enables one to answer relevant questions and evaluate outcomes. Data collection is a research component in all study fields, including physical and social sciences, humanities, and business. While methods vary by discipline, the emphasis on ensuring accurate and honest collection remains the same. The goal for all data collection is to capture quality evidence that allows analysis to lead to the formulation of convincing and credible answers to the questions that have been posed. Data collection and validation consists of four steps when it involves taking a census and seven steps when it involves sampling.Regardless of the field of study or preference for defining data (quantitative or qualitative), accurate data collection is essential to maintain research integrity. The selection of appropriate data collection instruments (existing, modified, or newly developed) and delineated instructions for their correct use reduce the likelihood of errors.
A formal data collection process is necessary as it ensures that the data gathered are both defined and accurate. This way, subsequent decisions based on arguments embodied in the findings are made using valid data. The process provides both a baseline from which to measure and in certain cases an indication of what to improve.
There are 5 common data collection methods:

closed-ended surveys and quizzes,
open-ended surveys and questionnaires,
1-on-1 interviews,
focus groups, and
direct observation.

",Data collection,1,1
144,42253,"Data mining is the process of extracting and discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. Data mining is an interdisciplinary subfield of computer science and statistics with an overall goal of extracting information (with intelligent methods) from a data set and transforming the information into a comprehensible structure for further use. Data mining is the analysis step of the ""knowledge discovery in databases"" process, or KDD. Aside from the raw analysis step, it also involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating.The term ""data mining"" is a misnomer because the goal is the extraction of patterns and knowledge from large amounts of data, not the extraction (mining) of data itself. It also is a buzzword and is frequently applied to any form of large-scale data or information processing (collection, extraction, warehousing, analysis, and statistics) as well as any application of computer decision support system, including artificial intelligence (e.g., machine learning) and business intelligence. The book Data mining: Practical machine learning tools and techniques with Java (which covers mostly machine learning material) was originally to be named Practical machine learning, and the term data mining was only added for marketing reasons. Often the more general terms (large scale) data analysis and analytics—or, when referring to actual methods, artificial intelligence and machine learning—are more appropriate.
The actual data mining task is the semi-automatic or automatic analysis of large quantities of data to extract previously unknown, interesting patterns such as groups of data records (cluster analysis), unusual records (anomaly detection), and dependencies (association rule mining, sequential pattern mining). This usually involves using database techniques such as spatial indices. These patterns can then be seen as a kind of summary of the input data, and may be used in further analysis or, for example, in machine learning and predictive analytics. For example, the data mining step might identify multiple groups in the data, which can then be used to obtain more accurate prediction results by a decision support system. Neither the data collection, data preparation, nor result interpretation and reporting is part of the data mining step, although they do belong to the overall KDD process as additional steps.
The difference between data analysis and data mining is that data analysis is used to test models and hypotheses on the dataset, e.g., analyzing the effectiveness of a marketing campaign, regardless of the amount of data. In contrast, data mining uses machine learning and statistical models to uncover clandestine or hidden patterns in a large volume of data.The related terms data dredging, data fishing, and data snooping refer to the use of data mining methods to sample parts of a larger population data set that are (or may be) too small for reliable statistical inferences to be made about the validity of any patterns discovered. These methods can, however, be used in creating new hypotheses to test against the larger data populations.

",Data mining,1,1
145,35458904,"Data science is an interdisciplinary field that uses scientific methods, processes, algorithms and systems to extract knowledge and insights from noisy, structured and unstructured data, and apply knowledge from data across a broad range of application domains. Data science is related to data mining, machine learning and big data.Data science is a ""concept to unify statistics, data analysis, informatics, and their related methods"" in order to ""understand and analyse actual phenomena"" with data. It uses techniques and theories drawn from many fields within the context of mathematics, statistics, computer science, information science, and domain knowledge. However, data science is different from computer science and information science. Turing Award winner Jim Gray imagined data science as a ""fourth paradigm"" of science (empirical, theoretical, computational, and now data-driven) and asserted that ""everything about science is changing because of the impact of information technology"" and the data deluge.A data scientist is someone who creates programming code and combines it with statistical knowledge to create insights from data.

",Data science,1,1
146,8495,"A data set (or dataset) is a collection of data. In the case of tabular data, a data set corresponds to one or more database tables, where every column of a table represents a particular variable, and each row corresponds to a given record of the data set in question. The data set lists values for each of the variables, such as for example height and weight of an object, for each member of the data set.  Data sets can also consist of a collection of documents or files.In the open data discipline, data set is the unit to measure the information released in a public open data repository.  The European Open Data portal aggregates more than half a million data sets.  Some other issues (real-time data sources, non-relational data sets, etc.) increases the difficulty to reach a consensus about it.",Data set,1,0
147,93817,"In computer science and computer programming, a data type or simply type is an attribute of data which tells the compiler or interpreter how the programmer intends to use the data. Most programming languages support basic data types of integer numbers (of varying sizes), floating-point numbers (which approximate real numbers), characters and Booleans. A data type constrains the possible values that an expression, such as a variable or a function, might take. This data type defines the operations that can be done on the data, the meaning of the data, and the way values of that type can be stored.

",Data type,1,0
148,20574280,"David Amiel Freedman (5 March 1938 – 17 October 2008) was Professor of Statistics at the University of California, Berkeley. He was a distinguished mathematical statistician whose wide-ranging research included the analysis of martingale inequalities, Markov processes, de Finetti's theorem, consistency of Bayes estimators, sampling, the bootstrap, and procedures for testing and evaluating models. He published extensively on methods for causal inference and the behavior of standard statistical models under non-standard conditions – for example, how regression models behave when fitted to data from randomized experiments.  Freedman also wrote widely on the application—and misapplication—of statistics in the social sciences, including epidemiology, public policy, and law.",David A. Freedman,1,0
149,469578,"A decision support system (DSS) is an information system that supports business or organizational decision-making activities. DSSs serve the management, operations and planning levels of an organization (usually mid and higher management) and help people make decisions about problems that may be rapidly changing and not easily specified in advance—i.e. unstructured and semi-structured decision problems. Decision support systems can be either fully computerized or human-powered, or a combination of both.
While academics have perceived DSS as a tool to support decision making processes, DSS users see DSS as a tool to facilitate organizational processes. Some authors have extended the definition of DSS to include any system that might support decision making and some DSS include a decision-making software component; Sprague (1980) defines a properly termed DSS as follows:

DSS tends to be aimed at the less well structured, underspecified problem that upper level managers typically face;
DSS attempts to combine the use of models or analytic techniques with traditional data access and retrieval functions;
DSS specifically focuses on features which make them easy to use by non-computer-proficient people in an interactive mode; and
DSS emphasizes flexibility and adaptability to accommodate changes in the environment and the decision making approach of the user.DSSs include knowledge-based systems. A properly designed DSS is an interactive software-based system intended to help decision makers compile useful information from a combination of raw data, documents, and personal knowledge, or business models to identify and solve problems and make decisions.
Typical information that a decision support application might gather and present includes:

inventories of information assets (including legacy and relational data sources, cubes, data warehouses, and data marts),
comparative sales figures between one period and the next,
projected revenue figures based on product sales assumptions.

",Decision support system,1,1
150,232602,"A decision tree  is a decision support tool that uses a tree-like model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.
Decision trees are commonly used in operations research, specifically in decision analysis, to help identify a strategy most likely to reach a goal, but are also a popular tool in machine learning.",Decision tree,1,1
151,7579995,"The decomposition of time series is a statistical task that deconstructs a time series into several components, each representing one of the underlying categories of patterns. There are two principal types of decomposition, which are outlined below.

",Decomposition of time series,1,0
152,61093,"Deductive reasoning is the mental process of drawing deductive inferences. An inference is deductively valid if its conclusion follows logically from its premises, i.e. if it is impossible for the premises to be true and the conclusion to be false. For example, the inference from the premises ""all men are mortal"" and ""Socrates is a man"" to the conclusion ""Socrates is mortal"" is deductively valid. An argument is sound if it is valid and all its premises are true. Some theorists define deduction in terms of the intentions of the author: they have to intend for the premises to offer deductive support to the conclusion. With the help of this modification, it is possible to distinguish valid from invalid deductive reasoning: it is invalid if the author's belief about the deductive support is false, but even invalid deductive reasoning is a form of deductive reasoning. 
Psychology is interested in deductive reasoning as a psychological process, i.e. how people actually draw inferences. Logic, on the other hand, focuses on the deductive relation of logical consequence between the premises and the conclusion or how people should draw inferences. There are different ways of conceptualizing this relation. According to the semantic approach, an argument is deductively valid if and only if there is no possible interpretation of this argument where its premises are true and its conclusion is false. The syntactic approach, on the other hand, holds that an argument is deductively valid if and only if its conclusion can be deduced from its premises using a valid rule of inference. A rule of inference is a schema of drawing a conclusion from a set of premises based only on their logical form. There are various rules of inference, like the modus ponens and the modus tollens. Invalid deductive arguments, which do not follow a rule of inference, are called formal fallacies. Rules of inference are definitory rules and contrast to strategic rules, which specify what inferences one needs to draw in order to arrive at an intended conclusion. Deductive reasoning contrasts with non-deductive or ampliative reasoning. For ampliative arguments, like inductive or abductive arguments, the premises offer weaker support to their conclusion: they make it more likely but they do not guarantee its truth. They make up for this drawback by being able to provide genuinely new information not already found in the premises, unlike deductive arguments.
Cognitive psychology investigates the mental processes responsible for deductive reasoning. One of its topics concerns the factors determining whether people draw valid or invalid deductive inferences. One factor is the form of the argument: for example, people are more successful for arguments of the form modus ponens than for modus tollens. Another is the content of the arguments: people are more likely to believe that an argument is valid if the claim made in its conclusion is plausible. A general finding is that people tend to perform better for realistic and concrete cases than for abstract cases. Psychological theories of deductive reasoning aim to explain these findings by providing an account of the underlying psychological processes. Mental logic theories hold that deductive reasoning is a language-like process that happens through the manipulation of representations using rules of inference. Mental model theories, on the other hand, claim that deductive reasoning involves models of possible states of the world without the medium of language or rules of inference. According to dual-process theories of reasoning, there are two qualitatively different cognitive systems responsible for reasoning.
The problem of deductive reasoning is relevant to various fields and issues. Epistemology tries to understand how justification is transferred from the belief in the premises to the belief in the conclusion in the process of deductive reasoning. Probability logic studies how the probability of the premises of an inference affects the probability of its conclusion. The controversial thesis of deductivism denies that there are other correct forms of inference besides deduction. Natural deduction is a type of proof system based on simple and self-evident rules of inference. In philosophy, the geometrical method is a way of philosophizing that starts from a small set of self-evident axioms and tries to build a comprehensive logical system using deductive reasoning.",Deductive reasoning,1,0
153,2321606,"In statistics, the number of degrees of freedom is the number of values in the final calculation of a statistic that are free to vary.Estimates of statistical parameters can be based upon different amounts of information or data. The number of independent pieces of information that go into the estimate of a parameter is called the degrees of freedom. In general, the degrees of freedom of an estimate of a parameter are equal to the number of independent scores that go into the estimate minus the number of parameters used as intermediate steps in the estimation of the parameter itself.  For example, if the variance is to be estimated from a random sample of N independent scores, then the degrees of freedom is equal to the number of independent scores (N) minus the number of parameters estimated as intermediate steps (one, namely, the sample mean) and is therefore equal to N − 1.Mathematically, degrees of freedom is the number of dimensions of the domain of a random vector, or essentially the number of ""free"" components (how many components need to be known before the vector is fully determined).
The term is most often used in the context of linear models (linear regression, analysis of variance), where certain random vectors are constrained to lie in linear subspaces, and the number of degrees of freedom is the dimension of the subspace. The degrees of freedom are also commonly associated with the squared lengths (or ""sum of squares"" of the coordinates) of such vectors, and the parameters of chi-squared and other distributions that arise in associated statistical testing problems.
While introductory textbooks may introduce degrees of freedom as distribution parameters or through hypothesis testing, it is the underlying geometry that defines degrees of freedom, and is critical to a proper understanding of the concept.",Degrees of freedom (statistics),1,0
154,8189,"Demographic statistics are measures of the characteristics of, or changes to, a population. Records of births, deaths, marriages, immigration and emigration and a regular census of population provide information that is key to making sound decisions about national policy.
A useful summary of such data is the population pyramid. It provides data about the sex and age distribution of the population in an accessible graphical format.
Another summary is called the life table. For a cohort of persons born in the same year, it traces and projects their life experiences from birth to death. For a given cohort, the proportion expected to survive each year (or decade in an abridged life table) is presented in tabular or graphical form.
The ratio of males to females by age indicates the consequences of differing mortality rates on the sexes. Thus, while values above one are common for newborns, the ratio dwindles until it is well below one for the older population.",Demographic statistics,1,1
155,18963910,"Geography (from Greek: γεωγραφία, geographia, literally ""earth description"") is a field of science devoted to the study of the lands, features, inhabitants, and phenomena of the Earth and planets. The first person to use the word γεωγραφία was Eratosthenes (276–194 BC). Geography is an all-encompassing discipline that seeks an understanding of Earth and its human and natural complexities—not merely where objects are, but also how they have changed and come to be.
Geography is often defined in terms of two branches: human geography and physical geography. Human geography is concerned with the study of people and their communities, cultures, economies, and interactions with the environment by studying their relations with and across space and place. Physical geography is concerned with the study of processes and patterns in the natural environment like the atmosphere, hydrosphere, biosphere, and geosphere.
The four historical traditions in geographical research are spatial analyses of the natural and the human phenomena, area studies of places and regions, studies of human-land relationships, and the Earth sciences. Geography has been called ""the world discipline"" and ""the bridge between the human and the physical sciences"".",Geography,1,1
156,554671,"In probability and statistics,
density estimation is the construction of an estimate, based on observed data, of an unobservable underlying probability density function.  The unobservable density function is thought of as the density according to which a large population is distributed; the data are usually thought of as a random sample from that population.
A variety of approaches to density estimation are used, including Parzen windows and a range of data clustering techniques, including vector quantization. The most basic form of density estimation is a rescaled histogram.",Density estimation,1,1
157,437701,"Dependent and Independent variables are variables in mathematical modeling, statistical modeling and experimental sciences. Dependent variables receive this name because, in an experiment, their values are studied under the supposition or demand that they depend, by some law or rule (e.g., by a mathematical function), on the values of other variables. Independent variables, in turn, are not seen as depending on any other variable in the scope of the experiment in question. In this sense, some common independent variables are time, space, density, mass, fluid flow rate, and previous values of some observed value of interest (e.g. human population size) to predict future values (the dependent variable).Of the two, it is always the dependent variable whose variation is being studied, by altering inputs, also known as regressors in a statistical context. In an experiment, any variable that can be attributed a value without attributing a value to any other variable is called an independent variable.  Models and experiments test the effects that the independent variables have on the dependent variables. Sometimes, even if their influence is not of direct interest, independent variables may be included for other reasons, such as to account for their potential confounding effect.",Dependent and independent variables,1,0
158,8187,"A descriptive statistic (in the count noun sense) is a summary statistic that quantitatively describes or summarizes features from a collection of information, while descriptive statistics (in the mass noun sense) is the process of using and analysing those statistics. Descriptive statistics is distinguished from inferential statistics (or inductive statistics) by its aim to summarize a sample, rather than use the data to learn about the population that the sample of data is thought to represent. This generally means that descriptive statistics, unlike inferential statistics, is not developed on the basis of probability theory, and are frequently nonparametric statistics. Even when a data analysis draws its main conclusions using inferential statistics, descriptive statistics are generally also presented. For example, in papers reporting on human subjects, typically a table is included giving the overall sample size, sample sizes in important subgroups (e.g., for each treatment or exposure group), and demographic or clinical characteristics such as the average age, the proportion of subjects of each sex, the proportion of subjects with related co-morbidities, etc.
Some measures that are commonly used to describe a data set are measures of central tendency and measures of variability or dispersion. Measures of central tendency include the mean, median and mode, while measures of variability include the standard deviation (or variance), the minimum and maximum values of the variables, kurtosis and skewness.",Descriptive statistics,1,0
159,9541,"The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.
In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as ""input variables"" or ""predictor variables."" The change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as ""output variables"" or ""response variables."" The experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. Experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources.  There are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment.
Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity.
Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making. The study of the design of experiments is an important topic in metascience.",Design of experiments,1,1
160,2473516,"In statistics, the Dickey–Fuller test tests the null hypothesis that a unit root is present in an autoregressive time series model. The alternative hypothesis is different depending on which version of the test is used, but is usually stationarity or trend-stationarity. The test is named after the statisticians David Dickey and Wayne Fuller, who developed it in 1979.",Dickey–Fuller test,1,0
161,3404894,"Difference in differences (DID or DD) is a statistical technique used in econometrics  and quantitative research in the social sciences that attempts to mimic an experimental research design using observational study data, by studying the differential effect of a treatment on a 'treatment group' versus a 'control group' in a natural experiment. It calculates the effect of a treatment (i.e., an explanatory variable or an independent variable) on an outcome (i.e., a response variable or dependent variable) by comparing the average change over time in the outcome variable for the treatment group to the average change over time for the control group. Although it is intended to mitigate the effects of extraneous factors and selection bias, depending on how the treatment group is chosen, this method may still be subject to certain biases (e.g., mean regression, reverse causality and omitted variable bias).
In contrast to a time-series estimate of the treatment effect on subjects (which analyzes differences over time) or a cross-section estimate of the treatment effect (which measures the difference between treatment and control groups), difference in differences uses panel data to measure the differences, between the treatment and control group, of the changes in the outcome variable that occur over time.

",Difference in differences,1,0
162,330206,"In mathematics, a differentiable function of one real variable is a function whose derivative exists at each point in its domain. In other words, the graph of a differentiable function has a non-vertical tangent line at each interior point in its domain. A differentiable function is smooth (the function is locally well approximated as a linear function at each interior point) and does not contain any break, angle, or cusp.
If x0 is an interior point in the domain of a function f, then f is said to be differentiable at x0 if the derivative 
  
    
      
        
          f
          ′
        
        (
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle f'(x_{0})}
   exists. In other words, the graph of f has a non-vertical tangent line at the point (x0, f(x0)). f is said to be differentiable on U if it is differentiable at every point of U. f is said to be continuously differentiable if its derivative is also a continuous function over the domain the function 
  
    
      
        f
      
    
    {\displaystyle f}
  . Generally speaking, f is said to be of class 
  
    
      
        
          C
          
            k
          
        
      
    
    {\displaystyle C^{k}}
   if its first 
  
    
      
        k
      
    
    {\displaystyle k}
   derivatives 
  
    
      
        
          f
          
            ′
          
        
        (
        x
        )
        ,
        
          f
          
            ′
            ′
          
        
        (
        x
        )
        ,
        …
        ,
        
          f
          
            (
            k
            )
          
        
        (
        x
        )
      
    
    {\displaystyle f^{\prime }(x),f^{\prime \prime }(x),\ldots ,f^{(k)}(x)}
   exist and are continuous over the domain of the function 
  
    
      
        f
      
    
    {\displaystyle f}
  .",Differentiable function,1,1
163,1424309,"In mathematics, a differential equation is an equation that relates one or more unknown functions and their derivatives. In applications, the functions generally represent physical quantities, the derivatives represent their rates of change, and the differential equation defines a relationship between the two. Such relations are common; therefore, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology.
Mainly the study of differential equations consists of the study of their solutions (the set of functions that satisfy each equation), and of the properties of their solutions. Only the simplest differential equations are solvable by explicit formulas; however, many properties of solutions of a given differential equation may be determined without computing them exactly.
Often when a closed-form expression for the solutions is not available, solutions may be approximated numerically using computers. The theory of dynamical systems puts emphasis on qualitative analysis of systems described by differential equations, while many numerical methods have been developed to determine solutions with a given degree of accuracy.",Differential equation,1,1
165,8625,"Differential geometry is a mathematical discipline that studies the geometry of smooth shapes and smooth spaces, otherwise known as smooth manifolds. It uses the techniques of differential calculus, integral calculus, linear algebra and multilinear algebra. The field has its origins in the study of spherical geometry as far back as antiquity. It also relates to astronomy, the geodesy of the Earth, and later  the study of hyperbolic geometry by Lobachevsky. The simplest examples of smooth spaces are the plane and space curves and surfaces in the three-dimensional Euclidean space, and the study of these shapes formed the basis for development of modern differential geometry during the 18th and 19th centuries.
Since the late 19th century, differential geometry has grown into a field concerned more generally with geometric structures on differentiable manifolds. A geometric structure is one which defines some notion of size, distance, shape, volume, or other rigidifying structure. For example, in Riemannian geometry distances and angles are specified, in symplectic geometry volumes may be computed, in conformal geometry only angles are specified, and in gauge theory certain fields are given over the space. Differential geometry is closely related to, and is sometimes taken to include, differential topology, which concerns itself with properties of differentiable manifolds which do not rely on any additional geometric structure (see that article for more discussion on the distinction between the two subjects). Differential geometry is also related to the geometric aspects of the theory of differential equations, otherwise known as geometric analysis.
Differential geometry finds applications throughout mathematics and the natural sciences. Most prominently the language of differential geometry was used by Albert Einstein in his theory of general relativity, and subsequently by physicists in the development of quantum field theory and the standard model of particle physics. Outside of physics, differential geometry finds applications in chemistry, economics, engineering, control theory, computer graphics and computer vision, and recently in machine learning.",Differential geometry,1,0
166,8562,"In mathematics, differential topology is the field dealing with the topological properties and smooth properties of smooth manifolds. In this sense differential topology is distinct from the closely related field of differential geometry, which concerns the geometric properties of smooth manifolds, including notions of size, distance, and rigid shape. By comparison differential topology is concerned with coarser properties, such as the number of holes in a manifold, its homotopy type, or the structure of its diffeomorphism group. Because many of these coarser properties may be captured algebraically, differential topology has strong links to algebraic topology.

The central goal of the field of differential topology is the classification of all smooth manifolds up to diffeomorphism. Since dimension is an invariant of smooth manifolds up to diffeomorphism type, this classification is often studied by classifying the (connected) manifolds in each dimension separately:

In dimension 1, the only smooth manifolds up to diffeomorphism are the circle, the real number line, and allowing a boundary, the half-closed interval 
  
    
      
        [
        0
        ,
        1
        )
      
    
    {\displaystyle [0,1)}
   and fully closed interval 
  
    
      
        [
        0
        ,
        1
        ]
      
    
    {\displaystyle [0,1]}
  .
In dimension 2, every closed surface is classified up to diffeomorphism by its genus, the number of holes (or equivalently its Euler characteristic), and whether or not it is orientable. This is the famous classification of closed surfaces. Already in dimension two the classification of non-compact surfaces becomes difficult, due to the existence of exotic spaces such as  Jacob's ladder.
In dimension 3, William Thurston's geometrization conjecture, proven by Grigori Perelman, gives a partial classification of compact three-manifolds. Included in this theorem is the Poincaré conjecture, which states that any closed, simply connected three-manifold is homeomorphic (and in fact diffeomorphic) to the 3-sphere.
Beginning in dimension 4, the classification becomes much more difficult for two reasons. Firstly, every finitely presented group appears as the fundamental group of some 4-manifold, and since the fundamental group is a diffeomorphism invariant, this makes the classification of 4-manifolds at least as difficult as the classification of finitely presented groups. By the word problem for groups, which is equivalent to the halting problem, it is impossible to classify such groups, so a full topological classification is impossible. Secondly, beginning in dimension four it is possible to have smooth manifolds that are homeomorphic, but with distinct, non-diffeomorphic smooth structures. This is true even for the Euclidean space 
  
    
      
        
          
            R
          
          
            4
          
        
      
    
    {\displaystyle \mathbb {R} ^{4}}
  , which admits many exotic 
  
    
      
        
          
            R
          
          
            4
          
        
      
    
    {\displaystyle \mathbb {R} ^{4}}
   structures. This means that the study of differential topology in dimensions 4 and higher must use tools genuinely outside the realm of the regular continuous topology of topological manifolds. One of the central open problems in differential topology is the four-dimensional smooth Poincaré conjecture, which asks if every smooth 4-manifold that is homeomorphic to the 4-sphere, is also diffeomorphic to it. That is, does the 4-sphere admit only one smooth structure? This conjecture is true in dimensions 1, 2, and 3, by the above classification results, but is known to be false in dimension 7 due to the Milnor spheres.
Important tools in studying the differential topology of smooth manifolds include the construction of smooth topological invariants of such manifolds, such as de Rham cohomology or the intersection form, as well as smoothable topological constructions, such as smooth surgery theory or the construction of cobordisms. Morse theory is an important tool which studies smooth manifolds by considering the critical points of differentiable functions on the manifold, demonstrating how the smooth structure of the manifold enters into the set of tools available. Often times more geometric or analytical techniques may be used, by equipping a smooth manifold with a Riemannian metric or by studying a differential equation on it. Care must be taken to ensure that the resulting information is insensitive to this choice of extra structure, and so genuinely reflects only the topological properties of the underlying smooth manifold. For example, the Hodge theorem provides a geometric and analytical interpretation of the de Rham cohomology, and gauge theory was used by Simon Donaldson to prove facts about the intersection form of simply connected 4-manifolds. In some cases techniques from contemporary physics may appear, such as topological quantum field theory, which can be used to compute topological invariants of smooth spaces.
Famous theorems in differential topology include the Whitney embedding theorem, the hairy ball theorem, the Hopf theorem, the Poincaré–Hopf theorem, Donaldson's theorem, and the Poincaré conjecture.",Differential topology,1,0
167,502205,"In mathematics, Diophantine geometry is the study of Diophantine equations by means of powerful methods in algebraic geometry. By the 20th century it became clear for some mathematicians that methods of algebraic geometry are ideal tools to study these equations.Four theorems in Diophantine geometry which are of fundamental importance include:
Mordell–Weil Theorem
Roth's Theorem
Siegel's Theorem
Falting's Theorem",Diophantine geometry,1,0
168,386468,"Discrete geometry and combinatorial geometry are branches of geometry that study combinatorial properties and constructive methods of discrete geometric objects.  Most questions in discrete geometry involve finite or discrete sets of basic geometric objects, such as points, lines, planes, circles, spheres, polygons, and so forth.  The subject focuses on the combinatorial properties of these objects, such as how they intersect one another, or how they may be arranged to cover a larger object.
Discrete geometry has a large overlap with convex geometry and computational geometry, and is closely related to subjects such as finite geometry, combinatorial optimization, digital geometry, discrete differential geometry, geometric graph theory, toric geometry, and combinatorial topology.",Discrete geometry,1,0
169,8492,"Discrete mathematics is the study of mathematical structures that can be considered ""discrete"" (in a way analogous to discrete variables, having a bijection with the set of natural numbers) rather than ""continuous"" (analogously to continuous functions). Objects studied in discrete mathematics include integers, graphs, and statements in logic. By contrast, discrete mathematics excludes topics in ""continuous mathematics"" such as real numbers, calculus or Euclidean geometry. Discrete objects can often be enumerated by integers; more formally, discrete mathematics has been characterized as the branch of mathematics dealing with countable sets (finite sets or sets with the same cardinality as the natural numbers). However, there is no exact definition of the term ""discrete mathematics"".The set of objects studied in discrete mathematics can be finite or infinite. The term finite mathematics is sometimes applied to parts of the field of discrete mathematics that deals with finite sets, particularly those areas relevant to business.
Research in discrete mathematics increased in the latter half of the twentieth century partly due to the development of digital computers which operate in ""discrete"" steps and store data in ""discrete"" bits. Concepts and notations from discrete mathematics are useful in studying and describing objects and problems in branches of computer science, such as computer algorithms, programming languages, cryptography, automated theorem proving, and software development. Conversely, computer implementations are significant in applying ideas from discrete mathematics to real-world problems.
Although the main objects of study in discrete mathematics are discrete objects, analytic methods from ""continuous"" mathematics are often employed as well.
In university curricula, ""Discrete Mathematics"" appeared in the 1980s, initially as a computer science support course; its contents were somewhat haphazard at the time. The curriculum has thereafter developed in conjunction with efforts by ACM and MAA into a course that is basically intended to develop mathematical maturity in first-year students; therefore, it is nowadays a prerequisite for mathematics majors in some universities as well. Some high-school-level discrete mathematics textbooks have appeared as well. At this level, discrete mathematics is sometimes seen as a preparatory course, not unlike precalculus in this respect.The Fulkerson Prize is awarded for outstanding papers in discrete mathematics.",Discrete mathematics,1,1
170,25896411,"In information geometry, a divergence is a kind of statistical distance: a binary function which establishes the separation from one probability distribution to another on a statistical manifold. 
The simplest divergence is squared Euclidean distance (SED), and divergences can be viewed as generalizations of SED. The other most important divergence is relative entropy (Kullback–Leibler divergence, KL divergence), which is central to information theory. There are numerous other specific divergences and classes of divergences, notably f-divergences and Bregman divergences (see § Examples).",Divergence (statistics),1,0
171,422994,"A digital object identifier (DOI) is a persistent identifier or handle used to uniquely identify various objects, standardized by the International Organization for Standardization (ISO). DOIs are an implementation of the Handle System; they also fit within the URI system (Uniform Resource Identifier). They are widely used to identify academic, professional, and government information, such as journal articles, research reports, data sets, and official publications. DOIs have also been used to identify other types of information resources, such as commercial videos.
A DOI aims to resolve to its target, the information object to which the DOI refers. This is achieved by binding the DOI to metadata about the object, such as a URL where the object is located. Thus, by being actionable and interoperable, a DOI differs from ISBNs or ISRCs which are identifiers only. The DOI system uses the indecs Content Model for representing metadata.
The DOI for a document remains fixed over the lifetime of the document, whereas its location and other metadata may change. Referring to an online document by its DOI should provide a more stable link than directly using its URL. But if its URL changes, the publisher must update the metadata for the DOI to maintain the link to the URL. It is the publisher's responsibility to update the DOI database. If they fail to do so, the DOI resolves to a dead link leaving the DOI useless.The developer and administrator of the DOI system is the International DOI Foundation (IDF), which introduced it in 2000. Organizations that meet the contractual obligations of the DOI system and are willing to pay to become a member of the system can assign DOIs. The DOI system is implemented through a federation of registration agencies coordinated by the IDF. By late April 2011 more than 50 million DOI names had been assigned by some 4,000 organizations, and by April 2013 this number had grown to 85 million DOI names assigned through 9,500 organizations.

",Digital object identifier,1,1
172,277248,"In a blind or blinded experiment, information which may influence the participants of the experiment is withheld until after the experiment is complete. Good blinding can reduce or eliminate experimental biases that arise from a participants' expectations, observer's effect on the participants, observer bias, confirmation bias, and other sources. A blind can be imposed on any participant of an experiment, including subjects, researchers, technicians, data analysts, and evaluators. In some cases, while blinding would be useful, it is impossible or unethical. For example, it is not possible to blind a patient to their treatment in a physical therapy intervention. A good clinical protocol ensures that blinding is as effective as possible within ethical and practical constraints.
During the course of an experiment, a participant becomes unblinded if they deduce or otherwise obtain information that has been masked to them. For example, a patient who experiences a side effect may correctly guess their treatment, becoming unblinded. Unblinding is common in blinded experiments, particularly in pharmacological trials. In particular, trials on pain medication and antidepressants are poorly blinded. Unblinding that occurs before the conclusion of a study is a source of experimental error, as the bias that was eliminated by blinding is re-introduced. The CONSORT reporting guidelines recommend that all studies assess and report unblinding. In practice, very few studies do so.Blinding is an important tool of the scientific method, and is used in many fields of research. In some fields, such as medicine, it is considered essential. In clinical research, a trial that is not a blinded trial is called an open trial.

",Blinded experiment,1,0
173,4864529,"In statistics, the Durbin–Watson statistic is a test statistic used to detect the presence of autocorrelation at lag 1 in the residuals (prediction errors) from a regression analysis. It is named after James Durbin and Geoffrey Watson. The small sample distribution of this ratio was derived by John von Neumann (von Neumann, 1941). Durbin and Watson (1950, 1951) applied this statistic to the residuals from least squares regressions, and developed bounds tests for the null hypothesis that the errors are serially uncorrelated against the alternative that they follow a first order autoregressive process. Note that the distribution of this test statistic does not depend on the estimated regression coefficients and the variance of the errors.A similar assessment can be also carried out with the Breusch–Godfrey test and the Ljung–Box test.",Durbin–Watson statistic,1,0
174,10390,"Econometrics is the application of statistical methods to economic data in order to give empirical content to economic relationships. More precisely, it is ""the quantitative analysis of actual economic phenomena based on the concurrent development of theory and observation, related by appropriate methods of inference"". An introductory economics textbook describes econometrics as allowing economists ""to sift through mountains of data to extract simple relationships"". Jan Tinbergen is one of the two founding fathers of econometrics. The other, Ragnar Frisch, also coined the term in the sense in which it is used today.A basic tool for econometrics is the multiple linear regression model. Econometric theory uses statistical theory and mathematical statistics to evaluate and develop econometric methods. Econometricians try to find estimators that have desirable statistical properties including unbiasedness, efficiency, and consistency. Applied econometrics uses theoretical econometrics and real-world data for assessing economic theories, developing econometric models, analysing economic history, and forecasting.",Econometrics,1,0
175,322317,"Economic data or economic statistics are data (quantitative measures) describing an actual economy, past or present. These are typically found in time-series form, that is, covering more than one time period (say the monthly unemployment rate for the last five years) or in cross-sectional data in one time period (say for consumption and income levels for sample households). Data may also be collected from surveys of for example individuals and firms or aggregated to sectors and industries of a single economy or for the international economy. A collection of such data in table form comprises a data set.
Methodological economic and statistical elements of the subject include measurement, collection, analysis, and publication of data. 'Economic statistics' may also refer to a subtopic of official statistics produced by official organizations (e.g. statistical institutes, intergovernmental organizations such as United Nations, European Union or OECD, central banks, ministries, etc.).  Economic data provide an empirical basis for economic research, whether descriptive or econometric. Data archives are also a key input for assessing the replicability of empirical findings and for use in decision making as to economic policy.
At the level of an economy, many data are organized and compiled according to the methodology of national accounting.  Such data include Gross National Product and its components, Gross National Expenditure, Gross National Income in the National Income and Product Accounts, and also the capital stock and national wealth. In these examples data may be stated in nominal or real values, that is, in money or inflation-adjusted terms. Other economic indicators include a variety of alternative measures of output, orders, trade, the labor force, confidence, prices, and financial series (e.g., money and interest rates). At the international level there are many series including international trade, international financial flows, direct investment flows (between countries) and exchange rates.
For time-series data, reported measurements can be hourly (e.g. for stock markets), daily, monthly, quarterly, or annually. Estimates such as averages are often subjected to seasonal adjustment to remove weekly or seasonal-periodicity elements, for example, holiday-period sales and seasonal unemployment.Within a country the data are usually produced by one or more statistical organizations, e.g., a governmental or quasi-governmental organization and/or the central banks.  International statistics are produced by several international bodies and firms, including the International Monetary Fund and the Bank for International Settlements.
Studies in experimental economics may also generate data, rather than using data collected for other purposes. Designed randomized experiments may provide more reliable conclusions than do observational studies. Like epidemiology, economics often studies the behavior of humans over periods too long to allow completely controlled experiments, in which case economists can use observational studies or quasi-experiments; in these studies, economists collect data which are then analyzed with statistical methods (econometrics).
Many methods can be used to analyse the data.  These include, e.g., time-series analysis using multiple regression, Box–Jenkins analysis, and seasonality analysis.  Analysis may be univariate (modeling one series) or multivariate (from several series). Econometricians, economic statisticians, and financial analysts formulate models, whether for past relationships or for economic forecasting. These models may include partial equilibrium microeconomics aimed at examining particular parts of an economy or economies, or they may cover a whole economic system, as in general equilibrium theory or in macroeconomics. Economists use these models to understand past events and to forecast future events, e.g., demand, prices and employment. Methods have also been developed for analyzing or correcting results from use of incomplete data and errors in variables.",Economic data,1,0
176,9223,"Economics () is the social science that studies the production, distribution, and consumption of goods and services.Economics focuses on the behaviour and interactions of economic agents and how economies work. Microeconomics is a field which analyzes what's viewed as basic elements in the economy, including individual agents and markets, their interactions, and the outcomes of interactions. Individual agents may include, for example, households, firms, buyers, and sellers. Macroeconomics analyzes the economy as a system where production, consumption, saving, and investment interact, and factors affecting it: employment of the resources of labour, capital, and land, currency inflation, economic growth, and public policies that have impact on these elements.
Other broad distinctions within economics include those between positive economics, describing ""what is"", and normative economics, advocating ""what ought to be""; between economic theory and applied economics; between rational and behavioural economics; and between mainstream economics and heterodox economics.Economic analysis can be applied throughout society, including business, finance, health care, engineering and government. It is also applied to such diverse subjects as crime, education, the family, feminism, law, philosophy, politics, religion, social institutions, war, science, and the environment.",Economics,1,0
177,437276,"In statistics, an effect size is a value measuring the strength of the relationship between two variables in a population, or a sample-based estimate of that quantity. It can refer to the value of a statistic calculated from a sample of data, the value of a parameter for a hypothetical population, or to the equation that operationalizes how statistics or parameters lead to the effect size value. Examples of effect sizes include the correlation between two variables, the regression coefficient in a regression, the mean difference, or the risk of a particular event (such as a heart attack) happening. Effect sizes complement statistical hypothesis testing, and play an important role in power analyses, sample size planning, and in meta-analyses. The cluster of data-analysis methods concerning effect sizes is referred to as estimation statistics.
Effect size is an essential component when evaluating the strength of a statistical claim, and it is the first item (magnitude) in the MAGIC criteria. The standard deviation of the effect size is of critical importance, since it indicates how much uncertainty is included in the measurement. A standard deviation that is too large will make the measurement nearly meaningless. In meta-analysis, where the purpose is to combine multiple effect sizes, the uncertainty in the effect size is used to weigh effect sizes, so that large studies are considered more important than small studies.  The uncertainty in the effect size is calculated differently for each type of effect size, but generally only requires knowing the study's sample size (N), or the number of observations (n) in each group.
Reporting effect sizes or estimates thereof (effect estimate [EE], estimate of effect) is considered good practice when presenting empirical research findings in many fields.  The reporting of effect sizes facilitates the interpretation of the importance of a research result, in contrast to its statistical significance. Effect sizes are particularly prominent in social science and in medical research (where size of treatment effect is important).
Effect sizes may be measured in relative or absolute terms. In relative effect sizes, two groups are directly compared with each other, as in odds ratios and relative risks. For absolute effect sizes, a larger absolute value always indicates a stronger effect. Many types of measurements can be expressed as either absolute or relative, and these can be used together because they convey different information. A prominent task force in the psychology research community made the following recommendation:

Always present effect sizes for primary outcomes...If the units of measurement are meaningful on a practical level (e.g., number of cigarettes smoked per day), then we usually prefer an unstandardized measure (regression coefficient or mean difference) to a standardized measure (r or d).",Effect size,1,0
178,27250866,"In the comparison of various statistical procedures, efficiency is a measure of quality of an estimator, of an experimental design, or of a hypothesis testing procedure.  Essentially, a more efficient estimator, experiment, or test needs fewer observations than a less efficient one to achieve a given error performance. 
An efficient estimator is characterized by a small variance or mean square error, indicating that there is a small deviance between the estimated value and the ""true"" value. The relative efficiency of two procedures is the ratio of their efficiencies, although often this concept is used where the comparison is made between a given procedure and a notional ""best possible"" procedure. The efficiencies and the relative efficiency of two procedures theoretically depend on the sample size available for the given procedure, but it is often possible to use the asymptotic relative efficiency (defined as the limit of the relative efficiencies as the sample size grows) as the principal comparison measure.

",Efficiency (statistics),1,1
181,1643759,"Egon Sharpe Pearson  (11 August 1895 – 12 June 1980) was one of three children of Karl Pearson and Maria, née Sharpe, and, like his father, a leading British statistician.",Egon Pearson,1,0
182,9710,"Elementary algebra encompasses some of the basic concepts of algebra, one of the main branches of mathematics. It is typically taught to secondary school students and builds on their understanding of arithmetic. Whereas arithmetic deals with specified numbers, algebra introduces quantities without fixed values, known as variables. This use of variables entails use of algebraic notation and an understanding of the general rules of the operations introduced in arithmetic.  Unlike abstract algebra, elementary algebra is not concerned with algebraic structures outside the realm of real and complex numbers.
The use of variables to denote quantities allows general relationships between quantities to be formally and concisely expressed, and thus enables solving a broader scope of problems. Many quantitative relationships in science and mathematics are expressed as algebraic equations.",Elementary algebra,1,0
183,27014075,"In probability and statistics, an elliptical distribution is any member of a broad family of probability distributions that generalize the multivariate normal distribution. Intuitively, in the simplified two and three dimensional case, the joint distribution forms an ellipse and an ellipsoid, respectively, in iso-density plots.
In statistics, the normal distribution is used in classical multivariate analysis, while elliptical distributions are used in generalized multivariate analysis, for the study of symmetric distributions with tails that are heavy, like the multivariate t-distribution, or light (in comparison with the normal distribution). Some statistical methods that were originally motivated by the study of the normal distribution have good performance for general elliptical distributions (with finite variance), particularly for spherical distributions (which are defined below). Elliptical distributions are also used in robust statistics to evaluate proposed multivariate-statistical procedures.",Elliptical distribution,1,0
184,2690455,"In statistics, an empirical distribution function (commonly also called an empirical Cumulative Distribution Function, eCDF) is the distribution function associated with the empirical measure of a sample. This cumulative distribution function is a step function that jumps up by 1/n at each of the n data points. Its value at any specified value of the measured variable is the fraction of observations of the measured variable that are less than or equal to the specified value.
The empirical distribution function is an estimate of the cumulative distribution function that generated the points in the sample. It converges with probability 1 to that underlying distribution, according to the Glivenko–Cantelli theorem.  A number of results exist to quantify the rate of convergence of the empirical distribution function to the underlying cumulative distribution function.",Empirical distribution function,1,0
185,10294,"In cryptography, encryption is the process of encoding information. This process converts the original representation of the information, known as plaintext, into an alternative form known as ciphertext. Ideally, only authorized parties can decipher a ciphertext back to plaintext and access the original information. Encryption does not itself prevent interference but denies the intelligible content to a would-be interceptor. 
For technical reasons, an encryption scheme usually uses a pseudo-random encryption key generated by an algorithm. It is possible to decrypt the message without possessing the key but, for a well-designed encryption scheme, considerable computational resources and skills are required. An authorized recipient can easily decrypt the message with the key provided by the originator to recipients but not to unauthorized users. 
Historically, various forms of encryption have been used to aid in cryptography. Early encryption techniques were often used in military messaging. Since then, new techniques have emerged and become commonplace in all areas of modern computing. Modern encryption schemes use the concepts of public-key and symmetric-key. Modern encryption techniques ensure security because modern computers are inefficient at cracking the encryption.

",Encryption,1,0
186,9251,"Engineering is the use of scientific principles to design and build machines, structures, and other items, including bridges, tunnels, roads, vehicles, and buildings. The discipline of engineering encompasses a broad range of more specialized fields of engineering, each with a more specific emphasis on particular areas of applied mathematics, applied science, and types of application. See glossary of engineering.
The term engineering is derived from the Latin ingenium, meaning ""cleverness"" and  ingeniare, meaning ""to contrive, devise"".",Engineering,1,0
187,5381408,"Engineering mathematics is a branch of applied mathematics concerning mathematical methods and techniques that are typically used in engineering and industry. Along with fields like engineering physics and engineering geology, both of which may belong in the wider category engineering science, engineering mathematics is an interdisciplinary subject motivated by engineers' needs both for practical, theoretical and other considerations outwith their specialization, and to deal with constraints to be effective in their work.

",Engineering mathematics,1,0
188,9546,"Engineering statistics combines engineering and statistics using scientific methods for analyzing data. Engineering statistics involves data concerning manufacturing processes such as: component dimensions, tolerances, type of material, and fabrication process control. There are many methods used in engineering analysis and they are often displayed as histograms to give a visual of the data as opposed to being just numerical. Examples of methods are:
Design of Experiments (DOE) is a methodology for formulating scientific and engineering problems using statistical models. The protocol specifies a randomization procedure for the experiment and specifies the primary data-analysis, particularly in hypothesis testing. In a secondary analysis, the statistical analyst further examines the data to suggest other questions and to help plan future experiments. In engineering applications, the goal is often to optimize a process or product, rather than to subject a scientific hypothesis to test of its predictive adequacy. The use of optimal (or near optimal) designs reduces the cost of experimentation.
Quality control and process control use statistics as a tool to manage conformance to specifications of manufacturing processes and their products.
Time and methods engineering use statistics to study repetitive operations in manufacturing in order to set standards and find optimum (in some sense) manufacturing procedures.
Reliability engineering which measures the ability of a system to perform for its intended function (and time) and has tools for improving performance.
Probabilistic design involving the use of probability in product and system design
System identification uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models.",Engineering statistics,1,0
189,8919339,"Environment statistics is the application of statistical methods to environmental science. It covers procedures for dealing with questions concerning the natural environment in its undisturbed state, the interaction of humanity with the environment, and urban environments. The field of environmental statistics has seen rapid growth in the past few decades as a response to increasing concern over the environment in the public, organizational, and governmental sectors.
The United Nations' Framework for the Development of Environment Statistics (FDES) defines the scope of environment statistics as follows:
The scope of environment statistics covers biophysical aspects of the environment and those aspects of the socio-economic system that directly influence and interact with the environment.
The scope of environment, social and economic statistics overlap. It is not easy – or necessary – to draw a clear line dividing these areas. Social and economic statistics that describe processes or activities with a direct impact on, or direct interaction with, the environment are used widely in environment statistics. They are within the scope of the FDES.",Environmental statistics,1,0
190,66997,"Epidemiology is the study and analysis of the distribution (who, when, and where), patterns and determinants of health and disease conditions in defined population.
It is a cornerstone of public health, and shapes policy decisions and evidence-based practice by identifying risk factors for disease and targets for preventive healthcare. Epidemiologists help with study design, collection, and statistical analysis of data, amend interpretation and dissemination of results (including peer review and occasional systematic review). Epidemiology has helped develop methodology used in clinical research, public health studies, and, to a lesser extent, basic research in the biological sciences.Major areas of epidemiological study include disease causation, transmission, outbreak investigation, disease surveillance, environmental epidemiology, forensic epidemiology, occupational epidemiology, screening, biomonitoring, and comparisons of treatment effects such as in clinical trials. Epidemiologists rely on other scientific disciplines like biology to better understand disease processes, statistics to make efficient use of the data and draw appropriate conclusions, social sciences to better understand proximate and distal causes, and engineering for exposure assessment.
Epidemiology, literally meaning ""the study of what is upon the people"", is derived from Greek  epi 'upon, among',  demos 'people, district', and  logos 'study, word, discourse', suggesting that it applies only to human populations. However, the term is widely used in studies of zoological populations (veterinary epidemiology), although the term ""epizoology"" is available, and it has also been applied to studies of plant populations (botanical or plant disease epidemiology).The distinction between ""epidemic"" and ""endemic"" was first drawn by Hippocrates, to distinguish between diseases that are ""visited upon"" a population (epidemic) from those that ""reside within"" a population (endemic). The term ""epidemiology"" appears to have first been used to describe the study of epidemics in 1802 by the Spanish physician Villalba in Epidemiología Española. Epidemiologists also study the interaction of diseases in a population, a condition known as a syndemic.
The term epidemiology is now widely applied to cover the description and causation of not only epidemic, infectious disease, but of disease in general, including related conditions. Some examples of topics examined through epidemiology include as high blood pressure, mental illness and obesity. Therefore, this epidemiology is based upon how the pattern of the disease causes change in the function of human beings.",Epidemiology,1,0
191,461509,"In statistics and optimization, errors and residuals are two closely related and easily confused measures of the deviation of an observed value of an element of a statistical sample from its ""true value"" (not necessarily observable). The error of an observation is the deviation of the observed value from the true value of a quantity of interest (for example, a population mean). The residual is the difference between the observed value and the estimated value of the quantity of interest (for example, a sample mean). The distinction is most important in regression analysis, where the concepts are sometimes called the regression errors and regression residuals and where they lead to the concept of studentized residuals.
In econometrics, ""errors"" are also called disturbances.

",Errors and residuals,1,1
192,16693348,"In statistics, the method of estimating equations is a way of specifying how the parameters of a statistical model should be estimated. This can be thought of as a generalisation of many classical methods—the method of moments, least squares, and maximum likelihood—as well as some recent methods like M-estimators.
The basis of the method is to have, or to find, a set of simultaneous equations involving both the sample data and the unknown model parameters which are to be solved in order to define the estimates of the parameters. Various components of the equations are defined in terms of the set of observed data on which the estimates are to be based.
Important examples of estimating equations are the likelihood equations.",Estimating equations,1,0
193,1565926,"Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component.  The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. An estimator attempts to approximate the unknown parameters using the measurements.
In estimation theory, two approaches are generally considered: 
The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest
The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.",Estimation theory,1,0
194,36687154,"Estimation (or estimating) is the process of finding an estimate or approximation, which is a value that is usable for some purpose even if input data may be incomplete, uncertain, or unstable. The value is nonetheless usable because it is derived from the best information available. Typically, estimation involves ""using the value of a statistic derived from a sample to estimate the value of a corresponding population parameter"". The sample provides information that can be projected, through various formal or informal processes, to determine a range most likely to describe the missing information. An estimate that turns out to be incorrect will be an overestimate if the estimate exceeds the actual result and an underestimate if the estimate falls short of the actual result.

",Estimation,1,0
195,9417,"Euclidean geometry is a mathematical system attributed to  ancient Greek mathematician Euclid, which he described in his textbook on geometry: the Elements. Euclid's approach consists in assuming a small set of intuitively appealing axioms, and deducing many other propositions (theorems) from these. Although many of Euclid's results had been stated earlier, Euclid was the first to organize these propositions into a logical system in which each result is proved from axioms and previously proved theorems.The Elements begins with plane geometry, still taught in secondary school (high school) as the first axiomatic system and the first examples of mathematical proofs. It goes on to the solid geometry of three dimensions. Much of the Elements states results of what are now called algebra and number theory, explained in geometrical language.For more than two thousand years, the adjective ""Euclidean"" was unnecessary because no other sort of geometry had been conceived. Euclid's axioms seemed so intuitively obvious (with the possible exception of the parallel postulate) that any theorem proved from them was deemed true in an absolute, often metaphysical, sense. Today, however, many other self-consistent non-Euclidean geometries are known, the first ones having been discovered in the early 19th century. An implication of Albert Einstein's theory of general relativity is that physical space itself is not Euclidean, and Euclidean space is a good approximation for it only over short distances (relative to the strength of the  gravitational field).Euclidean geometry is an example of synthetic geometry, in that it proceeds logically from axioms describing basic properties of geometric objects such as points and lines, to propositions about those objects. This is in contrast to analytic geometry, introduced almost 2,000 years later by René Descartes, which uses coordinates to express geometric properties as algebraic formulas.",Euclidean geometry,1,0
196,25964,"In political science, a revolution (Latin: revolutio, ""a turn around"") is a fundamental and relatively sudden change in political power and political organization which occurs when the population revolts against the government, typically due to perceived oppression (political, social, economic) or political incompetence.Revolutions have occurred throughout human history and vary widely in terms of methods, duration, and motivating ideology. Their results include major changes in culture, economy, and socio-political institutions, usually in response to perceived overwhelming autocracy or plutocracy.
Scholarly debates about what does and does not constitute a revolution center on several issues. Early studies of revolutions primarily analyzed events in European history from a psychological perspective, but more modern examinations include global events and incorporate perspectives from several social sciences, including sociology and political science. Several generations of scholarly thought on revolutions have generated many competing theories and contributed much to the current understanding of this complex phenomenon.
Notable revolutions in recent centuries include the creation of the United States through the American Revolutionary War (1775–1783), the French Revolution (1789–1799), the Haitian Revolution (1791–1804), the Spanish American wars of independence (1808–1826), the European Revolutions of 1848, the Russian Revolution in 1917, the Chinese Revolution of the 1940s, the Cuban Revolution in 1959, the Iranian Revolution in 1979, and the European Revolutions of 1989.",Revolution,1,0
197,418101,"Evolutionary biology  is the subfield of biology that studies the evolutionary processes (natural selection, common descent, speciation) that produced the diversity of life on Earth. Simply, it is also defined as the study of the history of life forms on Earth. Evolution is based on the theory that all species are related and they gradually change over time. In a population, the genetic variations affect the physical characteristics i.e. phenotypes of an organism. These changes in the phenotypes will be an advantage to some organisms, which will then be passed onto their offspring. Peppered Moth and Flightless birds are some examples of evolution in species over many generations. In the 1930s, the discipline of evolutionary biology emerged through what Julian Huxley called the modern synthesis of understanding, from previously unrelated fields of biological research, such as genetics and ecology, systematics, and paleontology. A person who studies Evolutionary biology is called an Evolutionary biologist. The importance of studying Evolutionary biology is mainly to understand the principles behind the origin and extinction of species.The investigational range of current research widened to encompass the genetic architecture of adaptation, molecular evolution, and the different forces that contribute to evolution, such as sexual selection, genetic drift, and biogeography. Moreover, the newer field of evolutionary developmental biology (""evo-devo"") investigates how embryogenesis, the development of the embryo, is controlled, thus yielding a wider synthesis that integrates developmental biology with the fields of study covered by the earlier evolutionary synthesis.",Evolutionary biology,1,0
198,9653,"In probability theory, the expected value (also called expectation, expectancy, mathematical expectation, mean, average, or first moment) is a generalization of the weighted average. Informally, the expected value is the arithmetic mean of a large number of independently selected outcomes of a random variable. 
The expected value of a random variable with a finite number of outcomes is a weighted average of all possible outcomes.  In the case of a continuum of possible outcomes, the expectation is defined by integration. In the axiomatic foundation for probability provided by measure theory, the expectation is given by Lebesgue integration.
The expected value of a random variable X is often denoted by E(X), E[X], or EX, with E also often stylized as E or 
  
    
      
        
          E
        
        .
      
    
    {\displaystyle \mathbb {E} .}",Expected value,1,0
199,59861,"An experiment is a procedure carried out to support or refute a hypothesis, or determine the efficacy or likelihood of something previously untried.  Experiments provide insight into cause-and-effect by demonstrating what outcome occurs when a particular factor is manipulated.  Experiments vary greatly in goal and scale but always rely on repeatable procedure and logical analysis of the results. There also exist  natural experimental studies.
A child may carry out basic experiments to understand how things fall to the ground, while teams of scientists may take years of systematic investigation to advance their understanding of a phenomenon. Experiments and other types of hands-on activities are very important to student learning in the science classroom. Experiments can raise test scores and help a student become more engaged and interested in the material they are learning, especially when used over time. Experiments can vary from personal and informal natural comparisons (e.g. tasting a range of chocolates to find a favorite), to highly controlled (e.g. tests requiring complex apparatus overseen by many scientists that hope to discover information about subatomic particles). Uses of experiments vary considerably between the natural and human sciences.
Experiments typically include controls, which are designed to minimize the effects of variables other than the single independent variable. This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method. Ideally, all variables in an experiment are controlled (accounted for by the control measurements) and none are uncontrolled. In such an experiment, if all controls work as expected, it is possible to conclude that the experiment works as intended, and that results are due to the effect of the tested variables.",Experiment,1,1
201,22346,"Observational error (or measurement error) is the difference between a measured value of a quantity and its true value. In statistics, an error is not necessarily a ""mistake"". Variability is an inherent part of the results of measurements and of the measurement process.
Measurement errors can be divided into two components: random and systematic.Random errors are errors in measurement that lead to measurable values being inconsistent when repeated measurements of a constant attribute or quantity are taken. Systematic errors are errors that are not determined by chance but are introduced by repeatable processes inherent to the system. Systematic error may also refer to an error with a non-zero mean, the effect of which is not reduced when observations are averaged.Measurement errors can be summarized in terms of accuracy and precision.
Measurement error should not be confused with measurement uncertainty.",Observational error,1,1
203,416589,"In statistics, exploratory data analysis is an approach of analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling and thereby contrasts traditional hypothesis testing. Exploratory data analysis has been promoted by John Tukey since 1970 to encourage statisticians to explore the data, and possibly formulate hypotheses that could lead to new data collection and experiments. EDA is different from initial data analysis (IDA), which focuses more narrowly on checking assumptions required for model fitting and hypothesis testing, and handling missing values and making transformations of variables as needed. EDA encompasses IDA.

",Exploratory data analysis,1,1
204,339174,"In probability and statistics, an exponential family is a parametric set of probability distributions of a certain form, specified below. This special form is chosen for mathematical convenience, based on some useful algebraic properties, as well as for generality, as exponential families are in a sense very natural sets of distributions to consider. The term exponential class is sometimes used in place of ""exponential family"", or the older term Koopman–Darmois family. The terms ""distribution"" and ""family"" are often used loosely: specifically, an exponential family is a set of distributions, where the specific distribution varies with the parameter; however, a parametric family of distributions is often referred to as ""a distribution"" (like ""the normal distribution"", meaning ""the family of normal distributions""), and the set of all exponential families is sometimes loosely referred to as ""the"" exponential family. They are distinct because they possess a variety of desirable properties, most importantly the existence of a sufficient statistic.
The concept of exponential families is credited to E. J. G. Pitman, G. Darmois, and B. O. Koopman in 1935–1936. Exponential families of distributions provides a general framework for selecting a possible alternative parameterisation of a parametric family of distributions, in terms of natural parameters, and for defining useful sample statistics, called the natural sufficient statistics of the family.",Exponential family,1,0
205,1890727,"Exponential smoothing is a rule of thumb technique for smoothing time series data using the exponential window function. Whereas in the simple moving average the past observations are weighted equally, exponential functions are used to assign exponentially decreasing weights over time. It is an easily learned and easily applied procedure for making some determination based on prior assumptions by the user, such as seasonality. Exponential smoothing is often used for analysis of time-series data.
Exponential smoothing is one of many window functions commonly applied to smooth data in signal processing, acting as low-pass filters to remove high-frequency noise. This method is preceded by Poisson's use of recursive exponential window functions in convolutions from the 19th century, as well as Kolmogorov and Zurbenko's use of recursive moving averages from their studies of turbulence in the 1940s.
The raw data sequence is often represented by 
  
    
      
        {
        
          x
          
            t
          
        
        }
      
    
    {\displaystyle \{x_{t}\}}
   beginning at time 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  , and the output of the exponential smoothing algorithm is commonly written as 
  
    
      
        {
        
          s
          
            t
          
        
        }
      
    
    {\displaystyle \{s_{t}\}}
  , which may be regarded as a best estimate of what the next value of 
  
    
      
        x
      
    
    {\displaystyle x}
   will be.  When the sequence of observations begins at time 
  
    
      
        t
        =
        0
      
    
    {\displaystyle t=0}
  , the simplest form of exponential smoothing is given by the formulas:

  
    
      
        
          
            
              
                
                  s
                  
                    0
                  
                
              
              
                
                =
                
                  x
                  
                    0
                  
                
              
            
            
              
                
                  s
                  
                    t
                  
                
              
              
                
                =
                α
                
                  x
                  
                    t
                  
                
                +
                (
                1
                −
                α
                )
                
                  s
                  
                    t
                    −
                    1
                  
                
                ,
                
                t
                >
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}s_{0}&=x_{0}\\s_{t}&=\alpha x_{t}+(1-\alpha )s_{t-1},\quad t>0\end{aligned}}}
  where 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is the smoothing factor, and 
  
    
      
        0
        <
        α
        <
        1
      
    
    {\displaystyle 0<\alpha <1}
  .",Exponential smoothing,1,0
206,546415,"In mathematics, extrapolation is a type of estimation, beyond the original observation range, of the value of a variable on the basis of its relationship with another variable. It is similar to interpolation, which produces estimates between known observations, but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results.  Extrapolation may also mean extension of a method, assuming similar methods will be applicable. Extrapolation may also apply to human experience to project, extend, or expand known experience into an area not known or previously experienced so as to arrive at a (usually conjectural) knowledge of the unknown (e.g. a driver extrapolates road conditions beyond his sight while driving). The extrapolation method can be applied in the interior reconstruction problem.

",Extrapolation,1,0
207,536080,"The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis.
A t-test is the most commonly applied when the test statistic would follow a normal distribution if the value of a scaling term in the test statistic were known.  When the scaling term is unknown and is replaced by an estimate based on the data, the test statistics (under certain conditions) follow a Student's t distribution. The t-test can be used, for example, to determine if the means of two sets of data are significantly different from each other.",Student's t-test,1,1
208,253492,"Factor analysis is a statistical method used to describe variability among observed, correlated variables in terms of a potentially lower number of unobserved variables called factors. For example, it is possible that variations in six observed variables mainly reflect the variations in two unobserved (underlying) variables. Factor analysis searches for such joint variations in response to unobserved latent variables. The observed variables are modelled as linear combinations of the potential factors plus ""error"" terms, hence factor analysis can be thought of as a special case of errors-in-variables models.Simply put, the factor loading of a variable quantifies the extent to which the variable is related to a given factor.A common rationale behind factor analytic methods is that the information gained about the interdependencies between observed variables can be used later to reduce the set of variables in a dataset. Factor analysis is commonly used in psychometrics, personality psychology, biology, marketing, product management, operations research, finance, and machine learning. It may help to deal with data sets where there are large numbers of observed variables that are thought to reflect a smaller number of underlying/latent variables. It is one of the most commonly used inter-dependency techniques and is used when the relevant set of variables shows a systematic inter-dependence and the objective is to find out the latent factors that create a commonality.",Factor analysis,1,1
209,1827237,"In statistics, a full factorial experiment is an experiment whose design consists of two or more factors, each with discrete possible values or ""levels"", and whose experimental units take on all possible combinations of these levels across all such factors. A full factorial design may also be called a fully crossed design. Such an experiment allows the investigator to study the effect of each factor on the response variable, as well as the effects of interactions between factors on the response variable.
For the vast majority of factorial experiments, each factor has only two levels. For example, with two factors each taking two levels, a factorial experiment would have four treatment combinations in total, and is usually called a 2×2 factorial design. In such a design, the interaction between the variables is often the most important. This applies even to scenarios where a main effect and an interaction is present.
If the number of combinations in a full factorial design is too high to be logistically feasible, a fractional factorial design may be done, in which some of the possible combinations (usually at least half) are omitted.

",Factorial experiment,1,0
210,11524,"The Fahrenheit scale () is a temperature scale based on one proposed in 1724 by the physicist Daniel Gabriel Fahrenheit (1686–1736). It uses the degree Fahrenheit (symbol: °F) as the unit. Several accounts of how he originally defined his scale exist, but the original paper suggests the lower defining point, 0 °F, was established as the freezing temperature of a solution of brine made from a mixture of water, ice, and ammonium chloride (a salt). The other limit established was his best estimate of the average human body temperature, originally set at 90 °F, then 96 °F (about 2.6 °F less than the modern value due to a later redefinition of the scale). However, he noted a middle point of 32 °F, to be set to the temperature of ice water.
For much of the 20th century, the Fahrenheit scale was defined by two fixed points with a 180 °F separation: the temperature at which pure water freezes was defined as 32 °F and the boiling point of water was defined to be 212 °F, both at sea level and under standard atmospheric pressure. It is now formally defined using the Kelvin scale and hence ultimately by the Boltzmann constant, the Planck constant, and the unperturbed ground-state hyperfine transition frequency of the caesium-133 atom.It is the official temperature scale in the United States (including its unincorporated territories), its freely associated states in the Western Pacific (Palau, the Federated States of Micronesia and the Marshall Islands), the Cayman Islands, and the former American colony of Liberia. Fahrenheit is used alongside the Celsius scale in Antigua and Barbuda and other countries which use the same meteorological service, such as Saint Kitts and Nevis, the Bahamas, and Belize. A handful of British Overseas Territories still use both scales, including the British Virgin Islands, Montserrat, Anguilla, and Bermuda. In the United Kingdom, degrees Fahrenheit figures are sometimes used in newspaper headlines to sensationalize heatwaves. All other countries now use Celsius (also known as centigrade), a scale formalized about 20 years after the Fahrenheit scale. However, the name Celsius was given to the centigrade scale much later, in 1948, in honor of the Swedish astronomer Anders Celsius.

",Fahrenheit,1,0
211,1336960,"Failure rate is the frequency with which an engineered system or component fails, expressed in failures per unit of time.  It is usually denoted by the Greek letter λ (lambda) and is often used in reliability engineering.
The failure rate of a system usually depends on time, with the rate varying over the life cycle of the system.  For example, an automobile's failure rate in its fifth year of service may be many times greater than its failure rate during its first year of service.  One does not expect to replace an exhaust pipe, overhaul the brakes, or have major transmission problems in a new vehicle.
In practice, the mean time between failures (MTBF, 1/λ) is often reported instead of the failure rate. This is valid and useful if the failure rate may be assumed constant – often used for complex units / systems, electronics – and is a general agreement in some reliability standards (Military and Aerospace). It does in this case only relate to the flat region of the bathtub curve, which is also called the ""useful life period"". Because of this, it is incorrect to extrapolate MTBF to give an estimate of the service lifetime of a component, which will typically be much less than suggested by the MTBF due to the much higher failure rates in the ""end-of-life wearout"" part of the ""bathtub curve"".
The reason for the preferred use for MTBF numbers is that the use of large positive numbers (such as 2000 hours) is more intuitive and easier to remember than very small numbers (such as 0.0005 per hour).
The MTBF is an important system parameter in systems where failure rate needs to be managed, in particular for safety systems. The MTBF appears frequently in the engineering design requirements, and governs frequency of required system maintenance and inspections.  In special processes called renewal processes, where the time to recover from failure can be neglected and the likelihood of failure remains constant with respect to time, the failure rate is simply the multiplicative inverse of the MTBF (1/λ).
A similar ratio used in the transport industries, especially in railways and trucking is ""mean distance between failures"", a variation which attempts to correlate actual loaded distances to similar reliability needs and practices.
Failure rates are important factors in the insurance, finance, commerce and regulatory industries and fundamental to the design of safe systems in a wide variety of applications.

",Failure rate,1,0
212,41668689,"A fan chart is made of a group of dispersion fan diagrams,
which may be positioned according to two categorising dimensions.
A dispersion fan diagram is a circular diagram which
reports the same information about a dispersion as a box plot:
namely median, quartiles, and two extreme values.",Fan chart (statistics),1,0
213,4357461,"A fat-tailed distribution is a probability distribution that exhibits a large skewness or kurtosis, relative to that of either a normal distribution or an exponential distribution. In common usage, the terms fat-tailed and heavy-tailed are sometimes synonymous; fat-tailed is sometimes also defined as a subset of heavy-tailed. Different research communities favor one or the other largely for historical reasons, and may have differences in the precise definition of either.
Fat-tailed distributions have been empirically encountered in a variety of areas: physics, earth sciences, economics and political science. The class of fat-tailed distributions includes those whose tails decay like a power law, which is a common point of reference in their use in the scientific literature. However, fat-tailed distributions also include other slowly-decaying distributions, such as the log-normal.",Fat-tailed distribution,1,0
214,14549801,"Financial management is the business function concerned with profitability, expenses, cash and credit, so that the ""organization may have the means to carry out its objective as satisfactorily as possible;""  
the latter often defined as maximizing the value of the firm for stockholders.
Financial managers (FM) are specialized professionals directly reporting to senior management, often the financial director (FD); the function is seen as 'Staff', and not 'Line'.",Financial management,1,0
215,267061,"A finite geometry is any geometric system that has only a finite number of points.
The familiar Euclidean geometry is not finite, because a Euclidean line contains infinitely many points. A geometry based on the graphics displayed on a computer screen, where the pixels are considered to be the points, would be a finite geometry. While there are many systems that could be called finite geometries, attention is mostly paid to the finite projective and affine spaces because of their regularity and simplicity.  Other significant types of finite geometry are finite Möbius or inversive planes and Laguerre planes, which are examples of a general type called Benz planes, and their higher-dimensional analogs such as higher finite inversive geometries.
Finite geometries may be constructed via linear algebra, starting from vector spaces over a finite field; the affine and projective planes so constructed are called Galois geometries.  Finite geometries can also be defined purely axiomatically. Most common finite geometries are Galois geometries, since any finite projective space of dimension three or greater is isomorphic to a projective space over a finite field (that is, the projectivization of a vector space over a finite field). However, dimension two has affine and projective planes that are not isomorphic to Galois geometries, namely the non-Desarguesian planes.  Similar results hold for other kinds of finite geometries.",Finite geometry,1,0
216,18484937,"Events are often triggered when a stochastic or random process first encounters a threshold. The threshold can be a barrier, boundary or specified state of a system. The amount of time required for a stochastic process, starting from some initial state, to encounter a threshold for the first time is referred to variously as a first hitting time. 
In statistics, first-hitting-time models are a sub-class of survival models.  The first hitting time, also called first passage time, of the barrier set 
  
    
      
        B
      
    
    {\displaystyle B}
   with respect to an instance of a stochastic process is the time until the stochastic process first enters 
  
    
      
        B
      
    
    {\displaystyle B}
  .
More colloquially, a first passage time in a stochastic system, is the time taken for a state variable to reach a certain value. Understanding this metric allows one to further understand the physical system under observation, and as such has been the topic of research in very diverse fields, from economics to ecology.The idea that a first hitting time of a stochastic process might describe the time to occurrence of an event has a long history, starting with an interest in the first passage time of Wiener diffusion processes in economics and then in physics in the early 1900s. Modeling the probability of financial ruin as a first passage time was an early application in the field of insurance. An interest in the mathematical properties of first-hitting-times and statistical models and methods for analysis of survival data appeared steadily between the middle and end of the 20th century.",First-hitting-time model,1,0
217,12636406,"Fisher's principle is an evolutionary model that explains why the sex ratio of most species that produce offspring through sexual reproduction is approximately 1:1 between males and females. A. W. F. Edwards has remarked that it is ""probably the most celebrated argument in evolutionary biology"".Fisher's principle was outlined by Ronald Fisher in his 1930 book The Genetical Theory of Natural Selection (but has been incorrectly attributed as original to Fisher). Fisher couched his argument in terms of parental expenditure, and predicted that parental expenditure on both sexes should be equal. Sex ratios that are 1:1 are hence known as ""Fisherian"", and those that are not 1:1 are ""non-Fisherian"" or ""extraordinary"" and occur because they break the assumptions made in Fisher's model.",Fisher's principle,1,0
218,598971,"In mathematical statistics, the Fisher information (sometimes simply called information) is a way of measuring the amount of information that an observable random variable X carries about an unknown parameter θ of a distribution that models X. Formally, it is the variance of the score, or the expected value of the observed information. In Bayesian statistics, the asymptotic distribution of the posterior mode depends on the Fisher information and not on the prior (according to the Bernstein–von Mises theorem, which was anticipated by Laplace for exponential families). The role of the Fisher information in the asymptotic theory of maximum-likelihood estimation was emphasized by the statistician Ronald Fisher (following some initial results by Francis Ysidro Edgeworth). The Fisher information is also used in the calculation of the Jeffreys prior, which is used in Bayesian statistics.
The Fisher information matrix is used to calculate the covariance matrices associated with maximum-likelihood estimates. It can also be used in the formulation of test statistics, such as the Wald test.
Statistical systems of a scientific nature (physical, biological, etc.) whose likelihood functions obey shift invariance have been shown to obey maximum Fisher information. The level of the maximum depends upon the nature of the system constraints.",Fisher information,1,0
219,515152,"Fisherian runaway or runaway selection is a sexual selection mechanism proposed by the mathematical biologist Ronald Fisher in the early 20th century, to account for the evolution of ostentatious male ornamentation by persistent, directional female choice. An example is the colourful and elaborate peacock plumage compared to the relatively subdued peahen plumage; the costly ornaments, notably the bird's extremely long tail, appear to be incompatible with natural selection. Fisherian runaway can be postulated to include sexually dimorphic phenotypic traits such as behavior expressed by a particular sex.
Extreme and apparently maladaptive sexual dimorphism represented a paradox for evolutionary biologists from Charles Darwin's time  up to the modern synthesis. Darwin attempted to resolve the paradox by assuming heredity for both the preference and the ornament, and supposed an ""aesthetic sense"" in higher animals, leading to powerful selection of both characteristics in subsequent generations. Fisher developed the theory further by assuming genetic correlation between the preference and the ornament, that initially the ornament signalled greater potential fitness (the likelihood of leaving more descendants), so preference for the ornament had a selective advantage. Subsequently, if strong enough, female preference for exaggerated ornamentation in mate selection could be enough to undermine natural selection even when the ornament has become non-adaptive. Over subsequent generations this could lead to runaway selection by positive feedback, and the speed with which the trait and the preference increase could (until counter-selection interferes) increase exponentially.Fisherian runaway has been difficult to demonstrate empirically, because it has been difficult to detect both an underlying genetic mechanism and a process by which it is initiated.

",Fisherian runaway,1,0
220,11376,"In computing, floating-point arithmetic (FP) is arithmetic using formulaic representation of real numbers as an approximation to support a trade-off between range and precision. For this reason, floating-point computation is often used in systems with very small and very large real numbers that require fast processing times. In general, a floating-point number is represented approximately with a fixed number of significant digits (the significand) and scaled using an exponent in some fixed base; the base for the scaling is normally two, ten, or sixteen. A number that can be represented exactly is of the following form:

where significand is an integer, base is an integer greater than or equal to two, and exponent is also an integer.
For example:

The term floating point refers to the fact that a number's radix point (decimal point, or, more commonly in computers, binary point) can ""float""; that is, it can be placed anywhere relative to the significant digits of the number. This position is indicated by the exponent, and thus the floating-point representation can be thought of as a form of scientific notation.
A floating-point system can be used to represent, with a fixed number of digits, numbers of different orders of magnitude: e.g. the distance between galaxies or the diameter of an atomic nucleus can be expressed with the same unit of length. The result of this dynamic range is that the numbers that can be represented are not uniformly spaced; the difference between two consecutive representable numbers varies with the chosen scale.

Over the years, a variety of floating-point representations have been used in computers.  In 1985, the IEEE 754 Standard for Floating-Point Arithmetic was established, and since the 1990s, the most commonly encountered representations are those defined by the IEEE.
The speed of floating-point operations, commonly measured in terms of FLOPS, is an important characteristic of a computer system, especially for applications that involve intensive mathematical calculations.
A floating-point unit (FPU, colloquially a math coprocessor) is a part of a computer system specially designed to carry out operations on floating-point numbers.",Floating-point arithmetic,1,0
221,246074,"Forecasting is the process of making predictions based on past and present data. Later these can be compared (resolved) against what happens. For example, a company might estimate their revenue in the next year, then compare it against the actual results. Prediction is a similar, but more general term. Forecasting might refer to specific formal statistical methods employing time series, cross-sectional or longitudinal data, or alternatively to less formal judgmental methods or the process of prediction and resolution itself. Usage can differ between areas of application: for example, in hydrology the terms ""forecast"" and ""forecasting"" are sometimes reserved for estimates of values at certain specific future times, while the term ""prediction"" is used for more general estimates, such as the number of times floods will occur over a long period.
Risk and uncertainty are central to forecasting and prediction; it is generally considered good practice to indicate the degree of uncertainty attaching to forecasts. In any case, the data must be up to date in order for the forecast to be as accurate as possible. In some cases the data used to predict the variable of interest is itself forecast.",Forecasting,1,0
222,11394359,"A forest plot, also known as a blobbogram, is a graphical display of estimated results from a number of scientific studies addressing the same question, along with the overall results. It was developed for use in medical research as a means of graphically representing a meta-analysis of the results of randomized controlled trials. In the last twenty years, similar meta-analytical techniques have been applied in observational studies (e.g. environmental epidemiology) and forest plots are often used in presenting the results of such studies also.
Although forest plots can take several forms, they are commonly presented with two columns. The left-hand column lists the names of the studies (frequently randomized controlled trials or epidemiological studies), commonly in chronological order from the top downwards. The right-hand column is a plot of the measure of effect (e.g. an odds ratio) for each of these studies (often represented by a square) incorporating confidence intervals represented by horizontal lines. The graph may be plotted on a natural logarithmic scale when using odds ratios or other ratio-based effect measures, so that the confidence intervals are symmetrical about the means from each study and to ensure undue emphasis is not given to odds ratios greater than 1 when compared to those less than 1. The area of each square is proportional to the study's weight in the meta-analysis. The overall meta-analysed measure of effect is often represented on the plot as a dashed vertical line. This meta-analysed measure of effect is commonly plotted as a diamond, the lateral points of which indicate confidence intervals for this estimate.
A vertical line representing no effect is also plotted. If the confidence intervals for individual studies overlap with this line, it demonstrates that at the given level of confidence their effect sizes do not differ from no effect for the individual study. The same applies for the meta-analysed measure of effect: if the points of the diamond overlap the line of no effect the overall meta-analysed result cannot be said to differ from no effect at the given level of confidence.
Forest plots date back to at least the 1970s. One plot is shown in a 1985 book about meta-analysis.: 252 
The first use in print of the expression ""forest plot"" may be in an abstract for a poster at the Pittsburgh (US) meeting of the Society for Clinical Trials in May 1996. An informative investigation on the origin of the notion ""forest plot"" was published in 2001. 
The name refers to the forest of lines produced. In September 1990, Richard Peto joked that the plot was named after a breast cancer researcher called Pat Forrest and as a result the name has sometimes been spelled ""forrest plot"".",Forest plot,1,0
223,169358,"Foundations of mathematics is the study of the philosophical and logical and/or algorithmic basis of mathematics, or, in a broader sense, the mathematical investigation of what underlies the philosophical theories concerning the nature of mathematics. In this latter sense, the distinction between foundations of mathematics and philosophy of mathematics turns out to be quite vague.
Foundations of mathematics can be conceived as the study of the basic mathematical concepts (set, function, geometrical figure, number, etc.) and how they form hierarchies of more complex structures and concepts, especially the fundamentally important structures that form the language of mathematics (formulas, theories and their models giving a meaning to formulas, definitions, proofs, algorithms, etc.) also called metamathematical concepts, with an eye to the philosophical aspects and the unity of mathematics. The search for foundations of mathematics is a central question of the philosophy of mathematics; the abstract nature of mathematical objects presents special philosophical challenges.
The foundations of mathematics as a whole does not aim to contain the foundations of every mathematical topic.
Generally, the foundations of a field of study refers to a more-or-less systematic analysis of its most basic or fundamental concepts, its conceptual unity and its natural ordering or hierarchy of concepts, which may help to connect it with the rest of human knowledge. The development, emergence, and clarification of the foundations can come late in the history of a field, and might not be viewed by everyone as its most interesting part.
Mathematics always played a special role in scientific thought, serving since ancient times as a model of truth and rigor for rational inquiry, and giving tools or even a foundation for other sciences (especially physics). Mathematics' many developments towards higher abstractions in the 19th century brought new challenges and paradoxes, urging for a deeper and more systematic examination of the nature and criteria of mathematical truth, as well as a unification of the diverse branches of mathematics into a coherent whole.
The systematic search for the foundations of mathematics started at the end of the 19th century and formed a new mathematical discipline called mathematical logic, which later had strong links to theoretical computer science.
It went through a series of crises with paradoxical results, until the discoveries stabilized during the 20th century as a large and coherent body of mathematical knowledge with several aspects or components (set theory, model theory, proof theory, etc.), whose detailed properties and possible variants are still an active research field.
Its high level of technical sophistication inspired many philosophers to conjecture that it can serve as a model or pattern for the foundations of other sciences.",Foundations of mathematics,1,0
224,15515301,"The foundations of statistics concern the epistemological debate in statistics over how one should conduct inductive inference from data. Among the issues considered in statistical inference are the question of Bayesian inference versus frequentist inference, the distinction between Fisher's ""significance testing"" and Neyman–Pearson ""hypothesis testing"", and whether the likelihood principle should be followed. Some of these issues have been debated for up to 200 years without resolution.Bandyopadhyay & Forster describe four statistical paradigms: ""(i) classical statistics or error statistics, (ii) Bayesian statistics, (iii) likelihood-based statistics, and (iv) the Akaikean-Information Criterion-based statistics"".
Savage's text Foundations of Statistics has been cited over 15000 times on Google Scholar. It states the following.

It is unanimously agreed that statistics depends somehow on probability.  But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.",Foundations of statistics,1,0
225,19095208,"Statistics is the theory and application of mathematics to the scientific method including hypothesis generation, experimental design, sampling, data collection, data summarization, estimation, prediction and inference from those results to the population from which the experimental sample was drawn. This article lists statisticians who have been instrumental in the development of theoretical and applied statistics.",Founders of statistics,1,0
226,11659,"In mathematics, Fourier analysis () is the study of the way general functions may be represented or approximated by sums of simpler trigonometric functions. Fourier analysis grew from the study of Fourier series, and is named after Joseph Fourier, who showed that representing a function as a sum of trigonometric functions greatly simplifies the study of heat transfer.
The subject of Fourier analysis encompasses a vast spectrum of mathematics. In the sciences and engineering, the process of decomposing a function into oscillatory components is often called Fourier analysis, while the operation of rebuilding the function from these pieces is known as Fourier synthesis. For example, determining what component frequencies are present in a musical note would involve computing the Fourier transform of a sampled musical note. One could then re-synthesize the same sound by including the frequency components as revealed in the Fourier analysis. In mathematics, the term Fourier analysis often refers to the study of both operations.
The decomposition process itself is called a Fourier transformation. Its output, the Fourier transform, is often given a more specific name, which depends on the domain and other properties of the function being transformed. Moreover, the original concept of Fourier analysis has been extended over time to apply to more and more abstract and general situations, and the general field is often known as harmonic analysis. Each transform used for analysis (see list of Fourier-related transforms) has a corresponding inverse transform that can be used for synthesis.
To use Fourier analysis, data must be equally spaced. Different approaches have been developed for analyzing unequally spaced data, notably the least-squares spectral analysis (LSSA) methods that use a least squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long gapped records; LSSA mitigates such problems.

",Fourier analysis,1,0
227,10913,"In mathematics, fractal is a term used to describe geometric shapes containing detailed structure at arbitrarily small scales, usually having a fractal dimension strictly exceeding the topological dimension. Many fractals appear similar at various scales, as illustrated in successive magnifications of the Mandelbrot set. This exhibition of similar patterns at increasingly smaller scales is called self-similarity, also known as expanding symmetry or unfolding symmetry; if this replication is exactly the same at every scale, as in the Menger sponge, the shape is called affine self-similar.  Fractal geometry lies within the mathematical branch of measure theory.
One way that fractals are different from finite geometric figures is how they scale. Doubling the edge lengths of a filled polygon multiplies its area by four, which is two (the ratio of the new to the old side length) raised to the power of two (the conventional dimension of the filled polygon). Likewise, if the radius of a filled sphere is doubled, its volume scales by eight, which is two (the ratio of the new to the old radius) to the power of three (the conventional dimension of the filled sphere). However, if a fractal's one-dimensional lengths are all doubled, the spatial content of the fractal scales by a power that is not necessarily an integer and is in general greater than its conventional dimension. This power is called the fractal dimension of the geometric object, to distinguish it from the conventional dimension (which is formally called the topological dimension).Analytically, many fractals are nowhere differentiable. An infinite fractal curve can be conceived of as winding through space differently from an ordinary line – although it is still  topologically 1-dimensional, its fractal dimension indicates that it locally fills space more efficiently than an ordinary line.

Starting in the 17th century with notions of recursion, fractals have moved through increasingly rigorous mathematical treatment to the study of continuous but not differentiable functions in the 19th century by the seminal work of Bernard Bolzano, Bernhard Riemann, and Karl Weierstrass, and on to the coining of the word fractal in the 20th century with a subsequent burgeoning of interest in fractals and computer-based modelling in the 20th century.There is some disagreement among mathematicians about how the concept of a fractal should be formally defined. Mandelbrot himself summarized it as ""beautiful, damn hard, increasingly useful. That's fractals."" More formally, in 1982 Mandelbrot defined fractal as follows: ""A fractal is by definition a set for which the Hausdorff–Besicovitch dimension strictly exceeds the topological dimension."" Later, seeing this as too restrictive, he simplified and expanded the definition to this: ""A fractal is a rough or fragmented geometric shape that can be split into parts, each of which is (at least approximately) a reduced-size copy of the whole."" Still later, Mandelbrot proposed ""to use fractal without a pedantic definition, to use fractal dimension as a generic term applicable to all the variants"".The consensus among mathematicians is that theoretical fractals are infinitely self-similar iterated and detailed mathematical constructs, of which many examples have been formulated and studied. Fractals are not limited to geometric patterns, but can also describe processes in time. Fractal patterns with various degrees of self-similarity have been rendered or studied in visual, physical, and aural media and found in nature, technology, art, architecture and law. Fractals are of particular relevance in the field of chaos theory because they show up in the geometric depictions of most chaotic processes (typically either as attractors or as boundaries between basins of attraction).",Fractal,1,0
228,49072,"Sir Francis Galton, FRS FRAI (; 16 February 1822 – 17 January 1911), was an English Victorian era polymath: a statistician, sociologist, psychologist, anthropologist, tropical explorer, geographer, inventor, meteorologist, proto-geneticist, psychometrician and a proponent of social Darwinism, eugenics and scientific racism. He was knighted in 1909.
Galton produced over 340 papers and books. He also created the statistical concept of correlation and widely promoted regression toward the mean. He was the first to apply statistical methods to the study of human differences and inheritance of intelligence, and introduced the use of questionnaires and surveys for collecting data on human communities, which he needed for genealogical and biographical works and for his anthropometric studies. He was a pioneer of eugenics, coining the term itself in 1883, and also coined the phrase ""nature versus nurture"". His book Hereditary Genius (1869) was the first social scientific attempt to study genius and greatness.As an investigator of the human mind, he founded psychometrics (the science of measuring mental faculties) and differential psychology, as well as the lexical hypothesis of personality. He devised a method for classifying fingerprints that proved useful in forensic science. He also conducted research on the power of prayer, concluding it had none due to its null effects on the longevity of those prayed for. His quest for the scientific principles of diverse phenomena extended even to the optimal method for making tea.As the initiator of scientific meteorology, he devised the first weather map, proposed a theory of anticyclones, and was the first to establish a complete record of short-term climatic phenomena on a European scale. He also invented the Galton Whistle for testing differential hearing ability. He was Charles Darwin's half-cousin.",Francis Galton,1,0
229,6125615,"Charles Frederick Mosteller (December 24, 1916 – July 23, 2006) was one of the most eminent statisticians of the 20th century. He was the founding chairman of Harvard's statistics department from 1957 to 1971, and served as the president of several professional bodies including the Psychometric Society, the American Statistical Association, the Institute of Mathematical Statistics, the American Association for the Advancement of Science, and the International Statistical Institute.",Frederick Mosteller,1,0
230,3932847,"Statistical software are specialized computer programs for analysis in statistics and econometrics.

",List of statistical software,1,0
231,157934,"In cryptanalysis, frequency analysis (also known as counting letters) is the study of the frequency of letters or groups of letters in a ciphertext. The method is used as an aid to breaking classical ciphers.
Frequency analysis is based on the fact that, in any given stretch of written language, certain letters and combinations of letters occur with varying frequencies. Moreover, there is a characteristic distribution of letters that is roughly the same for almost all samples of that language. For instance, given a section of English language, E, T, A and O are the most common, while Z, Q, X and J are rare. Likewise, TH, ER, ON, and AN are the most common pairs of letters (termed bigrams or digraphs), and SS, EE, TT, and FF are the most common repeats. The nonsense phrase ""ETAOIN SHRDLU"" represents the 12 most frequent letters in typical English language text.
In some ciphers, such properties of the natural language plaintext are preserved in the ciphertext, and these patterns have the potential to be exploited in a ciphertext-only attack.

",Frequency analysis,1,0
232,4839019,"In statistics, the frequency (or absolute frequency) of an event 
  
    
      
        i
      
    
    {\displaystyle i}
   is the number 
  
    
      
        
          n
          
            i
          
        
      
    
    {\displaystyle n_{i}}
   of times the observation occurred/recorded in an experiment or study.: 12–19  These frequencies are often depicted graphically or in tabular form.",Frequency (statistics),1,0
233,370346,"In physics, electronics, control systems engineering, and statistics, the frequency domain refers to the analysis of mathematical functions or signals with respect to frequency, rather than time.  Put simply, a time-domain graph shows how a signal changes over time, whereas a frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies.  A frequency-domain representation can also include information on the phase shift that must be applied to each sinusoid in order to be able to recombine the frequency components to recover the original time signal.
A given function or signal can be converted between the time and frequency domains with a pair of mathematical operators called transforms.  An example is the Fourier transform, which converts a time function into a complex valued sum or integral of sine waves of different frequencies, with amplitudes and phases, each of which represents a frequency component.  The ""spectrum"" of frequency components is the frequency-domain representation of the signal.  The inverse Fourier transform converts the frequency-domain function back to the time-domain function. A spectrum analyzer is a tool commonly used to visualize electronic signals in the frequency domain.
Some specialized signal processing techniques use transforms that result in a joint time–frequency domain, with the instantaneous frequency being a key link between the time domain and the frequency domain.

",Frequency domain,1,0
234,15537745,"Frequentist inference is a type of statistical inference based in frequentist probability, which treats “probability” in equivalent terms to “frequency” and draws conclusions from sample-data by means of emphasizing the frequency or proportion of findings in the data. Frequentist-inference underlies frequentist statistics, in which the well-established methodologies of statistical hypothesis testing and confidence intervals are founded.",Frequentist inference,1,0
235,3175261,"The Friedman test is a non-parametric statistical test developed by Milton Friedman. Similar to the parametric repeated measures ANOVA, it is used to detect differences in treatments across multiple test attempts. The procedure involves ranking each row (or block) together, then considering the values of ranks by columns. Applicable to complete block designs, it is thus a special case of the Durbin test.
Classic examples of use are:

n wine judges each rate k different wines. Are any of the k wines ranked consistently higher or lower than the others?
n welders each use k welding torches, and the ensuing welds were rated on quality. Do any of the k torches produce consistently better or worse welds?The Friedman test is used for one-way repeated measures analysis of variance by ranks. In its use of ranks it is similar to the Kruskal–Wallis one-way analysis of variance by ranks.
The Friedman test is widely supported by many statistical software packages.

",Friedman test,1,0
236,11180,"Functional analysis is a branch of mathematical analysis, the core of which is formed by the study of vector spaces endowed with some kind of limit-related structure (e.g. inner product, norm, topology, etc.) and the linear functions defined on these spaces and respecting these structures in a suitable sense. The historical roots of functional analysis lie in the study of spaces of functions and the formulation of properties of transformations of functions such as the Fourier transform as transformations defining continuous, unitary etc. operators between function spaces. This point of view turned out to be particularly useful for the study of differential and integral equations.
The usage of the word functional as a noun goes back to the calculus of variations, implying a function whose argument is a function. The term was first used in Hadamard's 1910 book on that subject. However, the general concept of a functional had previously been introduced in 1887 by the Italian mathematician and physicist Vito Volterra. The theory of nonlinear functionals was continued by students of Hadamard, in particular Fréchet and Lévy. Hadamard also founded the modern school of linear functional analysis further developed by Riesz and the group of Polish mathematicians around Stefan Banach.
In modern introductory texts on functional analysis, the subject is seen as the study of vector spaces endowed with a topology, in particular infinite-dimensional spaces. In contrast, linear algebra deals mostly with finite-dimensional spaces, and does not use topology. An important part of functional analysis is the extension of the theory of measure, integration, and probability to infinite dimensional spaces, also known as infinite dimensional analysis.",Functional analysis,1,0
237,914820,"In statistics, G-tests are likelihood-ratio or maximum likelihood statistical significance tests that are increasingly being used in situations where chi-squared tests were previously recommended.The general formula for G is

  
    
      
        G
        =
        2
        
          ∑
          
            i
          
        
        
          
            O
            
              i
            
          
          ⋅
          ln
          ⁡
          
            (
            
              
                
                  O
                  
                    i
                  
                
                
                  E
                  
                    i
                  
                
              
            
            )
          
        
        ,
      
    
    {\displaystyle G=2\sum _{i}{O_{i}\cdot \ln \left({\frac {O_{i}}{E_{i}}}\right)},}
  where 
  
    
      
        
          O
          
            i
          
        
        ≥
        0
      
    
    {\textstyle O_{i}\geq 0}
   is the observed count in a cell, 
  
    
      
        
          E
          
            i
          
        
        >
        0
      
    
    {\textstyle E_{i}>0}
   is the expected count under the null hypothesis, 
  
    
      
        ln
      
    
    {\textstyle \ln }
   denotes the natural logarithm, and the sum is taken over all non-empty cells. Furthermore, the total observed count should be equal to the total expected count:where 
  
    
      
        N
      
    
    {\textstyle N}
   is the total number of observations.
G-tests have been recommended at least since the 1981 edition of Biometry, a statistics textbook by Robert R. Sokal and F. James Rohlf.",G-test,1,0
238,11924,"Game theory is the study of mathematical models of strategic interactions among rational agents. It has applications in all fields of social science, as well as in logic, systems science and computer science. Originally, it addressed two-person zero-sum games, in which each participant's gains or losses are exactly balanced by those of other participants. In the 21st century, game theory applies to a wide range of behavioral relations; it is now an umbrella term for the science of logical decision making in humans, animals, as well as computers.
Modern game theory began with the idea of mixed-strategy equilibria in two-person zero-sum game and its proof by John von Neumann. Von Neumann's original proof used the Brouwer fixed-point theorem on continuous mappings into compact convex sets, which became a standard method in game theory and mathematical economics. His paper was followed by the 1944 book Theory of Games and Economic Behavior, co-written with Oskar Morgenstern, which considered cooperative games of several players. The second edition of this book provided an axiomatic theory of expected utility, which allowed mathematical statisticians and economists to treat decision-making under uncertainty.
Game theory was developed extensively in the 1950s by many scholars. It was explicitly applied to evolution in the 1970s, although similar developments go back at least as far as the 1930s. Game theory has been widely recognized as an important tool in many fields. As of 2020, with the Nobel Memorial Prize in Economic Sciences going to game theorists Paul Milgrom and Robert B. Wilson, fifteen game theorists have won the economics Nobel Prize. John Maynard Smith was awarded the Crafoord Prize for his application of evolutionary game theory.",Game theory,1,1
239,53481,"A game of chance is a game whose outcome is strongly influenced by some randomizing device.  Common devices used include dice, spinning tops, playing cards, roulette wheels, or numbered balls drawn from a container. A game of chance may be played as gambling if players wage money or anything of monetary value.
Alternatively, a game of skill  is one in which the outcome is determined mainly by mental or physical skill, rather than chance.While a game of chance may have some skill element to it, chance generally plays a greater role in determining its outcome. A game of skill may also may have elements of chance, but skill plays a greater role in determining its outcome.
Gambling is known in nearly all human societies, even though many have passed laws restricting it.  Early people used the knucklebones of sheep as dice.  Some people develop a psychological addiction to gambling, and will risk even food and shelter to continue.
Some games of chance may also involve a certain degree of skill. This is especially true when the player or players have decisions to make based upon previous or incomplete knowledge, such as blackjack. In other games, such as roulette and punto banco, (baccarat) the player may only choose the amount of bet and the thing he wants to bet on; the rest is up to chance, therefore these games are still considered games of chance with small amount of skills required. The distinction between 'chance' and 'skill' is relevant because in some countries chance games are illegal or at least regulated, but skill games are not.  Since there is no standardized definition, poker, for example, has been ruled a game of chance in Germany and, by at least one New York state Federal judge, a game of skill there.",Game of chance,1,0
240,877698,"The general linear model or general multivariate regression model is a compact way of simultaneously writing several multiple linear regression models. In that sense it is not a separate statistical linear model. The various multiple linear regression models may be compactly written as

  
    
      
        
          Y
        
        =
        
          X
        
        
          B
        
        +
        
          U
        
        ,
      
    
    {\displaystyle \mathbf {Y} =\mathbf {X} \mathbf {B} +\mathbf {U} ,}
  where Y is a matrix with series of multivariate measurements (each column being a set of measurements on one of the dependent variables), X is a matrix of observations on independent variables that might be a design matrix (each column being a set of observations on one of the independent variables), B is a matrix containing parameters that are usually to be estimated and U is a matrix containing errors (noise).
The errors are usually assumed to be uncorrelated across measurements, and follow a multivariate normal distribution.  If the errors do not follow a multivariate normal distribution, generalized linear models may be used to relax assumptions about Y and U.
The general linear model incorporates a number of different statistical models: ANOVA, ANCOVA, MANOVA, MANCOVA, ordinary linear regression, t-test and F-test. The general linear model is a generalization of multiple linear regression to the case of more than one dependent variable. If Y, B, and U were column vectors, the matrix equation above would represent multiple linear regression.
Hypothesis tests with the general linear model can be made in two ways: multivariate or as several independent univariate tests. In multivariate tests the columns of Y are tested together, whereas in univariate tests the columns of Y are tested independently, i.e., as multiple univariate tests with the same design matrix.

",General linear model,1,0
241,178649,"In mathematics, general topology is the branch of topology that deals with the basic set-theoretic definitions and constructions used in topology. It is the foundation of most other branches of topology, including differential topology, geometric topology, and algebraic topology. Another name for general topology is point-set topology.
The fundamental concepts in point-set topology are continuity, compactness, and connectedness: 

Continuous functions, intuitively, take nearby points to nearby points.
Compact sets are those that can be covered by finitely many sets of arbitrarily small size.
Connected sets are sets that cannot be divided into two pieces that are far apart.The terms 'nearby', 'arbitrarily small', and 'far apart' can all be made precise by using the concept of open sets. If we change the definition of 'open set', we change what continuous functions, compact sets, and connected sets are. Each choice of definition for 'open set' is called a topology. A set with a topology is called a topological space.
Metric spaces are an important class of topological spaces where a real, non-negative distance, also called a metric, can be defined on pairs of points in the set. Having a metric simplifies many proofs, and many of the most common topological spaces are metric spaces.",General topology,1,0
242,747122,"In statistics, a generalized linear model (GLM) is a flexible generalization of ordinary linear regression. The GLM generalizes linear regression by allowing the linear model to be related to the response variable via a link function and by allowing the magnitude of the variance of each measurement to be a function of its predicted value.
Generalized linear models were formulated by John Nelder and Robert Wedderburn as a way of unifying various other statistical models, including linear regression, logistic regression and Poisson regression. They proposed an iteratively reweighted least squares method for maximum likelihood estimation (MLE) of the model parameters. MLE remains popular and is the default method on many statistical computing packages. Other approaches, including Bayesian regression and least squares fitting to variance stabilized responses, have been developed.",Generalized linear model,1,1
243,13143,"In mathematics, generalized means (or power mean or Hölder mean from Otto Hölder) are a family of functions for aggregating sets of numbers. These include as special cases the Pythagorean means (arithmetic, geometric, and harmonic means).",Generalized mean,1,0
244,12398,"A geographic information system (GIS) is a type of database containing geographic data (that is, descriptions of phenomena for which location is relevant), combined with software tools for managing, analyzing, and visualizing those data. In a broader sense, one may consider such a system to also include human users and support staff, procedures and workflows,  body of knowledge of relevant concepts and methods, and institutional organizations.
The uncounted plural, geographic information systems, also abbreviated GIS, is the most common term for the industry and profession concerned with these systems. It is roughly synonymous with geoinformatics and part of the broader geospatial field, which also includes GPS, remote sensing, etc. Geographic information science, the academic discipline that studies these systems and their underlying geographic principles, may also be abbreviated as GIS, but the unambiguous GIScience is more common.Geographic information systems are utilized in multiple technologies, processes, techniques and methods. They are attached to various operations and numerous applications, that relate to: engineering, planning, management, transport/logistics, insurance, telecommunications, and business. For this reason, GIS and location intelligence applications are at the foundation of location-enabled services, which rely on geographic analysis and visualization.
GIS provides the capability to relate previously unrelated information, through the use of location as the ""key index variable"". Locations and extents that are found in the Earth's spacetime are able to be recorded through the date and time of occurrence, along with x, y, and z coordinates; representing, longitude (x), latitude (y), and elevation (z). All Earth-based, spatial–temporal, location and extent references should be relatable to one another, and ultimately, to a ""real"" physical location or extent. This key characteristic of GIS has begun to open new avenues of scientific inquiry and studies.",Geographic information system,1,1
246,13046,"In mathematics, the geometric mean is a mean or average, which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the arithmetic mean which uses their sum). The geometric mean is defined as the nth root of the product of n numbers, i.e., for a set of numbers x1, x2, ..., xn, the geometric mean is defined as

  
    
      
        
          
            (
            
              
                ∏
                
                  i
                  =
                  1
                
                
                  n
                
              
              
                x
                
                  i
                
              
            
            )
          
          
            
              1
              n
            
          
        
        =
        
          
            
              
                x
                
                  1
                
              
              
                x
                
                  2
                
              
              ⋯
              
                x
                
                  n
                
              
            
            
              n
            
          
        
      
    
    {\displaystyle \left(\prod _{i=1}^{n}x_{i}\right)^{\frac {1}{n}}={\sqrt[{n}]{x_{1}x_{2}\cdots x_{n}}}}
  or, equivalently, as the arithmetic mean in logscale:

  
    
      
        exp
        ⁡
        
          
            (
            
              
                
                  1
                  n
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              ln
              ⁡
              
                a
                
                  i
                
              
            
            )
          
        
      
    
    {\displaystyle \exp {\left({{\frac {1}{n}}\sum \limits _{i=1}^{n}\ln a_{i}}\right)}}
  For instance, the geometric mean of two numbers, say 2 and 8, is just the square root of their product, that is, 
  
    
      
        
          
            2
            ⋅
            8
          
        
        =
        4
      
    
    {\displaystyle {\sqrt {2\cdot 8}}=4}
  . As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the cube root of their product (1/8), which is 1/2, that is, 
  
    
      
        
          
            
              4
              ⋅
              1
              ⋅
              1
              
                /
              
              32
            
            
              3
            
          
        
        =
        1
        
          /
        
        2
      
    
    {\displaystyle {\sqrt[{3}]{4\cdot 1\cdot 1/32}}=1/2}
  . The geometric mean applies only to positive numbers.The geometric mean is often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as a set of growth figures:  values of the human population or interest rates of a financial investment over time. It also applies to benchmarking, where it is particularly useful for computing means of speedup ratios: since the mean of 0.5x (half as fast) and 2x (twice as fast) will be 1 (i.e., no speedup overall).
The geometric mean can be understood in terms of geometry. The geometric mean of two numbers, 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  , is the length of one side of a square whose area is equal to the area of a rectangle with sides of lengths 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
  . Similarly, the geometric mean of three numbers, 
  
    
      
        a
      
    
    {\displaystyle a}
  , 
  
    
      
        b
      
    
    {\displaystyle b}
  , and 
  
    
      
        c
      
    
    {\displaystyle c}
  , is the length of one edge of a cube whose volume is the same as that of a cuboid with sides whose lengths are equal to the three given numbers.
The geometric mean is one of the three classical Pythagorean means, together with the arithmetic mean and the harmonic mean. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see Inequality of arithmetic and geometric means.)",Geometric mean,1,0
247,621732,"In mathematics, geometric topology is the study of manifolds and maps between them, particularly embeddings of one manifold into another.",Geometric topology,1,0
248,18973446,"Geometry (from Ancient Greek  γεωμετρία (geōmetría) 'land measurement'; from  γῆ (gê) 'earth, land', and  μέτρον (métron) 'a measure') is, with arithmetic, one of the oldest branches of mathematics. It is concerned with properties of space that are related with distance, shape, size, and relative position of figures. A mathematician who works in the field of geometry is called a geometer.
Until the 19th century, geometry was almost exclusively devoted to Euclidean geometry, which includes the notions of point, line, plane, distance, angle, surface, and curve, as fundamental concepts.During the 19th century several discoveries enlarged dramatically the scope of geometry. One of the oldest such discoveries is Gauss' Theorema Egregium (""remarkable theorem"") that asserts roughly that the Gaussian curvature of a surface is independent from any specific embedding in a Euclidean space. This implies that surfaces can be studied intrinsically, that is, as stand-alone spaces, and has been expanded into the theory of manifolds and Riemannian geometry.
Later in the 19th century, it appeared that geometries without the parallel postulate (non-Euclidean geometries) can be developed without introducing any contradiction. The geometry that underlies general relativity is a famous application of non-Euclidean geometry.
Since then, the scope of geometry has been greatly expanded, and the field has been split in many subfields that depend on the underlying methods—differential geometry, algebraic geometry, computational geometry, algebraic topology, discrete geometry (also known as combinatorial geometry), etc.—or on the properties of Euclidean spaces that are disregarded—projective geometry that consider only alignment of points but not distance and parallelism, affine geometry that omits the concept of angle and distance, finite geometry that omits continuity, and others.
Originally developed to model the physical world, geometry has applications in almost all sciences, and also in art, architecture, and other activities that are related to graphics. Geometry also has applications in areas of mathematics that are apparently unrelated. For example, methods of algebraic geometry are fundamental in Wiles's proof of Fermat's Last Theorem, a problem that was stated in terms of elementary arithmetic, and remained unsolved for several centuries.",Geometry,1,0
249,13145,"Gerolamo Cardano (Italian: [dʒeˈrɔːlamo karˈdaːno]; also Girolamo or Geronimo; French: Jérôme Cardan; Latin: Hieronymus Cardanus; 24 September 1501– 21 September 1576) was an Italian polymath, whose interests and proficiencies ranged through those of mathematician, physician, biologist, physicist, chemist, astrologer, astronomer, philosopher, writer, and gambler. He was one of the most influential mathematicians of the Renaissance, and was one of the key figures in the foundation of probability and the earliest introducer of the binomial coefficients and the binomial theorem in the Western world. He wrote more than 200 works on science.Cardano partially invented and described several mechanical devices including the combination lock, the gimbal consisting of three concentric rings allowing a supported compass or gyroscope to rotate freely, and the Cardan shaft with universal joints, which allows the transmission of rotary motion at various angles and is used in vehicles to this day. He made significant contributions to hypocycloids, published in De proportionibus, in 1570. The generating circles of these hypocycloids were later named Cardano circles or cardanic circles and were used for the construction of the first high-speed printing presses.Today, he is well known for his achievements in algebra. In his 1545 book Ars Magna, he made the first systematic use of negative numbers in Europe, published with attribution the solutions of other mathematicians for the cubic and quartic equations, and acknowledged the existence of imaginary numbers.",Gerolamo Cardano,1,0
250,509709,"In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations which are approximated from a specified multivariate probability distribution, when direct sampling is difficult.  This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); or to compute an integral (such as the expected value of one of the variables).  Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.
Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference.  It is a randomized algorithm (i.e. an algorithm that makes use of random numbers), and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).
As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. As a result, care must be taken if independent samples are desired. Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded.",Gibbs sampling,1,0
251,2474821,"The goodness of fit of a statistical model describes how well it fits a set of observations. Measures of goodness of fit typically summarize the discrepancy between observed values and the values expected under the model in question. Such measures can be used in statistical hypothesis testing, e.g. to test for normality of residuals, to test whether two samples are drawn from identical distributions  (see Kolmogorov–Smirnov test), or whether outcome frequencies follow a specified distribution (see Pearson's chi-square test).  In the analysis of variance, one of the components into which the variance is partitioned may be a lack-of-fit sum of squares.",Goodness of fit,1,0
252,1648224,"The Granger causality test is a statistical hypothesis test for determining whether one time series is useful in forecasting another, first proposed in 1969. Ordinarily, regressions reflect ""mere"" correlations, but Clive Granger argued that causality in economics could be tested for by measuring the ability to predict the future values of a time series using prior values of another time series. Since the question of ""true causality"" is deeply philosophical, and because of the post hoc ergo propter hoc fallacy of assuming that one thing preceding another can be used as a proof of causation, econometricians assert that the Granger test finds only ""predictive causality"". Using the term ""causality"" alone is a misnomer, as Granger-causality is better described as  ""precedence"", or, as Granger himself later claimed in 1977, ""temporally related"". Rather than testing whether X causes Y, the Granger causality tests whether X forecasts Y.A time series X is said to Granger-cause Y if it can be shown, usually through a series of t-tests and F-tests on lagged values of X (and with lagged values of Y also included), that those X values provide statistically significant information about future values of Y.
Granger also stressed that some studies using ""Granger causality"" testing in areas outside economics reached ""ridiculous"" conclusions. ""Of course, many ridiculous papers appeared"", he said in his Nobel lecture. However, it remains a popular method for causality analysis in time series due to its computational simplicity. The original definition of Granger causality does not account for latent confounding effects and does not capture instantaneous and non-linear causal relationships, though several extensions have been proposed to address these issues.

",Granger causality,1,0
253,12401,"In mathematics, graph theory is the study of graphs, which are mathematical structures used to model pairwise relations between objects. A graph in this context is made up of vertices (also called nodes or points) which are connected by edges (also called links or lines). A distinction is made between undirected graphs, where edges link two vertices symmetrically, and directed graphs, where edges link two vertices asymmetrically. Graphs are one of the principal objects of study in discrete mathematics.",Graph theory,1,0
254,447298,"A graphical model or probabilistic graphical model (PGM) or structured probabilistic model is a probabilistic model for which a graph expresses the conditional dependence structure between random variables. They are commonly used in probability theory, statistics—particularly Bayesian statistics—and machine learning.",Graphical model,1,1
255,4305214,"Gretel is a German shortening of the given name Margarete.  
Notable people with this given name include:

A fictional character in the Brothers Grimm fairy tale Hansel and Gretel
Gretel Beer (1921–2010), Austrian-born English author of cooking books and travel reports and newspaper cookery writer
Gretel Bergmann (1914–2017), German Jewish high jumper who was prevented from competing in the 1936 Berlin Olympics
Gretel Ehrlich (born 1946), American travel writer, poet, and essayist
Gretel Killeen (born 1963), Australian presenter
Gretel Oberhollenzer-Rogger (born 1958), Italian ski mountaineer
Gretel Scarlett (born 1987), Australian actress
Gretel Tippett (born 1993), Australian netball player and former WNBL basketball player",Gretel,1,0
256,41890,"In abstract algebra, group theory studies the algebraic structures known as groups.  
The concept of a group is central to abstract algebra: other well-known algebraic structures, such as rings, fields, and vector spaces, can all be seen as groups endowed with additional operations and axioms. Groups recur throughout mathematics, and the methods of group theory have influenced many parts of algebra. Linear algebraic groups and Lie groups are two branches of group theory that have experienced advances and have become subject areas in their own right.
Various physical systems, such as crystals and the hydrogen atom, and three of the four known fundamental forces in the universe, may be modelled by symmetry groups. Thus group theory and the closely related representation theory have many important applications in physics, chemistry, and materials science. Group theory is also central to public key cryptography.
The early history of group theory dates from the 19th century. One of the most important mathematical achievements of the 20th century was the collaborative effort, taking up more than 10,000 journal pages and mostly published between 1960 and 2004, that culminated in a complete classification of finite simple groups.",Group theory,1,0
257,27705422,"Grouped data are data formed by aggregating individual observations of a variable into groups, so that a frequency distribution of these groups serves as a convenient means of summarizing or analyzing the data. There are two major types of grouping: data binning of a single-dimensional variable, replacing individual numbers by counts in bins; and grouping multi-dimensional variables by some of the dimensions (especially by independent variables), obtaining the distribution of ungrouped dimensions (especially the dependent variables).",Grouped data,1,0
258,14147,"Harmonic analysis is a branch of mathematics concerned with the representation of functions or signals as the superposition of basic waves, and the study of and generalization of the notions of Fourier series and Fourier transforms (i.e. an extended form of Fourier analysis). In the past two centuries, it has become a vast subject with applications in areas as diverse as number theory, representation theory, signal processing, quantum mechanics, tidal analysis and neuroscience.
The term ""harmonics"" originated as the Ancient Greek word harmonikos, meaning ""skilled in music"". In physical eigenvalue problems, it began to mean waves whose frequencies are integer multiples of one another, as are the frequencies of the harmonics of music notes, but the term has been generalized beyond its original meaning.
The classical Fourier transform on Rn is still an area of ongoing research, particularly concerning Fourier transformation on more general objects such as tempered distributions. For instance, if we impose some requirements on a distribution f, we can attempt to translate these requirements in terms of the Fourier transform of f. The Paley–Wiener theorem is an example of this. The Paley–Wiener theorem immediately implies that if f is a nonzero distribution of compact support (these include functions of compact support), then its Fourier transform is never compactly supported (i.e. if a signal is limited in one domain, it is unlimited in the other). This is a very elementary form of an uncertainty principle in a harmonic-analysis setting.
Fourier series can be conveniently studied in the context of Hilbert spaces, which provides a connection between harmonic analysis and functional analysis. There are four versions of the Fourier transform, dependent on the spaces that are mapped by the transformation (discrete/periodic–discrete/periodic: discrete Fourier transform, continuous/periodic–discrete/aperiodic: Fourier series, discrete/aperiodic–continuous/periodic: discrete-time Fourier transform, continuous/aperiodic–continuous/aperiodic: Fourier transform).",Harmonic analysis,1,0
259,14463,"In mathematics, the harmonic mean is one of several kinds of average, and in particular, one of the Pythagorean means. It is sometimes appropriate for situations when the average rate is desired.
The harmonic mean can be expressed as the reciprocal of the arithmetic mean of the reciprocals of the given set of observations. As a simple example, the harmonic mean of 1, 4, and 4 is

  
    
      
        
          
            (
            
              
                
                  
                    1
                    
                      −
                      1
                    
                  
                  +
                  
                    4
                    
                      −
                      1
                    
                  
                  +
                  
                    4
                    
                      −
                      1
                    
                  
                
                3
              
            
            )
          
          
            −
            1
          
        
        =
        
          
            3
            
              
                
                  1
                  1
                
              
              +
              
                
                  1
                  4
                
              
              +
              
                
                  1
                  4
                
              
            
          
        
        =
        
          
            3
            1.5
          
        
        =
        2
        
        .
      
    
    {\displaystyle \left({\frac {1^{-1}+4^{-1}+4^{-1}}{3}}\right)^{-1}={\frac {3}{{\frac {1}{1}}+{\frac {1}{4}}+{\frac {1}{4}}}}={\frac {3}{1.5}}=2\,.}",Harmonic mean,1,0
260,228851,"A faulty generalization is an informal fallacy wherein a conclusion is drawn about all or many instances of a phenomenon on the basis of one or a few instances of that phenomenon.  It is similar to a proof by example in mathematics. It is an example of jumping to conclusions. For example, one may generalize about all people or all members of a group, based on what one knows about just one or a few people:

If one meets a rude person from a given country X, one may suspect that most people in country X are rude.
If one sees only white swans, one may suspect that all swans are white.Expressed in more precise philosophical language, a fallacy of defective induction is a conclusion that has been made on the basis of weak premises, or one which is not justified by sufficient or unbiased evidence. Unlike fallacies of relevance, in fallacies of defective induction, the premises are related to the conclusions, yet only weakly buttress the conclusions, hence a faulty generalization is produced. The essence of this inductive fallacy lies on the overestimation of an argument based on insufficiently-large samples under an implied margin or error.",Faulty generalization,1,0
261,474880,"The Hawthorne effect is a type of reactivity in which individuals modify an aspect of their behavior in response to their awareness of being observed. The effect was discovered in the context of research conducted at the Hawthorne Western Electric plant; however, some scholars feel the descriptions are apocryphal.The original research involved workers who made electrical relays at the Hawthorne Works, a Western Electric plant in Cicero, Illinois. Between 1924 and 1927, the lighting study was conducted. Workers experienced a series of lighting changes in which productivity was said to increase with almost any change in the lighting. This turned out not to be true. In the study that was associated with Elton Mayo, which ran from 1928 to 1932, a series of changes in work structure were implemented (e.g., changes in rest periods) in a group of five women. However, this was a methodologically poor, uncontrolled study that did not permit any firm conclusions to be drawn.One of the later interpretations by Landsberger suggested that the novelty of being research subjects and the increased attention from such could lead to temporary increases in workers' productivity. This interpretation was dubbed ""the Hawthorne effect"".

",Hawthorne effect,1,1
263,6967420,"In mathematics, the Heinz mean (named after E. Heinz) of two non-negative real numbers A and B, was defined by Bhatia as:

  
    
      
        
          H
          
            x
          
        
        ⁡
        (
        A
        ,
        B
        )
        =
        
          
            
              
                A
                
                  x
                
              
              
                B
                
                  1
                  −
                  x
                
              
              +
              
                A
                
                  1
                  −
                  x
                
              
              
                B
                
                  x
                
              
            
            2
          
        
        .
      
    
    {\displaystyle \operatorname {H} _{x}(A,B)={\frac {A^{x}B^{1-x}+A^{1-x}B^{x}}{2}}.}
  with 0 ≤ x ≤ 1/2.
For different values of x, this Heinz mean interpolates between the arithmetic (x = 0) and geometric (x = 1/2) means such that for 0 < x < 1/2:

  
    
      
        
          
            A
            B
          
        
        =
        
          H
          
            
              1
              2
            
          
        
        ⁡
        (
        A
        ,
        B
        )
        <
        
          H
          
            x
          
        
        ⁡
        (
        A
        ,
        B
        )
        <
        
          H
          
            0
          
        
        ⁡
        (
        A
        ,
        B
        )
        =
        
          
            
              A
              +
              B
            
            2
          
        
        .
      
    
    {\displaystyle {\sqrt {AB}}=\operatorname {H} _{\frac {1}{2}}(A,B)<\operatorname {H} _{x}(A,B)<\operatorname {H} _{0}(A,B)={\frac {A+B}{2}}.}
  The Heinz means appear naturally when symmetrizing 

  
    
      
        α
      
    
    {\textstyle \alpha }
  -divergences.The Heinz mean may also be defined in the same way for positive semidefinite matrices, and satisfies a similar interpolation formula.",Heinz mean,1,0
264,70780067,"In statistics, a sequence (or a vector) of random variables is homoscedastic if all its random variables have the same finite variance. This is also known as homogeneity of variance. The complementary notion is called heteroscedasticity. The spellings homoskedasticity and heteroskedasticity are also frequently used.Assuming a  variable is homoscedastic when in reality it is heteroscedastic () results in unbiased but inefficient point estimates and in biased estimates of standard errors, and may result in overestimating the goodness of fit as measured by the Pearson coefficient.
The existence of heteroscedasticity is a major concern in  regression analysis and the analysis of variance, as it invalidates statistical tests of significance that assume that the modelling errors all have the same variance. While the ordinary least squares estimator is still unbiased in the presence of heteroscedasticity, it is inefficient and generalized least squares should be used instead.Because heteroscedasticity concerns expectations of the second moment of the errors, its presence is referred to as misspecification of the second order.The econometrician Robert Engle was awarded the 2003 Nobel Memorial Prize for Economics for his studies on regression analysis in the presence of heteroscedasticity, which led to his formulation of the autoregressive conditional heteroscedasticity (ARCH) modeling technique.

",Homoscedasticity and heteroscedasticity,1,1
265,13266,"A histogram is an approximate representation of the distribution of numerical data. The term was first introduced by Karl Pearson.  To construct a histogram, the first step is to ""bin"" (or ""bucket"") the range of values—that is, divide the entire range of values into a series of intervals—and then count how many values fall into each interval.  The bins are usually specified as consecutive, non-overlapping intervals of a variable. The bins (intervals) must be adjacent and are often (but not required to be) of equal size.If the bins are of equal size, a rectangle is erected over the bin with height proportional to the frequency—the number of cases in each bin. A histogram may also be normalized to display ""relative"" frequencies. It then shows the proportion of cases that fall into each of several categories, with the sum of the heights equaling 1.
However, bins need not be of equal width; in that case, the erected rectangle is defined to have its area proportional to the frequency of cases in the bin. The vertical axis is then not the frequency but frequency density—the number of cases per unit of the variable on the horizontal axis. Examples of variable bin width are displayed on Census bureau data below.
As the adjacent bins leave no gaps, the rectangles of a histogram touch each other to indicate that the original variable is continuous.Histograms give a rough sense of the density of the underlying distribution of the data, and often for density estimation: estimating the probability density function of the underlying variable. The total area of a histogram used for probability density is always normalized to 1. If the length of the intervals on the x-axis are all 1, then a histogram is identical to a relative frequency plot.
A histogram can be thought of as a simplistic kernel density estimation, which uses a kernel to smooth frequencies over the bins. This yields a smoother probability density function, which will in general more accurately reflect distribution of the underlying variable. The density estimate could be plotted as an alternative to the histogram, and is usually drawn as a curve rather than a set of boxes. Histograms are nevertheless preferred in applications, when their statistical properties need to be modeled. The correlated variation of a kernel density estimate is very difficult to describe mathematically, while it is simple for a histogram where each bin varies independently.
An alternative to kernel density estimation is the average shifted histogram,
which is fast to compute and gives a smooth curve estimate of the density without using kernels.
The histogram is one of the seven basic tools of quality control.Histograms are sometimes confused with bar charts. A histogram is used for continuous data, where the bins represent ranges of data, while a bar chart is a plot of categorical variables. Some authors recommend that bar charts have gaps between the rectangles to clarify the distinction.",Histogram,1,0
266,14220,"The history of mathematics deals with the origin of discoveries in mathematics and the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the patterns in nature, the field of astronomy and to record time and formulate calendars.
The earliest mathematical texts available are from Mesopotamia and Egypt – Plimpton 322 (Babylonian c. 2000 – 1900 BC), the Rhind Mathematical Papyrus (Egyptian c. 1800 BC) and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a ""demonstrative discipline"" began in the 6th century BC with the Pythagoreans, who coined the term ""mathematics"" from the ancient Greek μάθημα (mathema), meaning ""subject of instruction"". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.
Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century. At the end of the 19th century the International Congress of Mathematicians was founded and continues to spearhead advances in the field.",History of mathematics,1,0
267,14986442,"Statistics, in the modern sense of the word, began evolving in the 18th century in response to the novel needs of industrializing sovereign states. The evolution of statistics was, in particular, intimately connected with the development of European states following the peace of Westphalia (1648), and with the development of probability theory, which put statistics on a firm  theoretical basis.
In early times, the meaning was restricted to information about states, particularly demographics such as population. This was later extended to include all collections of information of all types, and later still it was extended to include the analysis and interpretation of such data. In modern terms, ""statistics"" means both sets of collected information, as in national accounts and temperature record, and analytical work which requires statistical inference. Statistical activities are often associated with models expressed using probabilities, hence the connection with probability theory. The large requirements of data processing have made statistics a key application of computing. A number of statistical concepts have an important impact on a wide range of sciences. These include the design of experiments and approaches to statistical inference such as Bayesian inference, each of which can be considered to have their own sequence in the development of the ideas underlying modern statistics.",History of statistics,1,0
268,9622185,"In statistics, the Hodges–Lehmann estimator is a robust and nonparametric estimator of a population's location parameter. For populations that are symmetric about one median, such as the (Gaussian) normal distribution or the Student t-distribution, the Hodges–Lehmann estimator is a consistent and median-unbiased estimate of the population median. For non-symmetric populations, the Hodges–Lehmann estimator estimates the ""pseudo–median"", which is closely related to the population median.
The Hodges–Lehmann estimator was proposed originally for estimating the location parameter of  one-dimensional populations, but it has been used for many more purposes. It has been used to estimate the differences between the members of two populations.  It has been generalized from univariate populations to multivariate populations, which produce samples of vectors.
It is based on the Wilcoxon signed-rank statistic. In statistical theory, it was an early example of a  rank-based estimator, an important class of estimators both in  nonparametric statistics and in robust statistics. The Hodges–Lehmann estimator was proposed in 1963 independently by Pranab Kumar Sen and by Joseph Hodges and Erich Lehmann, and so it is also called the ""Hodges–Lehmann–Sen estimator"".",Hodges–Lehmann estimator,1,0
269,139410,"Homological algebra is the branch of mathematics that studies homology in a general algebraic setting. It is a relatively young discipline, whose origins can be traced to investigations in combinatorial topology (a precursor to algebraic topology) and abstract algebra (theory of modules and syzygies) at the end of the 19th century, chiefly by Henri Poincaré and David Hilbert. 
The development of homological algebra was closely intertwined with the emergence of category theory. By and large, homological algebra is the study of homological functors and the intricate algebraic structures that they entail. One quite useful and ubiquitous concept in mathematics is that of chain complexes, which can be studied through both their homology and cohomology. Homological algebra affords the means to extract information contained in these complexes and present it in the form of homological invariants of rings, modules, topological spaces, and other 'tangible' mathematical objects. A powerful tool for doing this is provided by spectral sequences.
From its very origins, homological algebra has played an enormous role in algebraic topology. Its influence has gradually expanded and presently includes commutative algebra, algebraic geometry, algebraic number theory, representation theory, mathematical physics, operator algebras, complex analysis, and the theory of partial differential equations. K-theory is an independent discipline which draws upon methods of homological algebra, as does the noncommutative geometry of Alain Connes.",Homological algebra,1,0
271,61994831,"In mathematics, homotopy theory is a systematic study of situations in which maps come with homotopies between them. It originated as a topic in algebraic topology but nowadays is studied as an independent discipline. Besides algebraic topology, the theory has also been used in other areas of mathematics such as algebraic geometry (e.g., A1 homotopy theory) and category theory (specifically the study of higher categories).",Homotopy theory,1,0
272,2099648,"How to Lie with Statistics is a book written by Darrell Huff in 1954 presenting an introduction to statistics for the general reader. Not a statistician, Huff was a journalist who wrote many ""how to"" articles as a freelancer.
The book is a brief, breezy illustrated volume outlining the misuse of statistics and errors in the interpretation of statistics, and how these errors may create incorrect conclusions.
In the 1960s and 1970s, it became a standard textbook introduction to the subject of statistics for many college students.  It has become one of the best-selling statistics books in history, with over one and a half million copies sold in the English-language edition. It has also been widely translated.
Themes of the book include ""Correlation does not imply causation"" and ""Using random sampling"". It also shows how statistical graphs can be used to distort reality, for example by truncating the bottom of a line or bar chart, so that differences seem larger than they are, or by representing one-dimensional quantities on a pictogram by two- or three-dimensional objects to compare their sizes, so that the reader forgets that the images do not scale the same way the quantities do.
The original edition contained illustrations by artist Irving Geis. In a UK edition, these were replaced with cartoons by Mel Calman.

",How to Lie with Statistics,1,0
273,26666199,"In mathematics, hypercomplex analysis is the basic extension of real analysis and complex analysis to the study of functions where the argument is a hypercomplex number. The first instance is functions of a quaternion variable, where the argument is a quaternion (in this case, the sub-field of hypercomplex analysis is called quaternionic analysis).  A second instance involves functions of a motor variable where arguments are split-complex numbers.
In mathematical physics, there are hypercomplex systems called Clifford algebras. The study of functions with arguments from a Clifford algebra is called Clifford analysis.
A matrix may be considered a hypercomplex number. For example, the study of functions of 2 × 2 real matrices shows that the topology of the space of hypercomplex numbers determines the function theory. Functions such as square root of a matrix, matrix exponential, and logarithm of a matrix are basic examples of hypercomplex analysis. 
The function theory of diagonalizable matrices is particularly transparent since they have eigendecompositions. Suppose 
  
    
      
        
          T
          =
          
            ∑
            
              i
              =
              1
            
            
              N
            
          
          
            λ
            
              i
            
          
          
            E
            
              i
            
          
        
      
    
    {\displaystyle \textstyle T=\sum _{i=1}^{N}\lambda _{i}E_{i}}
   where the Ei are projections. Then for any polynomial 
  
    
      
        f
      
    
    {\displaystyle f}
  , 
  
    
      
        f
        (
        T
        )
        =
        
          ∑
          
            i
            =
            1
          
          
            N
          
        
        f
        (
        
          λ
          
            i
          
        
        )
        
          E
          
            i
          
        
        .
      
    
    {\displaystyle f(T)=\sum _{i=1}^{N}f(\lambda _{i})E_{i}.}
   
The modern terminology for a ""system of hypercomplex numbers"" is an algebra over the real numbers, and the algebras used in applications are often Banach algebras since Cauchy sequences can be taken to be convergent. Then the function theory is enriched by sequences and series. In this context the extension of holomorphic functions of a complex variable is developed as the holomorphic functional calculus.  Hypercomplex analysis on Banach algebras is called functional analysis.",Hypercomplex analysis,1,0
275,97922,"Digital image processing is the use of a digital computer to process digital images through an algorithm. As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased.

",Digital image processing,1,0
276,30876902,"In probability theory and  statistics, a collection of random variables is independent and identically distributed if each random variable has the same probability distribution as the others and all are mutually independent. This property is usually abbreviated as i.i.d. or iid or IID. IID was first used in statistics. With the development of science, IID has been applied in different fields such as data mining and signal processing.",Independent and identically distributed random variables,1,0
277,43558650,"In statistics and research design, an index is a composite statistic – a measure of changes in a representative group of individual data points, or in other words, a compound measure that aggregates multiple indicators. Indexes – also known as composite indicators – summarize and rank specific observations.Much data in the field of social sciences and sustainability are represented in various indices such as Gender Gap Index, Human Development Index or the Dow Jones Industrial Average. The ‘Report by the Commission on the Measurement of Economic Performance and Social Progress’, written by Joseph Stiglitz, Amartya Sen, and Jean-Paul Fitoussi in 2009  suggests that these measures have experienced a dramatic growth in recent years due to three concurring factors: 

improvements in the level of literacy (including statistical)
increased complexity of modern societies and economies, and
widespread availability of information technology.According to Earl Babbie, items in indexes are usually weighted equally, unless there are some reasons against it (for example, if two items reflect essentially the same aspect of a variable, they could have a weight of 0.5 each).According to the same author, constructing the items involves four steps. First, items should be selected based on their content validity, unidimensionality, the degree of specificity in which a dimension is to be measured, and their amount of variance. Items should be empirically related to one another, which leads to the second step of examining their multivariate relationships. Third, indexes scores are designed, which involves determining their score ranges and weights for the items. Finally, indexes should be validated, which involves testing whether they can predict indicators related to the measured variable not used in their construction.A handbook for the construction of composite indicators was published jointly by the OECD and by the European Commission's Joint Research Centre in 2008. The handbook – officially endorsed by the OECD high level statistical committee, describe ten recursive steps for developing an index:
Step 1: Theoretical framework
Step 2: Data selection
Step 3: Imputation of missing data
Step 4: Multivariate analysis
Step 5: Normalisation
Step 6: Weighting
Step 7: Aggregating indicators
Step 8: Sensitivity analysis
Step 9: Link to other measures
Step 10: VisualisationAs suggested by the list, many modelling choices are needed to construct a composite indicator, which makes their use controversial. The delicate issue of assigning and validating weights is discussed e.g. in. A sociological reading of the nature of composite indicators is offered by Paul-Marie Boulanger, who sees these measures at the intersection of three movements:
the democratisation of expertise, the concept that more knowledge is needed to tackle societal and environmental issues that can be provided by the sole experts – this line of thought connects to the concept of extended peer community developed by post-normal science
the impulse to the creation of a new public through a process of social discovery, which can be reconnected to the work of pragmatists such as John Dewey
the semiotic of Charles Sanders Peirce; Thus a CI is not just a sign or a number, but suggests an action or a behaviour.A subsequent work of the same author  analyses composite indicators in light of the social system theories of Niklas Luhmann to investigate how different measurements of progress are or are not taken up",Index (statistics),1,0
278,16693606,"In probability theory and statistics, the index of dispersion, dispersion index, coefficient of dispersion, relative variance, or variance-to-mean ratio (VMR), like the coefficient of variation, is a normalized measure of the dispersion of a probability distribution: it is a measure used to quantify whether a set of observed occurrences are clustered or dispersed compared to a standard statistical model.
It is defined as the ratio of the variance 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
   to the mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
  ,

  
    
      
        D
        =
        
          
            
              σ
              
                2
              
            
            μ
          
        
        .
      
    
    {\displaystyle D={\sigma ^{2} \over \mu }.}
  It is also known as the Fano factor, though this term is sometimes reserved for windowed data (the mean and variance are computed over a subpopulation), where the index of dispersion is used in the special case where the window is infinite. Windowing data is frequently done: the VMR is frequently computed over various intervals in time or small regions in space, which may be called ""windows"", and the resulting statistic called the Fano factor.
It is only defined when the mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is non-zero, and is generally only used for positive statistics, such as count data or time between events, or where the underlying distribution is assumed to be the exponential distribution or Poisson distribution.

",Index of dispersion,1,0
279,393736,"Inductive reasoning is a method of reasoning in which a body of observations is considered to derive a general principle. It consists of making broad generalizations based on specific observations. Inductive reasoning is distinct from deductive reasoning. If the premises are correct, the conclusion of a deductive argument is certain; in contrast, the truth of the conclusion of an inductive argument is probable, based upon the evidence given.",Inductive reasoning,1,1
280,27577,"Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability. Inferential statistical analysis infers properties of a population, for example by testing hypotheses and deriving estimates.  It is assumed that the observed data set is sampled from a larger population.
Inferential statistics can be contrasted with descriptive statistics. Descriptive statistics is solely concerned with properties of the observed data, and it does not rest on the assumption that the data come from a larger population. In machine learning, the term inference is sometimes used instead to mean ""make a prediction, by evaluating an already trained model""; in this context inferring properties of the model is referred to as training or learning (rather than inference), and using a model for prediction is referred to as inference (instead of prediction); see also predictive inference.",Statistical inference,1,1
281,185493,"Informal mathematics, also called naïve mathematics, has historically been the predominant form of mathematics at most times and in most cultures, and is the subject of modern ethno-cultural studies of mathematics. The philosopher Imre Lakatos in his Proofs and Refutations aimed to sharpen the formulation of informal mathematics, by reconstructing its role in nineteenth century mathematical debates and concept formation, opposing the predominant assumptions of mathematical formalism.  Informality may not discern between statements given by inductive reasoning (as in approximations which are deemed ""correct"" merely because they are useful), and statements derived by deductive reasoning.",Informal mathematics,1,0
282,18985062,"Information is that portion of the content of a signal or message which conveys meaning.  Information is not knowledge itself, but rather the representation of it.  Information is often layered:  The data available at one level are processed into information to be interpreted at the next level.  For example, in written text each symbol or letter conveys information relevant to the word it is part of, each word conveys information relevant to the phrase it is part of, each phrase conveys information relevant to the sentence it is part of, and so on until information at the top level is interpreted and becomes knowledge of the pertinent domain.  In a digital signal there is a base level of information that conveys bits which may then be interpreted into the symbols, letters, numbers, or structures that convey the information available at the next level up.  The key characteristic of information is that it is subject to interpretation and processing.
The concept of information is used to describe various concepts, including constraint, communication, control, data, form, education, knowledge, meaning, understanding, mental stimuli, pattern, perception, proposition, representation, and entropy.
The derivation of information from a signal or message may be thought of as the resolution of ambiguity or uncertainty that arises during the interpretation of patterns within the signal or message.Information may be structured as data. Redundant data can be compressed up to an optimal size, which is the theoretical limit of compression.
The information available through a collection of data may be derived by analysis.  For example, data may be collected from a single customer’s order at a restaurant.  The information available from many orders may be analyzed, and then becomes knowledge that is put to use when the business subsequently is able to identify the most popular or least popular dish.Information can be transmitted in time, via data storage, and space, via communication and telecommunication. Information is expressed either as the content of a message or through direct or indirect observation. That which is perceived can be construed as a message in its own right, and in that sense, information is always conveyed as the content of a message.
Information can be encoded into various forms for transmission and interpretation (for example, information may be encoded into a sequence of signs, or transmitted via a signal). It can also be encrypted for safe storage and communication.
The uncertainty of an event is measured by its probability of occurrence. Uncertainty is inversely proportional to the probability of occurrence. Information theory takes advantage of this by concluding that more uncertain events require more information to resolve their uncertainty. The bit is a typical unit of information. It is 'that which reduces uncertainty by half'. Other units such as the nat may be used. For example, the information encoded in one ""fair"" coin flip is log2(2/1) = 1 bit, and in two fair coin flips is log2(4/1) = 2 bits. A 2011 Science article estimated that 97% of technologically stored information was already in digital bits in 2007, and that the year 2002 was the beginning of the digital age for information storage (with digital storage capacity bypassing analog for the first time).

",Information,1,0
283,14773,"Information theory is the scientific study of the quantification, storage, and communication of digital information. The field was fundamentally established by the works of Harry Nyquist and Ralph Hartley, in the 1920s, and Claude Shannon in the 1940s.: vii  The field is at the intersection of probability theory, statistics, computer science, statistical mechanics, information engineering, and electrical engineering.
A key measure in information theory is entropy. Entropy quantifies the amount of uncertainty involved in the value of a random variable or the outcome of a random process. For example, identifying the outcome of a fair coin flip (with two equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (with six equally likely outcomes). Some other important measures in information theory are mutual information, channel capacity, error exponents, and relative entropy. Important sub-fields of information theory include source coding, algorithmic complexity theory, algorithmic information theory and information-theoretic security.
Applications of fundamental topics of information theory include source coding/data compression (e.g. for ZIP files), and channel coding/error detection and correction (e.g. for DSL). Its impact has been crucial to the success of the Voyager missions to deep space, the invention of the compact disc, the feasibility of mobile phones and the development of the Internet. The theory has also found applications in other areas, including statistical inference, cryptography, neurobiology, perception, linguistics, the evolution and function of molecular codes (bioinformatics), thermal physics, molecular dynamics, quantum computing, black holes, information retrieval, intelligence gathering, plagiarism detection, pattern recognition, anomaly detection and even art creation.",Information theory,1,1
284,1514405,"In statistics, econometrics, epidemiology and related disciplines, the method of instrumental variables (IV) is used to estimate causal relationships when controlled experiments are not feasible or when a treatment is not successfully delivered to every unit in a randomized experiment. Intuitively, IVs are used when an explanatory variable of interest is correlated with the error term, in which case ordinary least squares and ANOVA give biased results. A valid instrument induces changes in the explanatory variable but has no independent effect on the dependent variable, allowing a researcher to uncover the causal effect of the explanatory variable on the dependent variable.
Instrumental variable methods allow for consistent estimation when the explanatory variables (covariates) are correlated with the error terms in a regression model. Such correlation may occur when:

changes in the dependent variable change the value of at least one of the covariates (""reverse"" causation),
there are omitted variables that affect both the dependent and independent variables, or
the covariates are subject to non-random measurement error.Explanatory variables that suffer from one or more of these issues in the context of a regression are sometimes referred to as endogenous. In this situation, ordinary least squares produces biased and inconsistent estimates. However, if an instrument is available, consistent estimates may still be obtained. An instrument is a variable that does not itself belong in the explanatory equation but is correlated with the endogenous explanatory variables, conditionally on the value of other covariates.
In linear models, there are two main requirements for using IVs:

The instrument must be correlated with the endogenous explanatory variables, conditionally on the other covariates. If this correlation is strong, then the instrument is said to have a strong first stage. A weak correlation may provide misleading inferences about parameter estimates and standard errors.
The instrument cannot be correlated with the error term in the explanatory equation, conditionally on the other covariates. In other words, the instrument cannot suffer from the same problem as the original predicting variable. If this condition is met, then the instrument is said to satisfy the exclusion restriction.",Instrumental variables estimation,1,0
285,14563,"An integer (from the Latin integer meaning ""whole"") is colloquially defined as a number that can be written without a fractional component. For example, 21, 4, 0, and −2048 are integers, while 9.75, 5+1/2, and √2 are not.
The set of integers consists of zero (0), the positive natural numbers (1, 2, 3, ...), also called whole numbers or counting numbers, and their additive inverses (the negative integers, i.e., −1, −2, −3, ...). The set of integers is often denoted by the boldface (Z) or blackboard bold 
  
    
      
        (
        
          Z
        
        )
      
    
    {\displaystyle (\mathbb {Z} )}
   letter ""Z""—standing originally for the German word Zahlen (""numbers"").
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   is a subset of the set of all rational numbers 
  
    
      
        
          Q
        
      
    
    {\displaystyle \mathbb {Q} }
  , which in turn is a subset of the real numbers 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
  .  Like the natural numbers, 
  
    
      
        
          Z
        
      
    
    {\displaystyle \mathbb {Z} }
   is countably infinite.
The integers form the smallest group and the smallest ring containing the natural numbers. In algebraic number theory, the integers are sometimes qualified as rational integers to distinguish them from the more general algebraic integers. In fact, (rational) integers are algebraic integers that are also rational numbers.

",Integer,1,0
286,14794,"In computer science, an integer is a datum of integral data type, a data type that represents some range of mathematical integers. Integral data types may be of different sizes and may or may not be allowed to contain negative values. Integers are commonly represented in a computer as a group of binary digits (bits). The size of the grouping varies so the set of integer sizes available varies between different types of computers. Computer hardware nearly always provides a way to represent a processor register or memory address as an integer.",Integer (computer science),1,0
287,443083,"In statistics, an interaction may arise when considering the relationship among three or more variables, and describes a situation in which the effect of one causal variable on an outcome depends on the state of a second causal variable (that is, when effects of the two causes are not additive). Although commonly thought of in terms of causal relationships, the concept of an interaction can also describe non-causal associations. Interactions are often considered in the context of regression analyses or factorial experiments.
The presence of interactions can have important implications for the interpretation of statistical models.  If two variables of interest interact, the relationship between each of the interacting variables and a third ""dependent variable"" depends on the value of the other interacting variable.  In practice, this makes it more difficult to predict the consequences of changing the value of a variable, particularly if the variables it interacts with are hard to measure or difficult to control.
The notion of ""interaction"" is closely related to that of moderation that is common in social and health science research: the interaction between an explanatory variable and an environmental variable suggests that the effect of the explanatory variable has been moderated or modified by the environmental variable.",Interaction (statistics),1,0
288,11133686,"The International Statistical Institute (ISI) is a professional association of statisticians. It was founded in 1885, although there had been international statistical congresses since 1853. The institute has about 4,000 elected members from government, academia, and the private sector. The affiliated Associations have membership open to any professional statistician. The institute publishes a variety of books and journals, and holds an international conference every two years. The biennial convention was commonly known as the ISI Session; however, since 2011, it is now referred to as the ISI World Statistics Congress. The permanent office of the institute is located in the Statistics Netherlands building in Leidschenveen (The Hague), in the Netherlands.",International Statistical Institute,1,0
289,14569,"In the mathematical field of numerical analysis, interpolation is a type of estimation, a method of constructing (finding) new data points based on the range of a discrete set of known data points.In engineering and science, one often has a number of data points, obtained by sampling or experimentation, which represent the values of a function for a limited number of values of the independent variable. It is often required to interpolate; that is, estimate the value of that function for an intermediate value of the independent variable.
A closely related problem is the approximation of a complicated function by a simple function. Suppose the formula for some given function is known, but too complicated to evaluate efficiently. A few data points from the original function can be interpolated to produce a simpler function which is still fairly close to the original. The resulting gain in simplicity may outweigh the loss from interpolation error and give better performance in calculation process.",Interpolation,1,0
290,14812,"In descriptive statistics, the interquartile range (IQR) is a measure of statistical dispersion, which is the spread of the data. The IQR may also be called the midspread, middle 50%, fourth spread, or H‑spread. It is defined as the difference between the 75th and 25th percentiles of the data. To calculate the IQR, the data set is divided into quartiles, or four rank-ordered even parts via linear interpolation. These quartiles are denoted by Q1 (also called the lower quartile), Q2 (the median), and Q3 (also called the upper quartile). The lower quartile corresponds with the 25th percentile and the upper quartile corresponds with the 75th percentile, so IQR = Q3 −  Q1.The IQR is an example of a trimmed estimator, defined as the 25% trimmed range, which enhances the accuracy of dataset statistics by dropping lower contribution, outlying points. It is also used as a robust measure of scale It can be clearly visualized by the box on a Box plot.",Interquartile range,1,0
291,160381,"In statistics, interval estimation is the use of sample data to estimate an interval of plausible values of a parameter of interest. This is in contrast to point estimation, which gives a single value.The most prevalent forms of interval estimation are confidence intervals (a frequentist method) and  credible intervals (a Bayesian method);
less common forms include likelihood intervals and fiducial intervals.
Other forms of statistical intervals include tolerance intervals (covering a proportion of a sampled population) and  prediction intervals (an estimate of a future observation, used mainly in regression analysis).
Non-statistical methods that can lead to interval estimates include fuzzy logic.",Interval estimation,1,0
292,10477224,"The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis. It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species. Two of the three species were collected in the Gaspé Peninsula ""all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus"".The data set consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Based on the combination of these four features, Fisher developed a linear discriminant model to distinguish the species from each other. 
Fisher's paper was published in the Annals of Eugenics and includes discussion of the contained techniques' applications to the field of phrenology. This history has led some to suggest discontinuing use of the Iris dataset for teaching statistical techniques today and replacing it with less-controversial alternatives.",Iris flower data set,1,0
293,2836143,"In statistics and numerical analysis, isotonic regression or monotonic regression is the technique of fitting a free-form line to a sequence of observations such that the fitted line is non-decreasing (or non-increasing) everywhere, and lies as close to the observations as possible.",Isotonic regression,1,0
294,216262,"JSTOR (; short for Journal Storage) is a digital library founded in 1995 in New York City. Originally containing digitized back issues of academic journals, it now encompasses books and other primary sources as well as current issues of journals in the humanities and social sciences. It provides full-text searches of almost 2,000 journals.
As of 2013, more than 8,000 institutions in more than 160 countries had access to JSTOR. Most access is by subscription but some of the site is public domain, and open access content is available free of charge.JSTOR's revenue was $86 million in 2015.

",JSTOR,1,0
295,9158134,"In statistics, the jackknife is a resampling technique that is especially useful for bias and variance estimation. The jackknife pre-dates other common resampling methods such as the bootstrap. Given a sample of size 
  
    
      
        n
      
    
    {\displaystyle n}
  , a jackknife estimator can be built by aggregating the parameter estimates from each subsample of size 
  
    
      
        (
        n
        −
        1
        )
      
    
    {\displaystyle (n-1)}
   obtained by omitting one observation.The jackknife technique was developed by Maurice Quenouille (1924–1973) from 1949 and refined in 1956. John Tukey expanded on the technique in 1958 and proposed the name ""jackknife"" because, like a physical jack-knife (a compact folding knife), it is a rough-and-ready tool that can improvise a solution for a variety of problems even though specific problems may be more efficiently solved with a purpose-designed tool.The jackknife is a linear approximation of the bootstrap.",Jackknife resampling,1,0
296,16307,"Paul Jackson Pollock (; January 28, 1912 – August 11, 1956) was an American painter and a major figure in the abstract expressionist movement. He was widely noticed for his ""drip technique"" of pouring or splashing liquid household paint onto a horizontal surface, enabling him to view and paint his canvases from all angles. It was also called all-over painting and action painting, since he covered the entire canvas and used the force of his whole body to paint, often in a frenetic dancing style. This extreme form of abstraction divided the critics: some praised the immediacy of the creation, while others derided the random effects. In 2016, Pollock's painting titled Number 17A was reported to have fetched US$200 million in a private purchase.
A reclusive and volatile personality, Pollock struggled with alcoholism for most of his life. In 1945, he married the artist Lee Krasner, who became an important influence on his career and on his legacy. Pollock died at the age of 44 in an alcohol-related single-car accident when he was driving. In December 1956, four months after his death, Pollock was given a memorial retrospective exhibition at the Museum of Modern Art (MoMA) in New York City. A larger, more comprehensive exhibition of his work was held there in 1967. In 1998 and 1999, his work was honored with large-scale retrospective exhibitions at MoMA and at The Tate in London.",Jackson Pollock,1,0
297,30865916,"In statistics, the Jarque–Bera test is a goodness-of-fit test of whether sample data have the skewness and kurtosis matching a normal distribution. The test is named after Carlos Jarque and Anil K. Bera.
The test statistic is always nonnegative. If it is far from zero, it signals the data do not have a normal distribution.
The test statistic JB is defined as

  
    
      
        
          
            J
            B
          
        
        =
        
          
            n
            6
          
        
        
          (
          
            
              S
              
                2
              
            
            +
            
              
                1
                4
              
            
            (
            K
            −
            3
            
              )
              
                2
              
            
          
          )
        
      
    
    {\displaystyle {\mathit {JB}}={\frac {n}{6}}\left(S^{2}+{\frac {1}{4}}(K-3)^{2}\right)}
  where n is the number of observations (or degrees of freedom in general); S is the sample skewness, K is the sample kurtosis :

  
    
      
        S
        =
        
          
            
              
                
                  
                    μ
                    ^
                  
                
              
              
                3
              
            
            
              
                
                  
                    σ
                    ^
                  
                
              
              
                3
              
            
          
        
        =
        
          
            
              
                
                  1
                  n
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              (
              
                x
                
                  i
                
              
              −
              
                
                  
                    x
                    ¯
                  
                
              
              
                )
                
                  3
                
              
            
            
              
                (
                
                  
                    
                      1
                      n
                    
                  
                  
                    ∑
                    
                      i
                      =
                      1
                    
                    
                      n
                    
                  
                  (
                  
                    x
                    
                      i
                    
                  
                  −
                  
                    
                      
                        x
                        ¯
                      
                    
                  
                  
                    )
                    
                      2
                    
                  
                
                )
              
              
                3
                
                  /
                
                2
              
            
          
        
        ,
      
    
    {\displaystyle S={\frac {{\hat {\mu }}_{3}}{{\hat {\sigma }}^{3}}}={\frac {{\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{3}}{\left({\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}\right)^{3/2}}},}
  

  
    
      
        K
        =
        
          
            
              
                
                  
                    μ
                    ^
                  
                
              
              
                4
              
            
            
              
                
                  
                    σ
                    ^
                  
                
              
              
                4
              
            
          
        
        =
        
          
            
              
                
                  1
                  n
                
              
              
                ∑
                
                  i
                  =
                  1
                
                
                  n
                
              
              (
              
                x
                
                  i
                
              
              −
              
                
                  
                    x
                    ¯
                  
                
              
              
                )
                
                  4
                
              
            
            
              
                (
                
                  
                    
                      1
                      n
                    
                  
                  
                    ∑
                    
                      i
                      =
                      1
                    
                    
                      n
                    
                  
                  (
                  
                    x
                    
                      i
                    
                  
                  −
                  
                    
                      
                        x
                        ¯
                      
                    
                  
                  
                    )
                    
                      2
                    
                  
                
                )
              
              
                2
              
            
          
        
        ,
      
    
    {\displaystyle K={\frac {{\hat {\mu }}_{4}}{{\hat {\sigma }}^{4}}}={\frac {{\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{4}}{\left({\frac {1}{n}}\sum _{i=1}^{n}(x_{i}-{\bar {x}})^{2}\right)^{2}}},}
  where 
  
    
      
        
          
            
              
                μ
                ^
              
            
          
          
            3
          
        
      
    
    {\displaystyle {\hat {\mu }}_{3}}
   and 
  
    
      
        
          
            
              
                μ
                ^
              
            
          
          
            4
          
        
      
    
    {\displaystyle {\hat {\mu }}_{4}}
   are the estimates of third and fourth central moments, respectively, 
  
    
      
        
          
            
              x
              ¯
            
          
        
      
    
    {\displaystyle {\bar {x}}}
   is the sample mean, and

  
    
      
        
          
            
              
                σ
                ^
              
            
          
          
            2
          
        
      
    
    {\displaystyle {\hat {\sigma }}^{2}}
   is the estimate of the second central moment, the variance.
If the data comes from a normal distribution, the JB statistic asymptotically has a chi-squared distribution with two degrees of freedom, so the statistic can be used to test the hypothesis that the data are from a normal distribution. The null hypothesis is a joint hypothesis of the skewness being zero and the excess kurtosis being zero. Samples from a normal distribution have an expected skewness of 0 and an expected excess kurtosis of 0 (which is the same as a kurtosis of 3). As the definition of JB shows, any deviation from this increases the JB statistic.
For small samples the chi-squared approximation is overly sensitive, often rejecting the null hypothesis when it is true. Furthermore, the distribution of p-values departs from a uniform distribution and becomes a right-skewed unimodal distribution, especially for small p-values. This leads to a large Type I error rate. The table below shows some p-values approximated by a chi-squared distribution that differ from their true alpha levels for small samples.

(These values have been approximated using Monte Carlo simulation in Matlab)
In MATLAB's implementation, the chi-squared approximation for the JB statistic's distribution is only used for large sample sizes (> 2000). For smaller samples, it uses a table derived from Monte Carlo simulations in order to interpolate p-values.",Jarque–Bera test,1,0
298,2386106,"Jerzy Neyman (April 16, 1894 – August 5, 1981; born Jerzy Spława-Neyman; Polish: [ˈjɛʐɨ ˈspwava ˈnɛjman]) was a Polish mathematician and statistician who spent the first part of his professional career at various institutions in Warsaw, Poland and then at University College London, and the second part at the University of California, Berkeley. Neyman first introduced the modern concept of a confidence interval into statistical hypothesis testing and co-revised Ronald Fisher's null hypothesis testing (in collaboration with Egon Pearson).",Jerzy Neyman,1,0
299,24339475,"In statistics, the Johansen test, named after Søren Johansen, is a procedure for testing cointegration of several, say k, I(1) time series.  This test permits more than one cointegrating relationship so is more generally applicable than the Engle–Granger test which is based on the Dickey–Fuller (or the augmented) test for unit roots in the residuals from a single (estimated) cointegrating relationship.There are two types of Johansen test, either with trace or with eigenvalue, and the inferences might be a little bit different. The null hypothesis for the trace test is that the number of cointegration vectors is r = r* < k, vs. the alternative that r = k. Testing proceeds sequentially for r* = 1,2, etc. and the first non-rejection of the null is taken as an estimate of r. The null hypothesis for the ""maximum eigenvalue"" test is as for the trace test but the alternative is r = r* + 1 and, again, testing proceeds sequentially for r* = 1,2,etc., with the first non-rejection used as an estimator for r.
Just like a unit root test, there can be a constant term, a trend term, both, or neither in the model.  For a general VAR(p) model:

  
    
      
        
          X
          
            t
          
        
        =
        μ
        +
        Φ
        
          D
          
            t
          
        
        +
        
          Π
          
            p
          
        
        
          X
          
            t
            −
            p
          
        
        +
        ⋯
        +
        
          Π
          
            1
          
        
        
          X
          
            t
            −
            1
          
        
        +
        
          e
          
            t
          
        
        ,
        
        t
        =
        1
        ,
        …
        ,
        T
      
    
    {\displaystyle X_{t}=\mu +\Phi D_{t}+\Pi _{p}X_{t-p}+\cdots +\Pi _{1}X_{t-1}+e_{t},\quad t=1,\dots ,T}
  There are two possible specifications for error correction: that is, two vector error correction models (VECM):
1. The longrun VECM:

  
    
      
        Δ
        
          X
          
            t
          
        
        =
        μ
        +
        Φ
        
          D
          
            t
          
        
        +
        Π
        
          X
          
            t
            −
            p
          
        
        +
        
          Γ
          
            p
            −
            1
          
        
        Δ
        
          X
          
            t
            −
            p
            +
            1
          
        
        +
        ⋯
        +
        
          Γ
          
            1
          
        
        Δ
        
          X
          
            t
            −
            1
          
        
        +
        
          ε
          
            t
          
        
        ,
        
        t
        =
        1
        ,
        …
        ,
        T
      
    
    {\displaystyle \Delta X_{t}=\mu +\Phi D_{t}+\Pi X_{t-p}+\Gamma _{p-1}\Delta X_{t-p+1}+\cdots +\Gamma _{1}\Delta X_{t-1}+\varepsilon _{t},\quad t=1,\dots ,T}
  
where

  
    
      
        
          Γ
          
            i
          
        
        =
        
          Π
          
            1
          
        
        +
        ⋯
        +
        
          Π
          
            i
          
        
        −
        I
        ,
        
        i
        =
        1
        ,
        …
        ,
        p
        −
        1.
        
      
    
    {\displaystyle \Gamma _{i}=\Pi _{1}+\cdots +\Pi _{i}-I,\quad i=1,\dots ,p-1.\,}
  2. The transitory VECM:

  
    
      
        Δ
        
          X
          
            t
          
        
        =
        μ
        +
        Φ
        
          D
          
            t
          
        
        −
        
          Γ
          
            p
            −
            1
          
        
        Δ
        
          X
          
            t
            −
            p
            +
            1
          
        
        −
        ⋯
        −
        
          Γ
          
            1
          
        
        Δ
        
          X
          
            t
            −
            1
          
        
        +
        Π
        
          X
          
            t
            −
            1
          
        
        +
        
          ε
          
            t
          
        
        ,
        
        t
        =
        1
        ,
        ⋯
        ,
        T
      
    
    {\displaystyle \Delta X_{t}=\mu +\Phi D_{t}-\Gamma _{p-1}\Delta X_{t-p+1}-\cdots -\Gamma _{1}\Delta X_{t-1}+\Pi X_{t-1}+\varepsilon _{t},\quad t=1,\cdots ,T}
  
where

  
    
      
        
          Γ
          
            i
          
        
        =
        
          (
          
            
              Π
              
                i
                +
                1
              
            
            +
            ⋯
            +
            
              Π
              
                p
              
            
          
          )
        
        ,
        
        i
        =
        1
        ,
        …
        ,
        p
        −
        1.
        
      
    
    {\displaystyle \Gamma _{i}=\left(\Pi _{i+1}+\cdots +\Pi _{p}\right),\quad i=1,\dots ,p-1.\,}
  Be aware that the two are the same. In both VECM,

  
    
      
        Π
        =
        
          Π
          
            1
          
        
        +
        ⋯
        +
        
          Π
          
            p
          
        
        −
        I
        .
        
      
    
    {\displaystyle \Pi =\Pi _{1}+\cdots +\Pi _{p}-I.\,}
  Inferences are drawn on Π, and they will be the same, so is the explanatory power.",Johansen test,1,0
300,46351481,"John Ernst Freund (August 6, 1921 – August 14, 2004) was a prominent author of university level textbooks on statistics and a mathematics professor at Arizona State University. Born in Berlin, Germany, he emigrated to Mandatory Palestine in the 1930s. He studied at the University of London and at the University of California at Los Angeles, from which he received his bachelor's degree. He did graduate work at Columbia University and the University of Pittsburgh, from which he received his doctorate in 1952.In 1960 he was elected as a Fellow of the American Statistical Association.",John E. Freund,1,0
301,30875381,"John Graunt (24 April 1620 – 18 April 1674) has been regarded as the founder of demography. Graunt was one of the first demographers, and perhaps the first epidemiologist, though by profession he was a haberdasher. He was bankrupted later in life by losses suffered during Great Fire of London and the discrimination he faced following his conversion to Catholicism.",John Graunt,1,1
302,5865261,"John Ashworth Nelder  (8 October 1924 – 7 August 2010) was a British statistician known for his contributions to experimental design, analysis of variance, computational statistics, and statistical theory.",John Nelder,1,0
303,20900378,"John P. A. Ioannidis (; Greek: Ιωάννης Ιωαννίδης, Greek pronunciation: [iɔˈanis iɔaˈniðis]; born August 21, 1965) is a Greek-American physician-scientist, writer and Stanford University professor who has made contributions to evidence-based medicine, epidemiology, and clinical research. Ioannidis studies scientific research itself, meta-research primarily in clinical medicine and the social sciences. He has served on the editorial board of over twenty scientific journals.
Ioannidis's 2005 essay ""Why Most Published Research Findings Are False"" was the most-accessed article in the history of Public Library of Science (PLOS) as of 2020, with more than three million views.Ioannidis has been a prominent opponent of lockdowns during the COVID-19 pandemic.",John Ioannidis,1,0
304,415974,"John Wilder Tukey (; June 16, 1915 – July 26, 2000) was an American mathematician and statistician, best known for the development of the fast Fourier Transform (FFT) algorithm and box plot. The Tukey range test, the Tukey lambda distribution, the Tukey test of additivity, and the Teichmüller–Tukey lemma all bear his name. He is also credited with coining the term 'bit' and the first published use of the word software.",John Tukey,1,0
305,35615250,"In statistics, the Jonckheere trend test (sometimes called the Jonckheere–Terpstra test) is a test for an ordered alternative hypothesis within an independent samples (between-participants) design. It is similar to the Kruskal–Wallis test in that the null hypothesis is that several independent samples are from the same population. However, with the Kruskal–Wallis test there is no a priori ordering of the populations from which the samples are drawn. When there is an a priori ordering, the Jonckheere test has more statistical power than the Kruskal–Wallis test. The test was developed by Aimable Robert Jonckheere, who was a psychologist and statistician at University College London.
The null and alternative hypotheses can be conveniently expressed in terms of population medians for k populations (where k > 2). Letting θi be the population median for the ith population, the null hypothesis is:

  
    
      
        
          H
          
            0
          
        
        :
        
          θ
          
            1
          
        
        =
        
          θ
          
            2
          
        
        =
        ⋯
        =
        
          θ
          
            k
          
        
      
    
    {\displaystyle H_{0}:\theta _{1}=\theta _{2}=\cdots =\theta _{k}}
  The alternative hypothesis is that the population medians have an a priori ordering e.g.:

  
    
      
        
          H
          
            A
          
        
        :
        
          θ
          
            1
          
        
      
    
    {\displaystyle H_{A}:\theta _{1}}
   ≤ 
  
    
      
        
          θ
          
            2
          
        
      
    
    {\displaystyle \theta _{2}}
   ≤ 
  
    
      
        ⋯
      
    
    {\displaystyle \cdots }
   ≤ 
  
    
      
        
          θ
          
            k
          
        
      
    
    {\displaystyle \theta _{k}}
  with at least one strict inequality.",Jonckheere's trend test,1,0
306,13327143,"The Journal of the Royal Statistical Society is a peer-reviewed scientific journal of statistics. It comprises three series and is published by Wiley for the Royal Statistical Society.

",Journal of the Royal Statistical Society,1,0
307,2492557,"Juan Caramuel y Lobkowitz (Juan Caramuel de Lobkowitz, 23 May 1606 in Madrid — 7 or 8 September 1682 in Vigevano) was a Spanish Catholic scholastic philosopher, ecclesiastic, mathematician and writer. He is believed to be a great-grandson of Jan Popel z Lobkowicz.",Juan Caramuel y Lobkowitz,1,0
308,45327439,"Jurimetrics is the application of quantitative methods, and often especially probability and statistics, to law. In the United States, the journal Jurimetrics is published by the American Bar Association and Arizona State University. The Journal of Empirical Legal Studies is another publication that emphasizes the statistical analysis of law.
The term was coined in 1949 by Lee Loevinger in his article ""Jurimetrics: The Next Step Forward"". Showing the influence of Oliver Wendell Holmes, Jr., Loevinger quoted Holmes' celebrated phrase that:

“For the rational study of the law the blackletter man may be the man of the present, but the man of the future is the man of statistics and the master of economics.”
The first work on this topic is attributed to Nicolaus I Bernoulli in his doctoral dissertation De Usu Artis Conjectandi in Jure, written in 1709.",Jurimetrics,1,0
309,3168650,"The Kaplan–Meier estimator, also known as the product limit estimator, is a non-parametric statistic used to estimate the survival function from lifetime data.  In medical research, it is often used to measure the fraction of patients living for a certain amount of time after treatment. In other fields, Kaplan–Meier estimators may be used to measure the length of time people remain unemployed after a job loss, the time-to-failure of machine parts, or how long fleshy fruits remain on plants before they are removed by frugivores. The estimator is named after Edward L. Kaplan and Paul Meier, who each submitted similar manuscripts to the Journal of the American Statistical Association. The journal editor, John Tukey, convinced them to combine their work into one paper, which has been cited almost 61,000 times since its publication in 1958.The estimator of the survival function 
  
    
      
        S
        (
        t
        )
      
    
    {\displaystyle S(t)}
   (the probability that life is longer than 
  
    
      
        t
      
    
    {\displaystyle t}
  ) is given by:

  
    
      
        
          
            
              S
              ^
            
          
        
        (
        t
        )
        =
        
          ∏
          
            i
            :
             
            
              t
              
                i
              
            
            ≤
            t
          
        
        
          (
          
            1
            −
            
              
                
                  d
                  
                    i
                  
                
                
                  n
                  
                    i
                  
                
              
            
          
          )
        
        ,
      
    
    {\displaystyle {\widehat {S}}(t)=\prod \limits _{i:\ t_{i}\leq t}\left(1-{\frac {d_{i}}{n_{i}}}\right),}
  with 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
   a time when at least one event happened, di the number of events (e.g., deaths) that happened at time 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  , and 
  
    
      
        
          n
          
            i
          
        
      
    
    {\displaystyle n_{i}}
   the individuals known to have survived (have not yet had an event or been censored) up to time 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  .

",Kaplan–Meier estimator,1,0
310,164695,"Karl Pearson  (; born Carl Pearson; 27 March 1857 – 27 April 1936) was an English mathematician and biostatistician. He has been credited with establishing the discipline of mathematical statistics. He founded the world's first university statistics department at University College, London in 1911, and contributed significantly to the field of biometrics and meteorology. Pearson was also a proponent of social Darwinism, eugenics and scientific racism. Pearson was a protégé and biographer of Sir Francis Galton. He edited and completed both William Kingdon Clifford's Common Sense of the Exact Sciences (1885) and Isaac Todhunter's History of the Theory of Elasticity, Vol. 1 (1886–1893) and Vol. 2 (1893), following their deaths.",Karl Pearson,1,0
311,7287830,"In statistics, the Kendall rank correlation coefficient, commonly referred to as Kendall's τ coefficient (after the Greek letter τ, tau), is a statistic used to measure the ordinal association between two measured quantities. A τ test is a non-parametric hypothesis test for statistical dependence based on the τ coefficient.
It is a measure of rank correlation: the similarity of the orderings of the data when ranked by each of the quantities. It is named after Maurice Kendall, who developed it in 1938, though Gustav Fechner had proposed a similar measure in the context of time series in 1897.Intuitively, the Kendall correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully different for a correlation of −1) rank between the two variables.
Both Kendall's 
  
    
      
        τ
      
    
    {\displaystyle \tau }
   and Spearman's 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   can be formulated as special cases of a more general correlation coefficient.",Kendall rank correlation coefficient,1,0
312,16714,"In statistics, the Kolmogorov–Smirnov test (K-S test or KS test) is a nonparametric test of the equality of continuous (or discontinuous, see Section 2.2), one-dimensional probability distributions that can be used to compare a sample with a reference probability distribution (one-sample K–S test), or to compare two samples (two-sample K–S test).   In essence, the test answers the question ""What is the probability that this collection of samples could have been drawn from that probability distribution?"" or, in the second case, ""What is the probability that these two sets of samples were drawn from the same (but unknown) probability distribution?"".
It is named after  Andrey Kolmogorov and Nikolai Smirnov.
The Kolmogorov–Smirnov statistic quantifies a distance between the empirical distribution function of the sample and the cumulative distribution function of the reference distribution, or between the empirical distribution functions of two samples. The null distribution of this statistic is calculated under the null hypothesis that the sample is drawn from the reference distribution (in the one-sample case) or that the samples are drawn from the same distribution (in the two-sample case). In the one-sample case, the distribution considered under the null hypothesis may be continuous (see Section 2), purely discrete or mixed (see Section 2.2). In the two-sample case (see Section 3), the distribution considered under the null hypothesis is a continuous distribution but is otherwise unrestricted. However, the two sample test can also be performed under more general conditions that allow for discontinuity, heterogeneity and dependence across samples.The two-sample K–S test is one of the most useful and general nonparametric methods for comparing two samples, as it is sensitive to differences in both location and shape of the empirical cumulative distribution functions of the two samples.
The Kolmogorov–Smirnov test can be modified to serve as a goodness of fit test. In the special case of testing for normality of the distribution, samples are standardized and compared with a standard normal distribution. This is equivalent to setting the mean and variance of the reference distribution equal to the sample estimates, and it is known that using these to define the specific reference distribution changes the null distribution of the test statistic (see Test with estimated parameters). Various studies have found that, even in this corrected form, the test is less powerful for testing normality than the Shapiro–Wilk test or Anderson–Darling test. However, these other tests have their own disadvantages. For instance the Shapiro–Wilk test is known not to work well in samples with many identical values.

",Kolmogorov–Smirnov test,1,0
313,477026,"In statistics, originally in geostatistics, kriging or Kriging, also known as Gaussian process regression, is a method of interpolation based on Gaussian process governed by prior covariances.  Under suitable assumptions of the prior, kriging gives the best linear unbiased prediction (BLUP) at unsampled locations.  Interpolating methods based on other criteria such as smoothness (e.g., smoothing spline) may not yield the BLUP. The method is widely used in the domain of spatial analysis and computer experiments. The technique is also known as Wiener–Kolmogorov prediction, after Norbert Wiener and Andrey Kolmogorov.
The theoretical basis for the method was developed by the French mathematician Georges Matheron in 1960, based on the master's thesis of Danie G. Krige, the pioneering plotter of distance-weighted average gold grades at the Witwatersrand reef complex in South Africa. Krige sought to estimate the most likely distribution of gold based on samples from a few boreholes.  The English verb is to krige, and the most common noun is kriging; both are often pronounced with a hard ""g"", following an Anglicized pronunciation of the name ""Krige"". The word is sometimes capitalized as Kriging in the literature.
Though computationally intensive in its basic formulation, kriging can be scaled to larger problems using various approximation methods.",Kriging,1,0
314,1198757,"The Kruskal–Wallis test by ranks, Kruskal–Wallis H test (named after William Kruskal and W. Allen Wallis), or one-way ANOVA on ranks is a non-parametric method for testing whether samples originate from the same distribution. It is used for comparing two or more independent samples of equal or different sample sizes. It extends the Mann–Whitney U test, which is used for comparing only two groups. The parametric equivalent of the Kruskal–Wallis test is the one-way analysis of variance (ANOVA).
A significant Kruskal–Wallis test indicates that at least one sample stochastically dominates one other sample. The test does not identify where this stochastic dominance occurs or for how many pairs of groups stochastic dominance obtains. For analyzing the specific sample pairs for stochastic dominance, Dunn's test, pairwise Mann–Whitney tests with Bonferroni correction, or the more powerful but less well known Conover–Iman test are sometimes used.
Since it is a nonparametric method, the Kruskal–Wallis test does not assume a normal distribution of the residuals, unlike the analogous one-way analysis of variance. If the researcher can make the assumptions of an identically shaped and scaled distribution for all groups, except for any difference in medians, then the null hypothesis is that the medians of all groups are equal, and the alternative hypothesis is that at least one population median of one group is different from the population median of at least one other group. Otherwise, it is impossible to say, whether the rejection of the null hypothesis comes from the shift in locations or group dispersions. This is the same issue that happens also with the Mann-Whitney test.

",Kruskal–Wallis one-way analysis of variance,1,1
316,16848,"In probability theory and statistics, kurtosis (from Greek: κυρτός, kyrtos or kurtos, meaning ""curved, arching"") is a measure of the ""tailedness"" of the probability distribution of a real-valued random variable. Like skewness, kurtosis describes the shape of a probability distribution and there are different ways of quantifying it for a theoretical distribution and corresponding ways of estimating it from a sample from a population. Different measures of kurtosis may have different interpretations.
The standard measure of a distribution's kurtosis, originating with Karl Pearson, is a scaled version of the fourth moment of the distribution. This number is related to the tails of the distribution, not its peak; hence, the sometimes-seen characterization of kurtosis as ""peakedness"" is incorrect. For this measure, higher kurtosis corresponds to greater extremity of deviations (or outliers), and not the configuration of data near the mean.
It is common to compare the excess kurtosis (defined below) of a distribution to 0, which is the excess kurtosis of any univariate normal distribution. Distributions with negative excess kurtosis are said to be platykurtic, although this does not imply the distribution is ""flat-topped"" as is sometimes stated. Rather, it means the distribution produces fewer and/or less extreme outliers than the normal distribution. An example of a platykurtic distribution is the uniform distribution, which does not produce outliers.  Distributions with a positive excess kurtosis are said to be leptokurtic.  An example of a leptokurtic distribution is the Laplace distribution, which has tails that asymptotically approach zero more slowly than a Gaussian, and therefore produces more outliers than the normal distribution. It is common practice to use excess kurtosis, which is defined as Pearson's kurtosis minus 3, to provide a simple comparison to the normal distribution.  Some authors and software packages use ""kurtosis"" by itself to refer to the excess kurtosis. For clarity and generality, however, this article explicitly indicates where non-excess kurtosis is meant.
Alternative measures of kurtosis are: the L-kurtosis, which is a scaled version of the fourth L-moment; measures based on four population or sample quantiles. These are analogous to the alternative measures of skewness that are not based on ordinary moments.",Kurtosis,1,0
317,24350828,"In statistics, L-moments are a sequence of statistics used to summarize the shape of a probability distribution. They are linear combinations of order statistics (L-statistics) analogous to conventional moments, and can be used to calculate quantities analogous to standard deviation, skewness and kurtosis, termed the L-scale, L-skewness and L-kurtosis respectively (the L-mean is identical to the conventional mean). Standardised L-moments are called L-moment ratios and are analogous to standardized moments. Just as for conventional moments, a theoretical distribution has a set of population L-moments. Sample L-moments can be defined for a sample from the population, and can be used as estimators of the population L-moments.

",L-moment,1,0
318,26297705,"In the design of experiments in statistics, the lady tasting tea is a randomized experiment devised by Ronald Fisher and reported in his book The Design of Experiments (1935). The experiment is the original exposition of Fisher's notion of a null hypothesis, which is ""never proved or established, but is possibly disproved, in the course of experimentation"".The lady in question (Muriel Bristol) claimed to be able to tell whether the tea or the milk was added first to a cup. Fisher proposed to give her eight cups, four of each variety, in random order. One could then ask what the probability was for her getting the specific number of cups she identified correct, but just by chance.
Fisher's description is less than 10 pages in length and is notable for its simplicity and completeness regarding terminology, calculations and design of the experiment. The example is loosely based on an event in Fisher's life. The test used was Fisher's exact test.

",Lady tasting tea,1,0
319,13609399,"Least-squares spectral analysis (LSSA) is a method of estimating a frequency spectrum, based on a least squares fit of sinusoids to data samples, similar to Fourier analysis. Fourier analysis, the most used spectral method in science, generally boosts long-periodic noise in long gapped records; LSSA mitigates such problems. Unlike with Fourier analysis, data need not be equally spaced to use LSSA.
LSSA is also known as the Vaníček method or the Gauss-Vaniček method after Petr Vaníček, and as the Lomb method or the Lomb–Scargle periodogram, based on the contributions of Nicholas R. Lomb and, independently, Jeffrey D. Scargle.

",Least-squares spectral analysis,1,0
320,19048902,"Least absolute deviations (LAD), also known as least absolute errors (LAE), least absolute value (LAV), least absolute residual (LAR), sum of absolute deviations, or the L1 norm condition, is a statistical optimality criterion and the statistical optimization technique that relies on it. Similar to the least squares technique, it attempts to find a function which closely approximates a set of data. In the simple case of a set of (x,y) data, the approximation function is a simple ""trend line"" in two-dimensional Cartesian coordinates. The method minimizes the sum of absolute errors (SAE) (the sum of the absolute values of the vertical ""residuals"" between points generated by the function and corresponding points in the data). The least absolute deviations estimate also arises as the maximum likelihood estimate if the errors have a Laplace distribution. It was introduced in 1757 by Roger Joseph Boscovich.",Least absolute deviations,1,0
321,82359,"The method of least squares is a standard approach in regression analysis to approximate the solution of overdetermined systems (sets of equations in which there are more equations than unknowns) by minimizing the sum of the squares of the residuals (a residual being the difference between an observed value and the fitted value provided by a model) made in the results of each individual equation.
The most important application is in data fitting. When the problem has substantial uncertainties in the independent variable (the x variable), then simple regression and least-squares methods have problems; in such cases, the methodology required for fitting errors-in-variables models may be considered instead of that for least squares.
Least-squares problems fall into two categories: linear or ordinary least squares and nonlinear least squares, depending on whether or not the residuals are linear in all unknowns. The linear least-squares problem occurs in statistical regression analysis; it has a closed-form solution. The nonlinear problem is usually solved by iterative refinement; at each iteration the system is approximated by a linear one, and thus the core calculation is similar in both cases.
Polynomial least squares describes the variance in a prediction of the dependent variable as a function of the independent variable and the deviations from the fitted curve.
When the observations come from an exponential family with identity as its natural sufficient statistics and mild-conditions are satisfied (e.g. for normal, exponential, Poisson and binomial distributions), standardized least-squares estimates and maximum-likelihood estimates are identical. The method of least squares can also be derived as a method of moments estimator.
The following discussion is mostly presented in terms of linear functions but the use of least squares is valid and practical for more general families of functions. Also, by iteratively applying local quadratic approximation to the likelihood (through the Fisher information), the least-squares method may be used to fit a generalized linear model.
The least-squares method was officially discovered and published by Adrien-Marie Legendre (1805), though it is usually also co-credited to Carl Friedrich Gauss (1795) who contributed significant theoretical advances to the method and may have previously used it in his work.

",Least squares,1,1
322,342602,"In statistics, the Lehmann–Scheffé theorem is a prominent statement, tying together the ideas of completeness, sufficiency, uniqueness, and best unbiased estimation. The theorem states that any estimator which is unbiased for a given unknown quantity and that depends on the data only through a complete, sufficient statistic is the unique best unbiased estimator of that quantity. The Lehmann–Scheffé theorem is named after Erich Leo Lehmann and Henry Scheffé, given their two early papers.If T is a complete sufficient statistic for θ and E(g(T)) = τ(θ) then g(T) is the uniformly minimum-variance unbiased estimator (UMVUE) of τ(θ).",Lehmann–Scheffé theorem,1,0
323,821148,"Level of measurement or scale of measure is a classification that describes the nature of information within the values assigned to variables. Psychologist Stanley Smith Stevens developed the best-known classification with four levels, or scales, of measurement: nominal, ordinal, interval, and ratio. This framework of distinguishing levels of measurement originated in psychology and is widely criticized by scholars in other disciplines. Other classifications include those by Mosteller and Tukey, and by Chrisman.",Level of measurement,1,1
325,302244,"""Lies, damned lies, and statistics"" is a phrase describing the persuasive power of statistics to bolster weak arguments. It is also sometimes colloquially used to doubt statistics used to prove an opponent's point.
The phrase was popularized in the United States by Mark Twain (among others), who attributed it to the British prime minister Benjamin Disraeli. However, the phrase is not found in any of Disraeli's works and the earliest known appearances were years after his death. Several other people have been listed as originators of the quote, and it is often attributed to Twain himself.

","Lies, damned lies, and statistics",1,0
326,45035,"In statistics, the likelihood-ratio test assesses the goodness of fit of two competing statistical models based on the ratio of their likelihoods, specifically one found by maximization over the entire parameter space and another found after imposing some constraint. If the constraint (i.e., the null hypothesis) is supported by the observed data, the two likelihoods should not differ by more than sampling error. Thus the likelihood-ratio test tests whether this ratio is significantly different from one, or equivalently whether its natural logarithm is significantly different from zero.
The likelihood-ratio test, also known as Wilks test, is the oldest of the three classical approaches to hypothesis testing, together with the Lagrange multiplier test and the Wald test. In fact, the latter two can be conceptualized as approximations to the likelihood-ratio test, and are asymptotically equivalent. In the case of comparing two models each of which has no unknown parameters, use of the likelihood-ratio test can be justified by the Neyman–Pearson lemma. The lemma demonstrates that the test has the highest power among all competitors.",Likelihood-ratio test,1,0
327,44968,"The likelihood function (often simply called the likelihood) describes the joint probability of the observed data as a function of the parameters of the chosen statistical model. For each specific parameter value 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   in the parameter space, the likelihood function 
  
    
      
        p
        (
        X
        
          |
        
        θ
        )
      
    
    {\displaystyle p(X|\theta )}
   therefore assigns a probabilistic prediction to the observed data 
  
    
      
        X
      
    
    {\displaystyle X}
  . Since it is essentially the product of sampling densities, the likelihood generally encapsulates both the data-generating process as well as the missing-data mechanism that produced the observed sample.
To emphasize that the likelihood is a function (although not a probability density function) of the parameters, while the sample is taken as given, it is often written as 
  
    
      
        
          
            L
          
        
        (
        θ
        ∣
        X
        )
      
    
    {\displaystyle {\mathcal {L}}(\theta \mid X)}
  . According to the likelihood principle, all of the information a given sample provides about 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   is expressed in the likelihood function. In maximum likelihood estimation, the value which maximizes the probability of observing the given sample, i.e. 
  
    
      
        
          
            
              θ
              ^
            
          
        
        =
        
          argmax
          
            θ
            ∈
            Θ
          
        
        ⁡
        
          
            L
          
        
        (
        θ
        ∣
        X
        )
      
    
    {\displaystyle {\hat {\theta }}=\operatorname {argmax} _{\theta \in \Theta }{\mathcal {L}}(\theta \mid X)}
  , serves as a point estimate for the parameter of the distribution from which the sample was drawn. Meanwhile in Bayesian statistics, the likelihood function serves as the conduit through which sample information influences 
  
    
      
        p
        (
        θ
        ∣
        X
        )
      
    
    {\displaystyle p(\theta \mid X)}
  , the posterior probability of the parameter, via Bayes' rule.",Likelihood function,1,0
328,55754796,"In statistics, suppose that we have been given some data, and we are selecting a statistical model for that data. The relative likelihood compares the relative plausibilities of different candidate models or of different values of a parameter of a single model.

",Relative likelihood,1,0
329,2690426,"In statistics, the Lilliefors test is a normality test based on the Kolmogorov–Smirnov test. It is used to test the null hypothesis that data come from a normally distributed population, when the null hypothesis does not specify which normal distribution; i.e., it does not specify the expected value and variance of the distribution. It is named after Hubert Lilliefors, professor of statistics at George Washington University.
A variant of the test can be used to test the null hypothesis that data come from an exponentially distributed population, when the null hypothesis does not specify which exponential distribution.",Lilliefors test,1,0
330,26551602,"In mathematics, a limit is the value that a function (or sequence) approaches as the input (or index) approaches some value. Limits are essential to calculus and mathematical analysis, and are used to define continuity, derivatives, and integrals.
The concept of a limit of a sequence is further generalized to the concept of a limit of a topological net, and is closely related to limit and direct limit in category theory.
In formulas, a limit of a function is usually written as

  
    
      
        
          lim
          
            x
            →
            c
          
        
        f
        (
        x
        )
        =
        L
        ,
      
    
    {\displaystyle \lim _{x\to c}f(x)=L,}
  (although a few authors may use ""Lt"" instead of ""lim"")
and is read as ""the limit of f of x as x approaches c equals L"". The fact that a function f approaches the limit L as x approaches c is sometimes denoted by a right arrow (→ or 
  
    
      
        →
      
    
    {\displaystyle \rightarrow }
  ), as in

  
    
      
        f
        (
        x
        )
        →
        L
        
           as 
        
        x
        →
        c
        ,
      
    
    {\displaystyle f(x)\to L{\text{ as }}x\to c,}
  which reads ""
  
    
      
        f
      
    
    {\displaystyle f}
   of 
  
    
      
        x
      
    
    {\displaystyle x}
   tends to 
  
    
      
        L
      
    
    {\displaystyle L}
   as 
  
    
      
        x
      
    
    {\displaystyle x}
   tends to 
  
    
      
        c
      
    
    {\displaystyle c}
  "".",Limit (mathematics),1,0
331,48758386,"In statistics, linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). The case of one explanatory variable is called simple linear regression; for more than one, the process is called multiple linear regression. This term is distinct from multivariate linear regression, where multiple correlated dependent variables are predicted, rather than a single scalar variable.In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models. Most commonly, the conditional mean of the response given the values of the explanatory variables (or predictors) is assumed to be an affine function of those values; less commonly, the conditional median or some other quantile is used. Like all forms of regression analysis, linear regression focuses on the conditional probability distribution of the response given the values of the predictors, rather than on the joint probability distribution of all of these variables, which is the domain of multivariate analysis.
Linear regression was the first type of regression analysis to be studied rigorously, and to be used extensively in practical applications. This is because models which depend linearly on their unknown parameters are easier to fit than models which are non-linearly related to their parameters and because the statistical properties of the resulting estimators are easier to determine.
Linear regression has many practical uses. Most applications fall into one of the following two broad categories:

If the goal is prediction, forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of values of the response and explanatory variables. After developing such a model, if additional values of the explanatory variables are collected without an accompanying response value, the fitted model can be used to make a prediction of the response.
If the goal is to explain variation in the response variable that can be attributed to variation in the explanatory variables, linear regression analysis can be applied to quantify the strength of the relationship between the response and the explanatory variables, and in particular to determine whether some explanatory variables may have no linear relationship with the response at all, or to identify which subsets of explanatory variables may contain redundant information about the response.Linear regression models are often fitted using the least squares approach, but they may also be fitted in other ways, such as by minimizing the ""lack of fit"" in some other norm (as with least absolute deviations regression), or by minimizing a penalized version of the least squares cost function as in ridge regression (L2-norm penalty) and lasso (L1-norm penalty). Conversely, the least squares approach can be used to fit models that are not linear models. Thus, although the terms ""least squares"" and ""linear model"" are closely linked, they are not synonymous.",Linear regression,1,1
332,18422,"Linear algebra is the branch of mathematics concerning linear equations such as: 

  
    
      
        
          a
          
            1
          
        
        
          x
          
            1
          
        
        +
        ⋯
        +
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        =
        b
        ,
      
    
    {\displaystyle a_{1}x_{1}+\cdots +a_{n}x_{n}=b,}
  linear maps such as:

  
    
      
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            n
          
        
        )
        ↦
        
          a
          
            1
          
        
        
          x
          
            1
          
        
        +
        ⋯
        +
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle (x_{1},\ldots ,x_{n})\mapsto a_{1}x_{1}+\cdots +a_{n}x_{n},}
  and their representations in vector spaces and through matrices.Linear algebra is central to almost all areas of mathematics. For instance, linear algebra is fundamental in modern presentations of geometry, including for defining basic objects such as lines, planes and rotations. Also, functional analysis, a branch of mathematical analysis, may be viewed as the application of linear algebra to spaces of functions.
Linear algebra is also used in most sciences and fields of engineering, because it allows modeling many natural phenomena, and computing efficiently with such models. For nonlinear systems, which cannot be modeled with linear algebra, it is often used for dealing with first-order approximations, using the fact that the differential of a multivariate function at a point is the linear map that best approximates the function near that point.",Linear algebra,1,0
333,1470657,"Linear discriminant analysis (LDA), normal discriminant analysis (NDA), or discriminant function analysis is a generalization of Fisher's linear discriminant, a method used in statistics and other fields, to find a linear combination of features that characterizes or separates two or more classes of objects or events. The resulting combination may be used as a linear classifier, or, more commonly, for dimensionality reduction before later classification.
LDA is closely related to analysis of variance (ANOVA) and regression analysis, which also attempt to express one dependent variable as a linear combination of other features or measurements. However, ANOVA uses categorical independent variables and a continuous dependent variable, whereas discriminant analysis has continuous independent variables and a categorical dependent variable (i.e. the class label). Logistic regression and probit regression are more similar to LDA than ANOVA is, as they also explain a categorical variable by the values of continuous independent variables. These other methods are preferable in applications where it is not reasonable to assume that the independent variables are normally distributed, which is a fundamental assumption of the LDA method.
LDA is also closely related to principal component analysis (PCA) and factor analysis in that they both look for linear combinations of variables which best explain the data. LDA explicitly attempts to model the difference between the classes of data. PCA, in contrast, does not take into account any difference in class, and factor analysis builds the feature combinations based on differences rather than similarities. Discriminant analysis is also different from factor analysis in that it is not an interdependence technique: a distinction between independent variables and dependent variables (also called criterion variables) must be made.
LDA works when the measurements made on independent variables for each observation are continuous quantities. When dealing with categorical independent variables, the equivalent technique is discriminant correspondence analysis.Discriminant analysis is used when groups are known a priori (unlike in cluster analysis). Each case must have a score on one or more quantitative predictor measures, and a score on a group measure. In simple terms, discriminant function analysis is classification - the act of distributing things into groups, classes or categories of the same type.",Linear discriminant analysis,1,1
334,17904,"In statistics, the term linear model is used in different ways according to the context. The most common occurrence is in connection with regression models and the term is often taken as synonymous with linear regression model.  However, the term is also used in time series analysis with a different meaning. In each case, the designation ""linear"" is used to identify a subclass of models for which substantial reduction in the complexity of the related statistical theory is possible.

",Linear model,1,0
336,6534294,"The Ljung–Box test (named for Greta M. Ljung and George E. P. Box) is a type of statistical test of whether any of a group of autocorrelations of a time series are different from zero. Instead of testing randomness at each distinct lag, it tests the ""overall"" randomness based on a number of lags, and is therefore a portmanteau test.
This test is sometimes known as the Ljung–Box Q test, and it is closely connected to the Box–Pierce test (which is named after George E. P. Box and David A. Pierce). In fact, the Ljung–Box test statistic was described explicitly in the paper that led to the use of the Box–Pierce statistic, and from which that statistic takes its name. The Box–Pierce test statistic is a simplified version of the Ljung–Box statistic for which subsequent simulation studies have shown poor performance.The Ljung–Box test is widely applied in econometrics and other applications of time series analysis. A similar assessment can be also carried out with the Breusch–Godfrey test and the Durbin–Watson test.",Ljung–Box test,1,0
337,17516,"In statistics, a location parameter of a probability distribution is a scalar- or vector-valued parameter 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
  , which determines the ""location"" or shift of the distribution. In the literature of location parameter estimation, the probability distributions with such parameter are found to be formally defined in one of the following equivalent ways:

either as having a probability density function or probability mass function 
  
    
      
        f
        (
        x
        −
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle f(x-x_{0})}
  ; or
having a cumulative distribution function 
  
    
      
        F
        (
        x
        −
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle F(x-x_{0})}
  ; or
being defined as resulting from the random variable transformation 
  
    
      
        
          x
          
            0
          
        
        +
        X
      
    
    {\displaystyle x_{0}+X}
  , where 
  
    
      
        X
      
    
    {\displaystyle X}
   is a random variable with a certain, possibly unknown, distribution (See also #Additive_noise).A direct example of a location parameter is the parameter 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   of the normal distribution. To see this, note that the probability density function 
  
    
      
        f
        (
        x
        
          |
        
        μ
        ,
        σ
        )
      
    
    {\displaystyle f(x|\mu ,\sigma )}
   of a normal distribution 
  
    
      
        
          
            N
          
        
        (
        μ
        ,
        
          σ
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {N}}(\mu ,\sigma ^{2})}
   can have the parameter 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   factored out and be written as: 

  
    
      
        g
        (
        y
        −
        μ
        
          |
        
        σ
        )
        =
        
          
            1
            
              σ
              
                
                  2
                  π
                
              
            
          
        
        
          e
          
            −
            
              
                1
                2
              
            
            
              
                (
                
                  
                    y
                    σ
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle g(y-\mu |\sigma )={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {y}{\sigma }}\right)^{2}}}
  thus fulfilling the first of the definitions given above.
The above definition indicates, in the one-dimensional case, that if 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
   is increased, the probability density or mass function shifts rigidly to the right, maintaining its exact shape.
A location parameter can also be found in families having more than one parameter, such as location–scale families. In this case, the probability density function or probability mass function will be a special case of the more general form

  
    
      
        
          f
          
            
              x
              
                0
              
            
            ,
            θ
          
        
        (
        x
        )
        =
        
          f
          
            θ
          
        
        (
        x
        −
        
          x
          
            0
          
        
        )
      
    
    {\displaystyle f_{x_{0},\theta }(x)=f_{\theta }(x-x_{0})}
  where 
  
    
      
        
          x
          
            0
          
        
      
    
    {\displaystyle x_{0}}
   is the location parameter, θ represents additional parameters, and 
  
    
      
        
          f
          
            θ
          
        
      
    
    {\displaystyle f_{\theta }}
   is a function parametrized on the additional parameters.",Location parameter,1,0
338,2680667,"In probability theory, especially in mathematical statistics, a location–scale family is a family of probability distributions parametrized by a location parameter and a non-negative scale parameter. For any random variable 
  
    
      
        X
      
    
    {\displaystyle X}
   whose probability distribution function belongs to such a family, the distribution function of 
  
    
      
        Y
        
          
            
              
                =
              
              
                d
              
            
          
        
        a
        +
        b
        X
      
    
    {\displaystyle Y{\stackrel {d}{=}}a+bX}
   also belongs to the family (where 
  
    
      
        
          
            
              
                =
              
              
                d
              
            
          
        
      
    
    {\displaystyle {\stackrel {d}{=}}}
   means ""equal in distribution""—that is, ""has the same distribution as""). 
In other words, a class 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
   of probability distributions is a location–scale family if for all cumulative distribution functions 
  
    
      
        F
        ∈
        Ω
      
    
    {\displaystyle F\in \Omega }
   and any real numbers 
  
    
      
        a
        ∈
        
          R
        
      
    
    {\displaystyle a\in \mathbb {R} }
   and 
  
    
      
        b
        >
        0
      
    
    {\displaystyle b>0}
  , the distribution function 
  
    
      
        G
        (
        x
        )
        =
        F
        (
        a
        +
        b
        x
        )
      
    
    {\displaystyle G(x)=F(a+bx)}
   is also a member of 
  
    
      
        Ω
      
    
    {\displaystyle \Omega }
  .

If 
  
    
      
        X
      
    
    {\displaystyle X}
   has a cumulative distribution function 
  
    
      
        
          F
          
            X
          
        
        (
        x
        )
        =
        P
        (
        X
        ≤
        x
        )
      
    
    {\displaystyle F_{X}(x)=P(X\leq x)}
  , then 
  
    
      
        Y
        
          =
        
        a
        +
        b
        X
      
    
    {\displaystyle Y{=}a+bX}
   has a cumulative distribution function 
  
    
      
        
          F
          
            Y
          
        
        (
        y
        )
        =
        
          F
          
            X
          
        
        
          (
          
            
              
                y
                −
                a
              
              b
            
          
          )
        
      
    
    {\displaystyle F_{Y}(y)=F_{X}\left({\frac {y-a}{b}}\right)}
  .
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a discrete random variable with probability mass function 
  
    
      
        
          p
          
            X
          
        
        (
        x
        )
        =
        P
        (
        X
        =
        x
        )
      
    
    {\displaystyle p_{X}(x)=P(X=x)}
  , then 
  
    
      
        Y
        
          =
        
        a
        +
        b
        X
      
    
    {\displaystyle Y{=}a+bX}
   is a discrete random variable with probability mass function 
  
    
      
        
          p
          
            Y
          
        
        (
        y
        )
        =
        
          p
          
            X
          
        
        
          (
          
            
              
                y
                −
                a
              
              b
            
          
          )
        
      
    
    {\displaystyle p_{Y}(y)=p_{X}\left({\frac {y-a}{b}}\right)}
  .
If 
  
    
      
        X
      
    
    {\displaystyle X}
   is a continuous random variable with probability density function 
  
    
      
        
          f
          
            X
          
        
        (
        x
        )
      
    
    {\displaystyle f_{X}(x)}
  , then 
  
    
      
        Y
        
          =
        
        a
        +
        b
        X
      
    
    {\displaystyle Y{=}a+bX}
   is a continuous random variable with probability density function 
  
    
      
        
          f
          
            Y
          
        
        (
        y
        )
        =
        
          
            1
            b
          
        
        
          f
          
            X
          
        
        
          (
          
            
              
                y
                −
                a
              
              b
            
          
          )
        
      
    
    {\displaystyle f_{Y}(y)={\frac {1}{b}}f_{X}\left({\frac {y-a}{b}}\right)}
  .Moreover, if 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are two random variables whose distribution functions are members of the family, and assuming existence of the first two moments and  
  
    
      
        X
      
    
    {\displaystyle X}
   has zero mean and unit variance,
then 
  
    
      
        Y
      
    
    {\displaystyle Y}
   can be written as 
  
    
      
        Y
        
          
            
              
                =
              
              
                d
              
            
          
        
        
          μ
          
            Y
          
        
        +
        
          σ
          
            Y
          
        
        X
      
    
    {\displaystyle Y{\stackrel {d}{=}}\mu _{Y}+\sigma _{Y}X}
   , where 
  
    
      
        
          μ
          
            Y
          
        
      
    
    {\displaystyle \mu _{Y}}
   and 
  
    
      
        
          σ
          
            Y
          
        
      
    
    {\displaystyle \sigma _{Y}}
   are the mean and standard deviation of 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
In decision theory, if all alternative distributions available to a decision-maker are in the same location–scale family, and the first two moments are finite,  then a two-moment decision model can apply, and decision-making can be framed in terms of the means and the variances of the distributions.",Location–scale family,1,0
339,11871026,"The logrank test, or log-rank test, is a hypothesis test to compare the survival distributions of two samples.  It is a nonparametric test and appropriate to use when the data are right skewed and censored (technically, the censoring must be non-informative).  It is widely used in clinical trials to establish the efficacy of a new treatment in comparison with a control treatment when the measurement is the time to event (such as the time from initial treatment to a heart attack). The test is sometimes called the Mantel–Cox test, named after Nathan Mantel and David Cox. The logrank test can also be viewed as a time-stratified Cochran–Mantel–Haenszel test.
The test was first proposed by Nathan Mantel and was named the logrank test by Richard and Julian Peto.

",Logrank test,1,0
340,226631,"In statistics, the (binary) logistic model (or logit model) is a statistical model that models the probability of one event (out of two alternatives) taking place by having the log-odds (the logarithm of the odds) for the event be a linear combination of one or more independent variables (""predictors""). In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (the coefficients in the linear combination). Formally, in binary logistic regression there is a single binary dependent variable, coded by a indicator variable, where the two values are labeled ""0"" and ""1"", while the independent variables can each be a binary variable (two classes, coded by an indicator variable) or a continuous variable (any real value). The corresponding probability of the value labeled ""1"" can vary between 0 (certainly the value ""0"") and 1 (certainly the value ""1""), hence the labeling; the function that converts log-odds to probability is the logistic function, hence the name. The unit of measurement for the log-odds scale is called a logit, from logistic unit, hence the alternative names. See § Background and § Definition for formal mathematics, and § Example for a worked example.
Binary variables are widely used in statistics to model the probability of a certain class or event taking place, such as the probability of a team winning, of a patient being healthy, etc. (see § Applications), and the logistic model has been the most commonly used model for binary regression since about 1970. Binary variables can be generalized to categorical variables when there are more than two possible values (e.g. whether an image is of a cat, dog, lion, etc.), and the binary logistic regression generalized to multinomial logistic regression. If the multiple categories are ordered, one can use the ordinal logistic regression (for example the proportional odds ordinal logistic model). See § Extensions for further extensions. The logistic regression model itself simply models probability of output in terms of input and does not perform statistical classification (it is not a classifier), though it can be used to make a classifier, for instance by choosing a cutoff value and classifying inputs with probability greater than the cutoff as one class, below the cutoff as the other; this is a common way to make a binary classifier.
Analogous linear models for binary variables with a different sigmoid function instead of the logistic function (to convert the linear combination to a probability) can also be used, most notably the probit model; see § Alternatives. The defining characteristic of the logistic model is that increasing one of the independent variables multiplicatively scales the odds of the given outcome at a constant rate, with each independent variable having its own parameter; for a binary dependent variable this generalizes the odds ratio. More abstractly, the logistic function is the natural parameter for the Bernoulli distribution, and in this sense is the ""simplest"" way to convert a real number to a probability. In particular, it maximizes entropy (minimizes added information), and in this sense makes the fewest assumptions of the data being modeled; see § Maximum entropy.
The parameters of a logistic regression are most commonly estimated by maximum-likelihood estimation (MLE). This does not have a closed-form expression, unlike linear least squares; see § Model fitting. Logistic regression by MLE plays a similarly basic role for binary or categorical responses as linear regression by ordinary least squares (OLS) plays for scalar responses: it is a simple, well-analyzed baseline model; see § Comparison with linear regression for discussion. The logistic regression as a general statistical model was originally developed and popularized primarily by Joseph Berkson, beginning in Berkson (1944), where he coined ""logit""; see § History.",Logistic regression,1,1
341,442137,"In mathematical optimization and decision theory, a loss function or cost function (sometimes also called an error function)  is a function that maps an event or values of one or more variables onto a real number intuitively representing some ""cost"" associated with the event. An optimization problem seeks to minimize a loss function. An objective function is either a loss function or its opposite (in specific domains, variously called a reward function, a profit function, a utility function, a fitness function, etc.), in which case it is to be maximized. The loss function could include terms from several levels of the hierarchy.
In statistics, typically a  loss function is used for parameter estimation, and the event in question is some function of the difference between estimated and true values for an instance of data. The concept, as old as Laplace, was reintroduced in statistics by Abraham Wald in the middle of the 20th century.  In the context of economics, for example, this is usually economic cost or regret.  In classification, it is the penalty for an incorrect classification of an example. In actuarial science, it is used in an insurance context to model benefits paid over premiums, particularly since the works of Harald Cramér in the 1920s. In optimal control, the loss is the penalty for failing to achieve a desired value. In financial risk management, the function is mapped to a monetary loss.",Loss function,1,1
342,45194,"In mathematics, the Lp spaces are function spaces defined using a natural generalization of the p-norm for finite-dimensional vector spaces. They are sometimes called Lebesgue spaces, named after Henri Lebesgue (Dunford & Schwartz 1958, III.3), although according to the Bourbaki group (Bourbaki 1987) they were first introduced by Frigyes Riesz (Riesz 1910). Lp spaces form an important class of Banach spaces in functional analysis, and of topological vector spaces. Because of their key role in the mathematical analysis of measure and probability spaces, Lebesgue spaces are used also in the theoretical discussion of problems in physics, statistics, finance, engineering, and other disciplines.

",Lp space,1,0
343,4225388,"In statistics, M-estimators are a broad class of extremum estimators for which the objective function is a sample average. Both non-linear least squares and maximum likelihood estimation are special cases of M-estimators. The definition of M-estimators was motivated by robust statistics, which contributed new types of M-estimators. The statistical procedure of evaluating an M-estimator on a data set is called M-estimation. 48 samples of robust M-estimators can be found in a recent review study. More generally, an M-estimator may be defined to be a zero of an estimating function. This estimating function is often the derivative of another statistical function. For example, a maximum-likelihood estimate is the point where the derivative of the likelihood function with respect to the parameter is zero; thus, a maximum-likelihood estimator is a critical point of the score function.  In many applications, such M-estimators can be thought of as estimating characteristics of the population.",M-estimator,1,0
344,233488,"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.A subset of machine learning is closely related to computational statistics, which focuses on making predictions using computers, but not all machine learning is statistical learning. The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning. Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain.  In its application across business problems, machine learning is also referred to as predictive analytics.",Machine learning,1,1
345,19881,"Management (or managing) is the administration of an organization, whether it is a business, a non-profit organization, or a government body. It is the art and science of managing resources of the business. 
Management includes the activities of setting the strategy of an organization and coordinating the efforts of its employees (or of volunteers) to accomplish its objectives through the application of available resources, such as financial, natural, technological, and human resources. ""Run the business"" and ""Change the business"" are two concepts that are used in management to differentiate between the continued delivery of goods or services and adapting of goods or services to meet the changing needs of customers - see trend. The term ""management"" may also refer to those people who manage an organization—managers. 
Some people study management at colleges or universities; major degrees in management includes the Bachelor of Commerce (B.Com.), Bachelor of Business Administration (BBA.), Master of Business Administration (MBA.), Master in Management (MSM or MIM) and, for the public sector, the Master of Public Administration (MPA) degree. Individuals who aim to become management specialists or experts, management researchers, or professors may complete the Doctor of Management (DM), the Doctor of Business Administration (DBA), or the Ph.D. in Business Administration or Management. In the past few decades, there has been a movement for evidence-based management.Larger organizations generally have three hierarchical levels of managers, in a pyramid structure:

Senior managers such as members of a board of directors and a chief executive officer (CEO) or a president of an organization sets the strategic goals of the organization and make decisions on how the overall organization will operate. Senior managers are generally executive-level professionals who provide direction to middle management, and directly or indirectly report to them.
Middle managers such as branch managers, regional managers, department managers, and section managers, who provide direction to the front-line managers. They communicate the strategic goals of senior management to the front-line managers.
Line managers such as supervisors and front-line team leaders, oversee the work of regular employees (or volunteers, in some voluntary organizations) and provide direction on their work. Line managers often perform the traditional functions of management. They are usually considered part of the workforce and not a proper part of the organization's management.In smaller organizations, a manager may have a much wider scope and may perform several roles or even all of the roles commonly observed in a large organization.
Social scientists study management as an academic discipline, investigating areas such as social organization, organizational adaptation, and organizational leadership.",Management,1,0
346,20200,"Management science or Managerial Science (MS) is the broad interdisciplinary study of problem solving and decision making in human organizations, with strong links to management, economics, business, engineering, management consulting, and other fields. It uses various scientific research-based principles, strategies, and analytical methods including  mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and accurate management decisions by arriving at optimal or near optimal solutions to complex decision problems.  Management science helps businesses to achieve goals using various scientific methods.
The field was initially an outgrowth of applied mathematics, where early challenges were problems relating to the optimization of systems which could be modeled linearly, i.e., determining the optima (maximum value of profit, assembly line performance, crop yield, bandwidth, etc. or minimum of loss, risk, costs, etc.) of some objective function. Today, management science encompasses any organizational activity for which a problem is structured in mathematical form to generate managerially relevant insights.",Management science,1,0
347,42272,"In management accounting or managerial accounting, managers use accounting information in decision-making and to assist in the management and performance of their control functions.",Management accounting,1,0
348,433164,"In statistics, the Mann–Whitney U test (also called the Mann–Whitney–Wilcoxon (MWW/MWU), Wilcoxon rank-sum test, or Wilcoxon–Mann–Whitney test) is a nonparametric test of the null hypothesis that, for randomly selected values X and Y from two populations, the probability of X being greater than Y is equal to the probability of Y being greater than X.
Nonparametric tests used on two dependent samples are the Sign test and the Wilcoxon signed-rank test.

",Mann–Whitney U test,1,1
351,2971012,"Manufacturing process management (MPM) is a collection of technologies and methods used to define how products are to be manufactured. MPM differs from ERP/MRP which is used to plan the ordering of materials and other resources, set manufacturing schedules, and compile cost data.A cornerstone of MPM is the central repository for the integration of all these tools and activities aids in the exploration of alternative production line scenarios; making assembly lines more efficient with the aim of reduced lead time to product launch, shorter product times and reduced work in progress (WIP) inventories as well as allowing rapid response to product or product changes.

Production process planning
Manufacturing concept planning
Factory layout planning and analysis
work flow simulation.
walk-path assembly planning
plant design optimization
Mixed model line balancing.
Workloads on multiple stations.
Process simulation tools e.g. die press lines, manufacturing lines
Ergonomic simulation and assessment of production assembly tasks
Resource planning
Computer-aided manufacturing (CAM)
Numerical control CNC
Direct numerical control (DNC)
Tooling/equipment/fixtures development
Tooling and Robot work-cell setup and offline programming (OLP)
Generation of shop floor work instructions
Time and cost estimates
ABC – Manufacturing activity-based costing
Outline of industrial organization
Quality computer-aided quality assurance (CAQ)
Failure mode and effects analysis (FMEA)
Statistical process control (SPC)
Computer aided inspection with coordinate-measuring machine (CMM)
Tolerance stack-up analysis using PMI models.
Success measurements
Overall equipment effectiveness (OEE),
Communication with other systems
Enterprise resource planning (ERP)
Manufacturing operations management (MOM)
Product data management (PDM)
SCADA (supervisory control and data acquisition) real time process monitoring and control
Human–machine interface (HMI) (or man-machine interface (MMI))
Distributed control system (DCS)",Manufacturing process management,1,0
352,241637,"Marketing management is the organizational discipline which focuses on the practical application of marketing orientation, techniques and methods inside enterprises and organizations and on the management of a firm's marketing resources and activities.

",Marketing management,1,0
353,60876,"A Markov chain or Markov process is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. A countably infinite sequence, in which the chain moves state at discrete time steps, gives a discrete-time Markov chain (DTMC). A continuous-time process is called a continuous-time Markov chain (CTMC). It is named after the Russian mathematician Andrey Markov.
Markov chains have many applications as statistical models of real-world processes, such as studying cruise control systems in motor vehicles, queues or lines of customers arriving at an airport, currency exchange rates and animal population dynamics.Markov processes are the basis for general stochastic simulation methods known as Markov chain Monte Carlo, which are used for simulating sampling from complex probability distributions, and have found application in Bayesian statistics, thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory and speech processing.The adjectives Markovian and Markov are used to describe something that is related to a Markov process.",Markov chain,1,0
354,41990,"In linguistics, a mass noun, uncountable noun, non-count noun, uncount noun, or just uncountable, is a noun with the syntactic property that any quantity of it is treated as an undifferentiated unit, rather than as something with discrete elements. Non-count nouns are distinguished from count nouns.
Given that different languages have different grammatical features, the actual test for which nouns are mass nouns may vary between languages. In English, mass nouns are characterized by the impossibility of being directly modified by a numeral without specifying a unit of measurement and by the impossibility of being combined with an indefinite article (a or an). Thus, the mass noun ""water"" is quantified as ""20 litres of water"" while the count noun ""chair"" is quantified as ""20 chairs"". However, both mass and count nouns can be quantified in relative terms without unit specification (e.g., ""so much water"", ""so many chairs"").
Mass nouns have no concept of singular and plural, although in English they take singular verb forms.  However, many mass nouns in English can be converted to count nouns, which can then be used in the plural to denote (for instance) more than one instance or variety of a certain sort of entity – for example, ""Many cleaning agents today are technically not soaps, but detergents.""
Some nouns can be used indifferently as mass or count nouns, e.g., three cabbages or three heads of cabbage; three ropes or three lengths of rope. Some have different senses as mass and count nouns: paper is a mass noun as a material (three reams of paper, one sheet of paper), but a count noun as a unit of writing (""the students passed in their papers"").",Mass noun,1,0
355,18831,"Mathematics (from Ancient Greek  μάθημα; máthēma: 'knowledge, study, learning') is an area of knowledge that includes such topics as numbers (arithmetic, number theory), formulas and related structures (algebra), shapes and the spaces in which they are contained (geometry), and quantities and their changes (calculus and analysis).Most mathematical activity involves discovering and proving properties of abstract objects by pure reasoning. These objects are either abstractions from nature, such as natural numbers or lines, or — in modern mathematics — entities that are stipulated with certain properties, called axioms. A proof consists of a succession of applications of some deductive rules to already known results, including previously proved theorems, axioms and (in case of abstraction from nature) some basic properties that are considered as true starting points of the theory under consideration. The result of a proof is called a theorem. 
Mathematics is widely used in science for modeling phenomena. This enables the extraction of quantitative predictions from experimental laws. For example, the movement of planets can be accurately predicted using Newton's law of gravitation combined with mathematical computation. The independence of mathematical truth from any experimentation implies that the accuracy of such predictions depends only on the adequacy of the model for describing the reality. Inaccurate predictions imply the need for improving or changing mathematical models, not that mathematics is wrong in the models themselves. For example, the perihelion precession of Mercury cannot be explained by Newton's law of gravitation but is accurately explained by Einstein's general relativity. This experimental validation of Einstein's theory shows that Newton's law of gravitation is only an approximation, though accurate in everyday application. 
Mathematics is essential in many fields, including natural sciences, engineering, medicine, finance, computer science and social sciences.
Some areas of mathematics, such as statistics and game theory, are developed in close correlation with their applications and are often grouped under applied mathematics. Other mathematical areas are developed independently from any application (and are therefore called pure mathematics), but practical applications are often discovered later. A fitting example is the problem of integer factorization, which goes back to Euclid, but which had no practical application before its use in the RSA cryptosystem (for the security of computer networks).
Historically, the concept of a proof and its associated mathematical rigour first appeared in Greek mathematics, most notably in Euclid's Elements. Since its beginning, mathematics were essentially divided into geometry, and arithmetic (the manipulation of natural numbers and fractions), until the 16th and 17th centuries, when algebra and infinitesimal calculus were introduced as new areas. Since then, the interaction between mathematical innovations and scientific discoveries has led to a rapid increase in the development of mathematics. At the end of the 19th century, the foundational crisis of mathematics led to the systematization of the axiomatic method. This, in turn, gave rise to a dramatic increase in the number of mathematics areas and their fields of applications. An example of this is the Mathematics Subject Classification, which lists more than sixty first-level areas of mathematics.",Mathematics,1,1
356,48396,"Analysis is the branch of mathematics dealing with continuous functions, limits, and related theories, such as differentiation, integration, measure, infinite sequences, series, and analytic functions.These theories are usually studied in the context of real and complex numbers and functions. Analysis evolved from calculus, which involves the elementary concepts and techniques of analysis.
Analysis may be distinguished from geometry; however, it can be applied to any space of mathematical objects that has a definition of nearness (a topological space) or specific distances between objects  (a metric space).

",Mathematical analysis,1,1
357,662088,"Mathematical and theoretical biology, or biomathematics, is a branch of biology which employs theoretical analysis, mathematical models and abstractions of the living organisms to investigate the principles that govern the structure, development and behavior of the systems, as opposed to experimental biology which deals with the conduction of experiments to prove and validate the scientific theories. The field is sometimes called mathematical biology or biomathematics to stress the mathematical side, or theoretical biology to stress the biological side. Theoretical biology focuses more on the development of theoretical principles for biology while mathematical biology focuses on the use of mathematical tools to study biological systems, even though the two terms are sometimes interchanged.Mathematical biology aims at the mathematical representation and modeling of biological processes, using techniques and tools of applied mathematics. It can be useful in both theoretical and practical research. Describing systems in a quantitative manner means their behavior can be better simulated, and hence properties can be predicted that might not be evident to the experimenter. This requires precise mathematical models.
Because of the complexity of the living systems, theoretical biology employs several fields of mathematics, and has contributed to the development of new techniques.",Mathematical and theoretical biology,1,0
358,1570072,"Mathematical chemistry is the area of research engaged in novel applications of mathematics to chemistry; it concerns itself principally with the mathematical modeling of chemical phenomena. Mathematical chemistry has also sometimes been called computer chemistry, but should not be confused with computational chemistry.
Major areas of research in mathematical chemistry include chemical graph theory, which deals with topology such as the mathematical study of isomerism and the development of topological descriptors or indices which find application in quantitative structure-property relationships; and chemical aspects of group theory, which finds applications in stereochemistry and quantum chemistry. Another important area is molecular knot theory and circuit topology that describe the topology of folded linear molecules such as proteins and Nucleic Acids. 
The history of the approach may be traced back to the 19th century. Georg Helm published a treatise titled ""The Principles of Mathematical Chemistry: The Energetics of Chemical Phenomena"" in 1894. Some of the more contemporary periodical publications specializing in the field are MATCH Communications in Mathematical and in Computer Chemistry, first published in 1975, and the Journal of Mathematical Chemistry, first published in 1987. In 1986 a series of annual conferences MATH/CHEM/COMP taking place in Dubrovnik was
initiated by the late Ante Graovac.
The basic models for mathematical chemistry are molecular graph and topological index.
In 2005 the International Academy of Mathematical Chemistry (IAMC) was founded in Dubrovnik (Croatia) by Milan Randić. The Academy has 82 members (2009) from all over the world, including six scientists awarded with a Nobel Prize.",Mathematical chemistry,1,0
359,18967255,"Mathematical economics is the application of mathematical methods to represent theories and analyze problems in economics.  Often, these applied methods are beyond simple geometry, and may include differential and integral calculus, difference and differential equations, matrix algebra, mathematical programming, or other computational methods. Proponents of this approach claim that it allows the formulation of theoretical relationships with rigor, generality, and simplicity.Mathematics allows economists to form meaningful, testable propositions about  wide-ranging and complex subjects which could less easily  be expressed informally. Further, the language of mathematics allows economists to make specific, positive claims about controversial or contentious subjects that would be impossible without mathematics. Much of economic theory is currently presented in terms of mathematical economic models, a set of stylized and simplified mathematical relationships asserted to clarify assumptions and implications.Broad applications include:

optimization problems as to goal equilibrium, whether of a household, business firm, or policy maker
static (or equilibrium) analysis in which the economic unit (such as a household) or economic system (such as a market or the economy) is modeled as not changing
comparative statics as to a change from one equilibrium to another induced by a change in one or more factors
dynamic analysis, tracing changes in an economic system over time, for example from economic growth.Formal economic modeling began in the 19th century with the use of differential calculus to represent and explain economic behavior, such as utility maximization, an early economic application of mathematical optimization. Economics became more mathematical as a discipline throughout the first half of the 20th century, but introduction of new and generalized techniques in the period around the Second World War, as in game theory, would greatly broaden the use of mathematical formulations in economics.This rapid systematizing of economics alarmed critics of the discipline as well as some noted economists.  John Maynard Keynes, Robert Heilbroner, Friedrich Hayek and others have criticized the broad use of mathematical models for human behavior, arguing that some human choices are irreducible to mathematics.",Mathematical economics,1,0
360,33589680,"Mathematical finance, also known as quantitative finance and financial mathematics, is a field of applied mathematics, concerned with mathematical modeling of financial markets.
In general, there exist two separate branches of finance that require advanced quantitative techniques: derivatives pricing on the one hand, and risk and portfolio management on the other.
Mathematical finance overlaps heavily with the fields of computational finance and financial engineering. The latter focuses on applications and modeling, often by help of stochastic asset models, while the former focuses, in addition to analysis, on building tools of implementation for the models. 
Also related is quantitative investing, which relies on statistical and numerical models (and lately machine learning) as opposed to traditional fundamental analysis when managing portfolios.
French mathematician Louis Bachelier is considered the author of the first scholarly work on mathematical finance, published in 1900. But mathematical finance emerged as a discipline in the 1970s, following the work of Fischer Black, Myron Scholes and Robert Merton on option pricing theory. Mathematical investing originated from the research of mathematician Edward Thorp who used statistical methods to first invent card counting in blackjack and then applied its principles to modern systematic investing.The subject has a close relationship with the discipline of financial economics, which is concerned with much of the underlying theory that is involved in financial mathematics. 
Generally, mathematical finance will derive and extend the mathematical or numerical models without necessarily establishing a link to financial theory, taking observed market prices as input. Mathematical consistency is required, not compatibility with economic theory. Thus, for example, while a financial economist might study the structural reasons why a company may have a certain share price, a financial mathematician may take the share price as a given, and attempt to use stochastic calculus to obtain the corresponding value of derivatives of the stock. 
See: Valuation of options; Financial modeling; Asset pricing. 
The fundamental theorem of arbitrage-free pricing is one of the key theorems in mathematical finance, while the Black–Scholes equation and formula are amongst the key results.Today many universities offer degree and research programs in mathematical finance.

",Mathematical finance,1,0
361,19636,"Mathematical logic is the study of formal logic within mathematics. Major subareas include model theory, proof theory, set theory, and recursion theory. Research in mathematical logic commonly addresses the mathematical properties of formal systems of logic such as their expressive or deductive power. However, it can also include uses of logic to characterize correct mathematical reasoning or to establish foundations of mathematics.
Since its inception, mathematical logic has both contributed to and has been motivated by the study of foundations of mathematics. This study began in the late 19th century with the development of axiomatic frameworks for geometry, arithmetic, and analysis. In the early 20th century it was shaped by David Hilbert's program to prove the consistency of foundational theories. Results of Kurt Gödel, Gerhard Gentzen, and others provided partial resolution to the program, and clarified the issues involved in proving consistency. Work in set theory showed that almost all ordinary mathematics can be formalized in terms of sets, although there are some theorems that cannot be proven in common axiom systems for set theory. Contemporary work in the foundations of mathematics often focuses on establishing which parts of mathematics can be formalized in particular formal systems (as in reverse mathematics) rather than trying to find theories in which all of mathematics can be developed.",Mathematical logic,1,0
362,52033,"Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, with regard to some criterion, from some set of available alternatives. Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, and the development of solution methods has been of interest in mathematics for centuries.In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically choosing input values from within an allowed set and computing the value of the function. The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. More generally, optimization includes finding ""best available"" values of some objective function given a defined domain (or input), including a variety of different types of objective functions and different types of domains.",Mathematical optimization,1,1
363,173416,"Mathematical physics refers to the development of mathematical methods for application to problems in physics. The Journal of Mathematical Physics defines the field as ""the application of mathematics to problems in physics and the development of mathematical methods suitable for such applications and for the formulation of physical theories"". An alternative definition would also include those mathematics that are inspired by physics (also known as physical mathematics).",Mathematical physics,1,0
364,2081516,"Mathematical psychology is an approach to psychological research that is based on mathematical modeling of perceptual, thought, cognitive and motor processes, and on the establishment of law-like rules that relate quantifiable stimulus characteristics with quantifiable behavior (in practice often constituted by task performance). The mathematical approach is used with the goal of deriving hypotheses that are more exact and thus yield stricter empirical validations. There are five major research areas in mathematical psychology: learning and memory, perception and psychophysics, choice and decision-making, language and thinking, and measurement and scaling.Although psychology, as an independent subject of science, is a more recent discipline than physics, the application of mathematics to psychology has been done in the hope of emulating the success of this approach in the physical sciences, which dates back to at least the seventeenth century. Mathematics in psychology is used extensively roughly in two areas: one is the mathematical modeling of psychological theories and experimental phenomena, which leads to mathematical psychology, the other is the statistical approach of quantitative measurement practices in psychology, which leads to psychometrics.As quantification of behavior is fundamental in this endeavor, the theory of measurement is a central topic in mathematical psychology. Mathematical psychology is therefore closely related to psychometrics. However, where psychometrics is concerned with individual differences (or population structure) in mostly static variables, mathematical psychology focuses on process models of perceptual, cognitive and motor processes as inferred from the 'average individual'. Furthermore, where psychometrics investigates the stochastic dependence structure between variables as observed in the population, mathematical psychology almost exclusively focuses on the modeling of data obtained from experimental paradigms and is therefore even more closely related to experimental psychology, cognitive psychology, and psychonomics. Like computational neuroscience and econometrics, mathematical psychology theory often uses statistical optimality as a guiding principle, assuming that the human brain has evolved to solve problems in an optimized way. Central themes from cognitive psychology (e.g., limited vs. unlimited processing capacity, serial vs. parallel processing) and their implications are central in rigorous analysis in mathematical psychology. 
Mathematical psychologists are active in many fields of psychology, especially in psychophysics, sensation and perception, problem solving, decision-making, learning, memory, language, and the quantitative analysis of behavior, and contribute to the work of other subareas of psychology such as clinical psychology, social psychology, educational psychology, and psychology of music.",Mathematical psychology,1,0
365,4771336,"Mathematical sociology or the sociology of mathematics is an interdisciplinary field of research concerned both with the use of mathematics within sociological research as well as research into the relationships that exist between maths and society.Because of this mathematical sociology can have a diverse meaning depending on the authors in question and the kind of research being carried out. This creates contestation over whether mathematical sociology is a derivative of sociology, an intersection of the two disciplines, or a discipline in its own right. This is a dynamic, ongoing academic development that leaves mathematical sociology sometimes blurred and lacking in uniformity, presenting grey areas and need for further research into developing its academic remit.",Mathematical sociology,1,0
366,888711,"Mathematical statistics is the application of probability theory, a branch of mathematics, to statistics, as opposed to techniques for collecting statistical data. Specific mathematical techniques which are used for this include mathematical analysis, linear algebra, stochastic analysis, differential equations, and measure theory.",Mathematical statistics,1,0
368,22468661,"Mathematics and art are related in a variety of ways. Mathematics has itself been described as an art motivated by beauty. Mathematics can be discerned in arts such as music, dance, painting, architecture, sculpture, and textiles. This article focuses, however, on mathematics in the visual arts.
Mathematics and art have a long historical relationship. Artists have used mathematics since the 4th century BC when the Greek sculptor Polykleitos wrote his Canon, prescribing proportions conjectured to have been based on the ratio 1:√2 for the ideal male nude. Persistent popular claims have been made for the use of the golden ratio in ancient art and architecture, without reliable evidence. In the Italian Renaissance, Luca Pacioli wrote the influential treatise De divina proportione (1509), illustrated with woodcuts by Leonardo da Vinci, on the use of the golden ratio in art. Another Italian painter, Piero della Francesca, developed Euclid's ideas on perspective in treatises such as De Prospectiva Pingendi, and in his paintings. The engraver Albrecht Dürer made many references to mathematics in his work Melencolia I. In modern times, the graphic artist M. C. Escher made intensive use of tessellation and hyperbolic geometry, with the help of the mathematician H. S. M. Coxeter, while the De Stijl movement led by Theo van Doesburg and Piet Mondrian explicitly embraced geometrical forms. Mathematics has inspired textile arts such as quilting, knitting, cross-stitch, crochet, embroidery, weaving, Turkish and other carpet-making, as well as kilim. In Islamic art, symmetries are evident in forms as varied as Persian girih and Moroccan zellige tilework, Mughal jali pierced stone screens, and widespread muqarnas vaulting.
Mathematics has directly influenced art with conceptual tools such as linear perspective, the analysis of symmetry, and mathematical objects such as polyhedra and the Möbius strip. Magnus Wenninger creates colourful stellated polyhedra, originally as models for teaching. Mathematical concepts such as recursion and logical paradox can be seen in paintings by René Magritte and in engravings by M. C. Escher. Computer art often makes use of fractals including the Mandelbrot set, and sometimes explores other mathematical objects such as cellular automata. Controversially, the artist David Hockney has argued that artists from the Renaissance onwards made use of the camera lucida to draw precise representations of scenes; the architect Philip Steadman similarly argued that Vermeer used the camera obscura in his distinctively observed paintings.
Other relationships include the algorithmic analysis of artworks by X-ray fluorescence spectroscopy, the finding that traditional batiks from different regions of Java have distinct fractal dimensions, and stimuli to mathematics research, especially Filippo Brunelleschi's theory of perspective, which eventually led to Girard Desargues's projective geometry. A persistent view, based ultimately on the Pythagorean notion of harmony in music, holds that everything was arranged by Number, that God is the geometer of the world, and that therefore the world's geometry is sacred.",Mathematics and art,1,0
369,326471,"In contemporary education, mathematics education is the practice of teaching and learning mathematics, along with the associated scholarly research.
Researchers in mathematics education are primarily concerned with the tools, methods and approaches that facilitate practice or the study of practice; however, mathematics education research, known on the continent of Europe as the didactics or pedagogy of mathematics, has developed into an extensive field of study, with its concepts, theories, methods, national and international organisations, conferences and literature. This article describes some of the history, influences and recent controversies.",Mathematics education,1,0
370,3304216,"Mathematics during the Golden Age of Islam, especially during the 9th and 10th centuries, was built on Greek mathematics (Euclid, Archimedes, Apollonius) and Indian mathematics (Aryabhata, Brahmagupta). Important progress was made, such as full development of the decimal place-value system to include decimal fractions, the first systematised study of algebra, and advances in geometry and trigonometry.Arabic works played an important role in the transmission of mathematics to Europe during the 10th to 12th centuries.

",Mathematics in medieval Islam,1,0
371,1792433,"In Bayesian statistics, a maximum a posteriori probability (MAP) estimate is an estimate of an unknown quantity, that equals the mode of the posterior distribution. The MAP can be used to obtain a point estimate of an unobserved quantity on the basis of empirical data. It is closely related to the method of maximum likelihood (ML) estimation, but employs an augmented optimization objective which incorporates a prior distribution (that quantifies the additional information available through prior knowledge of a related event) over the quantity one wants to estimate.  MAP estimation can therefore be seen as a regularization of maximum likelihood estimation.",Maximum a posteriori estimation,1,0
372,140806,"In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of an assumed probability distribution, given some observed data. This is achieved by maximizing a likelihood function so that, under the assumed statistical model, the observed data is most probable. The point in the parameter space that maximizes the likelihood function is called the maximum likelihood estimate. The logic of maximum likelihood is both intuitive and flexible, and as such the method has become a dominant means of statistical inference.If the likelihood function is differentiable, the derivative test for determining maxima can be applied. In some cases, the first-order conditions of the likelihood function can be solved explicitly; for instance, the ordinary least squares estimator maximizes the likelihood of the linear regression model. Under most circumstances, however, numerical methods will be necessary to find the maximum of the likelihood function.
From the perspective of Bayesian inference, MLE is generally equivalent to maximum a posteriori (MAP) estimation under a uniform prior distribution on the parameters. In frequentist inference, MLE is a special case of an extremum estimator, with the objective function being the likelihood.",Maximum likelihood estimation,1,0
373,3305230,"In statistics, McNemar's test is a statistical test used on paired nominal data. It is applied to 2 × 2 contingency tables with a dichotomous trait, with matched pairs of subjects, to determine whether the row and column marginal frequencies are equal (that is, whether there is ""marginal homogeneity""). It is named after Quinn McNemar, who introduced it in 1947. An application of the test in genetics is the transmission disequilibrium test for detecting linkage disequilibrium.The commonly used parameters to assess a diagnostic test in medical sciences are sensitivity and specificity. Sensitivity (or recall) is the ability of a test to correctly identify the people with disease. Specificity is the ability of the test  to correctly identify those without the disease. 
Now presume two tests are performed on the same group of patients. And also presume that these tests have identical sensitivity and specificity. In this situation one is carried away by these findings and presume that both the tests are equivalent. However this may not be the case. For this we have to study the patients with disease and patients without disease (by a reference test). We also have to find out where these two tests disagree with each other. This is precisely the basis of McNemar's test. This test compares the sensitivity and specificity of  two diagnostic tests on the same group of patients.

",McNemar's test,1,0
374,20351457,"In statistics, the reduced chi-square statistic is used extensively in goodness of fit testing. It is also known as mean squared weighted deviation (MSWD) in isotopic dating and variance of unit weight in the context of weighted least squares.Its square root is called regression standard error, standard error of the regression, or standard error of the equation
(see Ordinary least squares#Reduced chi-squared)",Reduced chi-squared statistic,1,0
375,201816,"In statistics, the mean squared error (MSE) or mean squared deviation (MSD) of an estimator (of a procedure for estimating an unobserved quantity) measures the average of the squares of the errors—that is, the average squared difference between the estimated values and the actual value. MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. In machine learning, specifically empirical risk minimization, MSE may refer to the empirical risk (the average loss on an observed data set), as an estimate of the true MSE (the true risk: the average loss on the actual population distribution).
The MSE is a measure of the quality of an estimator.  As it is derived from the square of Euclidean distance, it is always a positive value that decreases as the error approaches zero.
The MSE is the second moment (about the origin) of the error, and thus incorporates both the variance of the estimator (how widely spread the estimates are from one data sample to another) and its bias (how far off the average estimated value is from the true value). For an unbiased estimator, the MSE is the variance of the estimator. Like the variance, MSE has the same units of measurement as the square of the quantity being estimated. In an analogy to standard deviation, taking the square root of MSE yields the root-mean-square error or root-mean-square deviation (RMSE or RMSD), which has the same units as the quantity being estimated; for an unbiased estimator, the RMSE is the square root of the variance, known as the standard error.",Mean squared error,1,0
376,23542,"Probability theory is the branch of mathematics concerned with probability. Although there are several different probability interpretations, probability theory treats the concept in a rigorous mathematical manner by expressing it through a set of axioms. Typically these axioms formalise probability in terms of a probability space, which assigns a measure taking values between 0 and 1, termed the probability measure, to a set of outcomes called the sample space. Any specified subset of the sample space is called an event.
Central subjects in probability theory include discrete and continuous random variables, probability distributions, and stochastic processes (which provide mathematical abstractions of non-deterministic or uncertain processes or measured quantities that may either be single occurrences or evolve over time in a random fashion).
Although it is not possible to perfectly predict random events, much can be said about their behavior. Two major results in probability theory describing such behaviour are the law of large numbers and the central limit theorem.
As a mathematical foundation for statistics, probability theory is essential to many human activities that involve quantitative analysis of data. Methods of probability theory also apply to descriptions of complex systems given only partial knowledge of their state, as in statistical mechanics or sequential estimation. A great discovery of twentieth-century physics was the probabilistic nature of physical phenomena at atomic scales, described in quantum mechanics. 

",Probability theory,1,1
377,19873,"In mathematics, the concept of a measure is a generalization and formalization of geometrical measures (length, area, volume) and other common notions, such as mass and probability of events. These seemingly distinct concepts have many similarities and can often be treated together in a single mathematical context. Measures are foundational in probability theory, integration theory, and can be generalized to assume negative values, as with electrical charge. Far-reaching  generalizations (such as spectral measures and projection-valued measures) of measure are widely used in quantum physics and physics in general.
The intuition behind this concept dates back to Ancient Greece, when Archimedes tried to calculate the area of a circle. But it was not until the late 19th and early 20th centuries that measure theory became a branch of mathematics. The foundations of modern measure theory were laid in the works of Émile Borel, Henri Lebesgue, Nikolai Luzin, Johann Radon, Constantin Carathéodory, and Maurice Fréchet, among others.",Measure (mathematics),1,0
378,18837,"In statistics and probability theory, the median is the value separating the higher half from the lower half of a data sample, a population, or a probability distribution. For a data set, it may be thought of as ""the middle"" value. The basic feature of the median in describing data compared to the mean (often simply described as the ""average"") is that it is not skewed by a small proportion of extremely large or small values, and therefore provides a better representation of a ""typical"" value. Median income, for example, may be a better way to suggest what a ""typical"" income is, because income distribution can be very skewed. The median is of central importance in robust statistics, as it is the most resistant statistic, having a breakdown point of 50%: so long as no more than half the data are contaminated, the median is not an arbitrarily large or small result.",Median,1,1
379,11082896,"Medical statistics deals with applications of statistics to medicine and the health sciences, including epidemiology, public health, forensic medicine, and clinical research. Medical statistics has been a recognized branch of statistics in the United Kingdom for more than 40 years but the term has not come into general use in North America, where the wider term 'biostatistics' is more commonly used. However, ""biostatistics"" more commonly connotes all applications of statistics to biology. Medical statistics is a subdiscipline of statistics. ""It is the science of summarizing, collecting, presenting and interpreting data in medical practice, and using them to estimate the magnitude of associations and test hypotheses. It has a central role in medical investigations. It not only provides a way of organizing information on a wider and more formal basis than relying on the exchange of anecdotes and personal experience, but also takes into account the intrinsic variation inherent in most biological processes.""",Medical statistics,1,0
380,30648906,"Medieval Roman law is the continuation and development of ancient Roman law that developed in the European Late Middle Ages. Based on the ancient text of Roman law, Corpus iuris civilis, it added many new concepts, and formed the basis of the later civil law systems that prevail in the vast majority of countries.",Medieval Roman law,1,0
382,2175504,"In statistics, the method of moments is a method of estimation of population parameters. The same principle is used to derive higher moments like skewness and kurtosis.
It starts by expressing the population moments (i.e., the expected values of powers of the random variable under consideration) as functions of the parameters of interest. Those expressions are then set equal to the sample moments. The number of such equations is the same as the number of parameters to be estimated. Those equations are then solved for the parameters of interest. The solutions are estimates of those parameters.  
The method of moments was introduced by Pafnuty Chebyshev in 1887 in the proof of the central limit theorem. The idea of matching empirical moments of a distribution to the population moments dates back at least to Pearson.",Method of moments (statistics),1,0
383,1399085,"Methods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes.

",Methods engineering,1,0
384,1583470,"In statistics a minimum-variance unbiased estimator (MVUE) or uniformly minimum-variance unbiased estimator (UMVUE) is an unbiased estimator that has lower variance than any other unbiased estimator for all possible values of the parameter.
For practical statistics problems, it is important to determine the MVUE if one exists, since less-than-optimal procedures would naturally be avoided, other things being equal. This has led to substantial development of statistical theory related to the problem of optimal estimation.
While combining the constraint of unbiasedness with the desirability metric of least variance leads to good results in most practical settings—making MVUE a natural starting point for a broad range of analyses—a targeted specification may perform better for a given problem; thus, MVUE is not always the best stopping point.",Minimum-variance unbiased estimator,1,1
385,19456533,"Minimum-distance estimation (MDE) is a conceptual method for fitting a statistical model to data, usually the empirical distribution. Often-used estimators such as ordinary least squares can be thought of as special cases of minimum-distance estimation.
While consistent and asymptotically normal, minimum-distance estimators are generally not statistically efficient when compared to maximum likelihood estimators, because they omit the Jacobian usually present in the likelihood function. This, however, substantially reduces the computational complexity of the optimization problem.",Minimum-distance estimation,1,0
386,7859676,"In statistics, missing data, or missing values, occur when no data value is stored for the variable in an observation.  Missing data are a common occurrence and can have a significant effect on the conclusions that can be drawn from the data.
Missing data can occur because of nonresponse: no information is provided for one or more items or for a whole unit (""subject""). Some items are more likely to generate a nonresponse than others: for example items about private subjects such as income. Attrition is a type of missingness that can occur in longitudinal studies—for instance studying development where a measurement is repeated after a certain period of time. Missingness occurs when participants drop out before the test ends and one or more measurements are missing.
Data often are missing in research in economics, sociology, and political science because governments or private entities choose not to, or fail to, report critical statistics, or because the information is not available. Sometimes missing values are caused by the researcher—for example, when data collection is done improperly or mistakes are made in data entry.These forms of missingness take different types, with different impacts on the validity of conclusions from research: Missing completely at random, missing at random, and missing not at random.  Missing data can be handled similarly as censored data.

",Missing data,1,0
387,187859,"Missouri State University (MSU or MO State), formerly Southwest Missouri State University, is a public university in Springfield, Missouri. Founded in 1905 as the Fourth District Normal School, it is the state's second largest university by enrollment, with an enrollment of 22,926 in the fall semester of 2021. The school also operates a campus in West Plains, Missouri offering associate degrees. A bachelor's degree in business is offered at Liaoning Normal University in China. The university also operates a fruit research station in Mountain Grove, Missouri and a Department of Defense and Strategic Studies program in Fairfax, Virginia.",Missouri State University,1,0
388,1217358,"Statistics, when used in a misleading fashion, can trick the casual observer into believing something other than what the data shows. That is, a misuse of statistics occurs when a statistical argument asserts a falsehood. In some cases, the misuse may be accidental. In others, it is purposeful and for the gain of the perpetrator. When the statistical reason involved is false or misapplied, this constitutes a statistical fallacy.
The false statistics trap can be quite damaging for the quest for knowledge.  For example, in medical science, correcting a falsehood may take decades and cost lives.
Misuses can be easy to fall into. Professional scientists, even mathematicians and professional statisticians, can be fooled by even some simple methods, even if they are careful to check everything.  Scientists have been known to fool themselves with statistics due to lack of knowledge of probability theory and lack of standardization of their tests.",Misuse of statistics,1,0
389,5431921,"A mixed model, mixed-effects model or mixed error-component model is a statistical model containing both fixed effects and random effects.  These models are useful in a wide variety of disciplines in the physical, biological and social sciences.
They are particularly useful in settings where repeated measurements are made on the same statistical units (longitudinal study), or where measurements are made on clusters of related statistical units. Because of their advantage in dealing with missing values, mixed effects models are often preferred over more traditional approaches such as repeated measures analysis of variance.
This page will discuss mainly linear mixed-effects models (LMEM) rather than generalized linear mixed models or nonlinear mixed-effects models.",Mixed model,1,0
390,1432127,"The mode is the value that appears most often in a set of data values. If X is a discrete random variable, the mode is the value x  (i.e, X = x) at which the  probability mass function takes its maximum value. In other words, it is the value that is most likely to be sampled.
Like the statistical mean and median, the mode is a way of expressing, in a (usually) single number, important information about a random variable or a population. The numerical value of the mode is the same as that of the mean and median in a normal distribution, and it may be very different in highly skewed distributions.
The mode is not necessarily unique to a given discrete distribution, since the probability mass function may take the same maximum value at several points x1, x2, etc. The most extreme case occurs in uniform distributions, where all values occur equally frequently.
When the probability density function of a continuous distribution has multiple local maxima it is common to refer to all of the local maxima as modes of the distribution. Such a continuous distribution is called multimodal (as opposed to unimodal).  A mode of a continuous probability distribution is often considered to be any value x at which its probability density function has a locally maximum value, so any peak is a mode.In symmetric unimodal distributions, such as the normal distribution, the mean (if defined), median and mode all coincide. For samples, if it is known that they are drawn from a symmetric unimodal distribution, the sample mean can be used as an estimate of the population mode.

",Mode (statistics),1,0
391,3664073,"Model selection is the task of selecting a statistical model from a set of candidate models, given data. In the simplest cases, a pre-existing set of data is considered. However, the task can also involve the design of experiments such that the data collected is well-suited to the problem of model selection. Given candidate models of similar predictive or explanatory power, the simplest model is most likely to be the best choice (Occam's razor).
Konishi & Kitagawa (2008, p. 75) state, ""The majority of the problems in statistical inference can be considered to be problems related to statistical modeling"".  Relatedly, Cox (2006, p. 197) has said, ""How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis"".
Model selection may also refer to the problem of selecting a few representative models from a large set of computational models for the purpose of decision making or optimization under uncertainty.",Model selection,1,0
392,11249626,"In statistics, model specification is part of the process of building a statistical model: specification consists of selecting an appropriate functional form for the model and choosing which variables to include. For example, given personal income 
  
    
      
        y
      
    
    {\displaystyle y}
   together with years of schooling 
  
    
      
        s
      
    
    {\displaystyle s}
   and on-the-job experience 
  
    
      
        x
      
    
    {\displaystyle x}
  , we might specify a functional relationship 
  
    
      
        y
        =
        f
        (
        s
        ,
        x
        )
      
    
    {\displaystyle y=f(s,x)}
   as follows:

  
    
      
        ln
        ⁡
        y
        =
        ln
        ⁡
        
          y
          
            0
          
        
        +
        ρ
        s
        +
        
          β
          
            1
          
        
        x
        +
        
          β
          
            2
          
        
        
          x
          
            2
          
        
        +
        ε
      
    
    {\displaystyle \ln y=\ln y_{0}+\rho s+\beta _{1}x+\beta _{2}x^{2}+\varepsilon }
  where 
  
    
      
        ε
      
    
    {\displaystyle \varepsilon }
   is the unexplained error term that is supposed to comprise independent and identically distributed Gaussian variables. 
The statistician Sir David Cox has said, ""How [the] translation from subject-matter problem to statistical model is done is often the most critical part of an analysis"".",Statistical model specification,1,0
393,368684,"In mathematics, the moments of a function are quantitative measures related to the shape of the function's graph. If the function represents mass, then the first moment is the center of the mass, and the second moment is the rotational inertia. If the function is a probability distribution, then the first moment is the expected value, the second central moment is the variance, the third standardized moment is the skewness, and the fourth standardized moment is the kurtosis. The mathematical concept is closely related to the concept of moment in physics.
For a distribution of mass or probability on a bounded interval, the collection of all the moments (of all orders, from 0 to ∞) uniquely determines the distribution (Hausdorff moment problem).  The same is not true on unbounded intervals (Hamburger moment problem).
In the mid-nineteenth century, Pafnuty Chebyshev became the first person to think systematically in terms of the moments of random variables.

",Moment (mathematics),1,0
394,7615996,"In statistics, the monotone likelihood ratio property is a property of the ratio of two probability density functions (PDFs). Formally, distributions ƒ(x) and g(x) bear the property if

  
    
      
        
          for every 
        
        
          x
          
            1
          
        
        >
        
          x
          
            0
          
        
        ,
        
        
          
            
              f
              (
              
                x
                
                  1
                
              
              )
            
            
              g
              (
              
                x
                
                  1
                
              
              )
            
          
        
        ≥
        
          
            
              f
              (
              
                x
                
                  0
                
              
              )
            
            
              g
              (
              
                x
                
                  0
                
              
              )
            
          
        
      
    
    {\displaystyle {\text{for every }}x_{1}>x_{0},\quad {\frac {f(x_{1})}{g(x_{1})}}\geq {\frac {f(x_{0})}{g(x_{0})}}}
  that is, if the ratio is nondecreasing in the argument 
  
    
      
        x
      
    
    {\displaystyle x}
  .
If the functions are first-differentiable, the property may sometimes be stated

  
    
      
        
          
            ∂
            
              ∂
              x
            
          
        
        
          (
          
            
              
                f
                (
                x
                )
              
              
                g
                (
                x
                )
              
            
          
          )
        
        ≥
        0
      
    
    {\displaystyle {\frac {\partial }{\partial x}}\left({\frac {f(x)}{g(x)}}\right)\geq 0}
  For two distributions that satisfy the definition with respect to some argument x, we say they ""have the MLRP in x."" For a family of distributions that all satisfy the definition with respect to some statistic T(X), we say they ""have the MLR in T(X).""

",Monotone likelihood ratio,1,0
395,4521038,"Multilevel models (also known as hierarchical linear models, linear mixed-effect model, mixed models, nested data models, random coefficient, random-effects models, random parameter models, or split-plot designs) are statistical models of parameters that vary at more than one level. An example could be a model of student performance that contains measures for individual students as well as measures for classrooms within which the students are grouped. These models can be seen as generalizations of linear models (in particular, linear regression), although they can also extend to non-linear models.  These models became much more popular after sufficient computing power and software became available.Multilevel models are particularly appropriate for research designs where data for participants are organized at more than one level (i.e., nested data). The units of analysis are usually individuals (at a lower level) who are nested within contextual/aggregate units (at a higher level). While the lowest level of data in multilevel models is usually an individual, repeated measurements of individuals may also be examined. As such, multilevel models provide an alternative type of analysis for univariate or multivariate analysis of repeated measures. Individual differences in growth curves may be examined. Furthermore, multilevel models can be used as an alternative to ANCOVA, where scores on the dependent variable are adjusted for covariates (e.g. individual differences) before testing treatment differences. Multilevel models are able to analyze these experiments without the assumptions of homogeneity-of-regression slopes that is required by ANCOVA.Multilevel models can be used on data with many levels, although 2-level models are the most common and the rest of this article deals only with these. The dependent variable must be examined at the lowest level of analysis.",Multilevel model,1,0
396,243946,"Multilinear algebra is a subfield of mathematics that extends the methods of linear algebra. Just as linear algebra is built on the concept of a vector and develops the theory of vector spaces, multilinear algebra builds on the concepts of p-vectors and multivectors with Grassmann algebras.

",Multilinear algebra,1,0
397,9444220,"In statistics, the multiple comparisons, multiplicity or multiple testing problem occurs when one considers a set of statistical inferences simultaneously or infers a subset of parameters selected based on the observed values.The more inferences are made, the more likely erroneous inferences become.  Several statistical techniques have been developed to address that problem, typically by requiring a stricter significance threshold for individual comparisons, so as to compensate for the number of inferences being made.",Multiple comparisons problem,1,0
399,18475546,"In statistics, multivariate adaptive regression splines (MARS) is a form of regression analysis introduced by Jerome H. Friedman in 1991. It is a non-parametric regression technique and can be seen as an extension of linear models that automatically models nonlinearities and interactions between variables.
The term ""MARS"" is trademarked and licensed to Salford Systems. In order to avoid trademark infringements, many open-source implementations of MARS are called ""Earth"".",Multivariate adaptive regression spline,1,0
400,682847,"In statistics, multivariate analysis of variance (MANOVA) is a procedure for comparing multivariate sample means. As a multivariate procedure, it is used when there are two or more dependent variables, and is often followed by significance tests involving individual dependent variables separately.",Multivariate analysis of variance,1,0
401,50347,"In probability theory and statistics, the multivariate normal distribution, multivariate Gaussian distribution, or joint normal distribution is a generalization of the one-dimensional (univariate) normal distribution to higher dimensions.  One definition is that a random vector is said to be k-variate normally distributed if every linear combination of its k components has a univariate normal distribution. Its importance derives mainly from the multivariate central limit theorem. The multivariate normal distribution is often used to describe, at least approximately, any set of (possibly) correlated real-valued random variables each of which clusters around a mean value.",Multivariate normal distribution,1,1
403,19384,"Multivariate statistics is a subdivision of statistics encompassing the simultaneous observation and analysis of more than one outcome variable. 
Multivariate statistics concerns understanding the different aims and background of each of the different forms of multivariate analysis, and how they relate to each other. The practical application of multivariate statistics to a particular problem may involve several types of univariate and multivariate analyses in order to understand the relationships between variables and their relevance to the problem being studied.
In addition, multivariate statistics is concerned with multivariate probability distributions, in terms of both

how these can be used to represent the distributions of observed data;
how they can be used as part of statistical inference, particularly where several different quantities are of interest to the same analysis.Certain types of problems involving multivariate data, for example simple linear regression and multiple regression, are not usually considered to be special cases of multivariate statistics because the analysis is dealt with by considering the (univariate) conditional distribution of a single outcome variable given the other variables.

",Multivariate statistics,1,0
404,1378125,"National accounts or national account systems (NAS) are the implementation of complete and consistent accounting techniques for measuring the economic activity of a nation. These include detailed underlying measures that rely on double-entry accounting. By design, such accounting makes the totals on both sides of an account equal even though they each measure different characteristics, for example production and the income from it. As a method, the subject is termed national accounting or, more generally, social accounting. Stated otherwise,  national accounts as systems may be distinguished from the economic data associated with those systems.  While sharing many common principles with business accounting, national accounts are based on economic concepts. One conceptual construct for representing flows of all economic transactions that take place in an economy is a social accounting matrix with accounts in each respective row-column entry.National accounting has developed in tandem with macroeconomics from the 1930s with its relation of aggregate demand to total output through interaction of such broad expenditure categories as consumption and investment. Economic data from national accounts are also used for empirical analysis of economic growth and development.",National accounts,1,0
406,38890,"Natural science is one of the branches of science concerned with the description, understanding and prediction of natural phenomena, based on empirical evidence from observation and experimentation. Mechanisms such as peer review and repeatability of findings are used to try to ensure the validity of scientific advances.
Natural science can be divided into two main branches: life science and physical science. Life science is alternatively known as biology, and physical science is subdivided into branches: physics, chemistry, earth science, and astronomy. These branches of natural science may be further divided into more specialized branches (also known as fields). As empirical sciences, natural sciences use tools from the formal sciences, such as mathematics and logic, converting information about nature into measurements which can be explained as clear statements of the ""laws of nature"".Modern natural science succeeded more classical approaches to natural philosophy, usually traced to Taoists traditions in Asia and in the West to ancient Greece. Galileo, Descartes, Bacon, and Newton debated the benefits of using approaches which were more mathematical and more experimental in a methodical way. Still, philosophical perspectives, conjectures, and presuppositions, often overlooked, remain necessary in natural science. Systematic data collection, including discovery science, succeeded natural history, which emerged in the 16th century by describing and classifying plants, animals, minerals, and so on. Today, ""natural history"" suggests observational descriptions aimed at popular audiences.",Natural science,1,0
407,22788211,"The Nelson–Aalen estimator is a non-parametric estimator of the cumulative hazard rate function in case of censored data or incomplete data. It is used in survival theory, reliability engineering and life insurance to estimate the cumulative number of expected events. An ""event"" can be the failure of a non-repairable component, the death of a human being, or any occurrence for which the experimental unit remains in the ""failed"" state (e.g., death) from the point at which it changed on. The estimator is given by

  
    
      
        
          
            
              H
              ~
            
          
        
        (
        t
        )
        =
        
          ∑
          
            
              t
              
                i
              
            
            ≤
            t
          
        
        
          
            
              d
              
                i
              
            
            
              n
              
                i
              
            
          
        
        ,
      
    
    {\displaystyle {\tilde {H}}(t)=\sum _{t_{i}\leq t}{\frac {d_{i}}{n_{i}}},}
  with 
  
    
      
        
          d
          
            i
          
        
      
    
    {\displaystyle d_{i}}
   the number of events at 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
   and 
  
    
      
        
          n
          
            i
          
        
      
    
    {\displaystyle n_{i}}
   the total individuals at risk at 
  
    
      
        
          t
          
            i
          
        
      
    
    {\displaystyle t_{i}}
  .The curvature of the Nelson–Aalen estimator gives an idea of the hazard rate shape. A concave shape is an indicator for infant mortality while a convex shape indicates wear out mortality.
It can be used for example when testing the homogeneity of Poisson processes.It was constructed by Wayne Nelson and Odd Aalen.",Nelson–Aalen estimator,1,0
408,22072718,"A biological network is a method of representing systems as complex sets of binary interactions or relations between various biological entities.  In general, networks or graphs are used to capture relationships between entities or objects. A typical graphing representation consists of a set of nodes connected by edges.",Biological network,1,0
409,15652764,"Non-linear least squares is the form of least squares analysis used to fit a set of m observations with a model that is non-linear in n unknown parameters (m ≥ n).  It is used in some forms of nonlinear regression.  The basis of the method is to approximate the model by a linear one and to refine the parameters by successive iterations. There are many similarities to linear least squares, but also some significant differences. In economic theory, the non-linear least squares method is applied in (i) the probit regression, (ii) threshold regression, (iii) smooth regression, (iv) logistic link regression, (v) Box-Cox transformed regressors (
  
    
      
        m
        (
        x
        ,
        
          θ
          
            i
          
        
        )
        =
        
          θ
          
            1
          
        
        +
        
          θ
          
            2
          
        
        
          x
          
            (
            
              θ
              
                3
              
            
            )
          
        
      
    
    {\displaystyle m(x,\theta _{i})=\theta _{1}+\theta _{2}x^{(\theta _{3})}}
  ).",Non-linear least squares,1,0
410,1045012,"In statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations.

",Nonlinear regression,1,0
411,4536068,"Nonparametric regression is a category of regression analysis in which the predictor does not take a predetermined form but is constructed according to information derived from the data. That is, no parametric form is assumed for the relationship between predictors and dependent variable. Nonparametric regression requires larger sample sizes than regression based on parametric models because the data must supply the model structure as well as the model estimates.",Nonparametric regression,1,0
412,223369,Nonparametric statistics is the branch of statistics that is not based solely on parametrized families of probability distributions (common examples of parameters are the mean and variance). Nonparametric statistics is based on either being distribution-free or having a specified distribution but with the distribution's parameters unspecified. Nonparametric statistics includes both descriptive statistics and statistical inference. Nonparametric tests are often used when the assumptions of parametric tests are violated.,Nonparametric statistics,1,0
413,21462,"In statistics, a normal distribution (also known as Gaussian, Gauss, or Laplace–Gauss distribution) is a type of continuous probability distribution for a real-valued random variable. The general form of its probability density function is

  
    
      
        f
        (
        x
        )
        =
        
          
            1
            
              σ
              
                
                  2
                  π
                
              
            
          
        
        
          e
          
            −
            
              
                1
                2
              
            
            
              
                (
                
                  
                    
                      x
                      −
                      μ
                    
                    σ
                  
                
                )
              
              
                2
              
            
          
        
      
    
    {\displaystyle f(x)={\frac {1}{\sigma {\sqrt {2\pi }}}}e^{-{\frac {1}{2}}\left({\frac {x-\mu }{\sigma }}\right)^{2}}}
  The parameter 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   is the mean or expectation of the distribution (and also its median and mode), while the parameter 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   is its standard deviation. The variance of the distribution is 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  . A random variable with a Gaussian distribution is said to be normally distributed, and is called a normal deviate.
Normal distributions are important in statistics and are often used in the natural and social sciences to represent real-valued random variables whose distributions are not known. Their importance is partly due to the central limit theorem. It states that, under some conditions, the average of many samples (observations) of a random variable with finite mean and variance is itself a random variable—whose distribution converges to a normal distribution as the number of samples increases. Therefore, physical quantities that are expected to be the sum of many independent processes, such as measurement errors, often have distributions that are nearly normal.Moreover, Gaussian distributions have some unique properties that are valuable in analytic studies. For instance, any linear combination of a fixed collection of normal deviates is a normal deviate. Many results and methods, such as propagation of uncertainty and least squares parameter fitting, can be derived analytically in explicit form when the relevant variables are normally distributed.
A normal distribution is sometimes informally called a bell curve. However, many other distributions are bell-shaped (such as the Cauchy, Student's t, and logistic distributions).
The univariate probability distribution is generalized for vectors in the multivariate normal distribution and for matrices in the matrix normal distribution.",Normal distribution,1,0
414,2142734,"Probability theory and statistics have some commonly used conventions, in addition to standard mathematical notation and mathematical symbols.",Notation in probability and statistics,1,0
415,226673,"In inferential statistics, the null hypothesis (often denoted H0) is that two possibilities are the same. The null hypothesis is that the observed difference is due to chance alone. Using statistical tests, it is possible to calculate the likelihood that the null hypothesis is true.",Null hypothesis,1,0
416,21527,"Number theory (or arithmetic or higher arithmetic in older usage) is a branch of pure mathematics devoted primarily to the study of the integers and integer-valued functions. German mathematician Carl Friedrich Gauss (1777–1855) said, ""Mathematics is the queen of the sciences—and number theory is the queen of mathematics."" Number theorists study prime numbers as well as the properties of mathematical objects made out of integers (for example, rational numbers) or defined as generalizations of the integers (for example, algebraic integers).
Integers can be considered either in themselves or as solutions to equations (Diophantine geometry). Questions in number theory are often best understood through the study of analytical objects (for example, the Riemann zeta function) that encode properties of the integers, primes or other number-theoretic objects in some fashion (analytic number theory). One may also study real numbers in relation to rational numbers, for example, as approximated by the latter (Diophantine approximation).
The older term for number theory is arithmetic. By the early twentieth century, it had been superseded by ""number theory"". (The word ""arithmetic"" is used by the general public to mean ""elementary calculations""; it has also acquired other meanings in mathematical logic, as in Peano arithmetic, and computer science, as in floating-point arithmetic.) The use of the term arithmetic for number theory regained some ground in the second half of the 20th century, arguably in part due to French influence. In particular, arithmetical is commonly preferred as an adjective to number-theoretic.",Number theory,1,0
417,21506,"Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) for the problems of mathematical analysis (as distinguished from discrete mathematics). Numerical analysis finds application in all fields of engineering and the physical sciences, and in the 21st century also the life and social sciences, medicine, business and even the arts. Current growth in computing power has enabled the use of more complex numerical analysis, providing detailed and realistic mathematical models in science and engineering. Examples of numerical analysis include: ordinary differential equations as found in celestial mechanics (predicting the motions of planets, stars and galaxies), numerical linear algebra in data analysis, and stochastic differential equations and Markov chains for simulating living cells in medicine and biology.
Before modern computers, numerical methods often relied on hand interpolation formulas, using data from large printed tables. Since the mid 20th century, computers calculate the required functions instead, but many of the same formulas continue to be used in software algorithms.The numerical point of view goes back to the earliest mathematical writings. A tablet from the Yale Babylonian Collection (YBC 7289), gives a sexagesimal numerical approximation of the square root of 2, the length of the diagonal in a unit square.
Numerical analysis continues this long tradition: rather than giving exact symbolic answers translated into digits and applicable only to real-world measurements, approximate solutions within specified error bounds are used.",Numerical analysis,1,1
418,6326483,"In fields such as epidemiology, social sciences, psychology  and statistics, an observational study draws inferences from a sample to a population where the independent variable is not under the control of the researcher because of ethical concerns or logistical constraints. One common observational study is about the possible effect of a treatment on subjects, where the assignment of subjects into a treated group versus a control group is outside the control of the investigator. This is in contrast with experiments, such as randomized controlled trials, where each subject is randomly assigned to a treated group or a control group. Observational studies, for lacking an assignment mechanism, naturally present difficulties for inferential analysis.

",Observational study,1,0
419,18646178,"Official statistics are statistics published by government agencies or other public bodies such as international organizations as a public good. They provide quantitative or qualitative information on all major areas of citizens' lives, such as economic and social development, living conditions, health, education, and the environment.During the 15th and 16th centuries, statistics were a method for counting and listing populations and State resources. The term statistics comes from the New Latin statisticum collegium (council of state) and refers to science of the state. According to the Organisation for Economic Co-operation and Development, official statistics are statistics disseminated by the national statistical system, excepting those that are explicitly not to be official"".Governmental agencies at all levels, including municipal, county, and state administrations, may generate and disseminate official statistics.  This broader possibility is accommodated by later definitions. For example:

Almost every country in the world has one or more government agencies (usually national institutes) that supply decision-makers and other users including the general public and the research community with a continuing flow of information (...). This bulk of data is usually called official statistics. Official statistics should be objective and easily accessible and produced on a continuing basis so that measurement of change is possible.
Official statistics result from the collection and processing of data into statistical information by a government institution or international organisation. They are then disseminated to help users develop their knowledge about a particular topic or geographical area, make comparisons between countries or understand changes over time. Official statistics make information on economic and social development accessible to the public, allowing the impact of government policies to be assessed, thus improving accountability.

",Official statistics,1,0
420,1361141,"In statistical significance testing, a one-tailed test and a two-tailed test are alternative ways of computing the statistical significance of a parameter inferred from a data set, in terms of a test statistic. A two-tailed test is appropriate if the estimated value is greater or less than a certain range of values, for example, whether a test taker may score above or below a specific range of scores. This method is used for null hypothesis testing and if the estimated value exists in the critical areas, the alternative hypothesis is accepted over the null hypothesis.  
A one-tailed test is appropriate if the estimated value may depart from the reference value in only one direction, left or right, but not both. An example can be whether a machine produces more than one-percent defective products. In this situation, if the estimated value exists in one of the one-sided critical areas, depending on the direction of interest (greater than or less than), the alternative hypothesis is accepted over the null hypothesis. Alternative names are one-sided and two-sided tests; the terminology ""tail"" is used because the extreme portions of distributions, where observations lead to rejection of the null hypothesis, are small and often ""tail off"" toward zero as in the normal distribution, colored in yellow, or ""bell curve"", pictured on the right and colored in green.",One- and two-tailed tests,1,0
421,1993994,"Operations management is an area of management concerned with designing and controlling the process of production and redesigning business operations in the production of goods or services. It involves the responsibility of ensuring that business operations are efficient in terms of using as few resources as needed and effective in meeting customer requirements.  
It is concerned with managing an entire production or service system which is the process that converts inputs (in the forms of raw materials, labor, consumers, and energy) into outputs (in the form of goods and/or services for consumers). Operations produce products, manage quality and create services. Operation management covers sectors like banking systems, hospitals, companies, working with suppliers, customers, and using technology. Operations is one of the major functions in an organization along with supply chains, marketing, finance and human resources. The operations function requires management of both the strategic and day-to-day production of goods and services.
In managing manufacturing or service operations several types of decisions are made including operations strategy, product design, process design, quality management, capacity, facilities planning, production planning and inventory control.  Each of these requires an ability to analyze the current situation and find better solutions to improve the effectiveness and efficiency of manufacturing or service operations. A modern, integrated vision of the many aspects of operations management may be found in recent textbooks on the subject.",Operations management,1,0
422,52444382,"Operations management for services has the functional responsibility for producing the services of an organization and providing them directly to its customers.: 6–7   It specifically deals with decisions required by operations managers for simultaneous production and consumption of an intangible product. These decisions concern the process, people, information and the system that produces and delivers the service.  It differs from operations management in general, since the processes of service organizations differ from those of manufacturing organizations.: 2–7 In a post-industrial economy, service firms provide most of the GDP and employment.  As a result, management of service operations within these service firms is essential for the economy.The services sector treats services as intangible products, service as a customer experience and service as a package of facilitating goods and services.  Significant aspects of service as a product are a basis for guiding decisions made by service operations managers. The extent and variety of services industries in which operations managers make decisions provides the context for decision making.
The six types of decisions made by operations managers in service organizations are: process, quality management, capacity & scheduling, inventory, service supply chain and information technology.",Operations management for services,1,0
423,43476,"Operations research (British English: operational research), often shortened to the initialism OR, is a discipline that deals with the development and application of advanced analytical methods to improve decision-making. It is considered to be a subfield of mathematical sciences. The term management science is occasionally used as a synonym.Employing techniques from other mathematical sciences, such as modeling, statistics, and optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on practical applications, operations research has overlap with many other disciplines, notably industrial engineering. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.",Operations research,1,1
424,277315,"An opinion poll, often simply referred to as a poll or a survey, is a human research survey of public opinion from a particular sample. Opinion polls are usually designed to represent the opinions of a population by conducting a series of questions and then extrapolating generalities in ratio or within confidence intervals. A person who conducts polls is referred to as a pollster.",Opinion poll,1,0
425,16414027,"An optimal decision is a decision that leads to at least as good a known or expected outcome as all other available decision options. It is an important concept in decision theory. In order to compare the different decision outcomes, one commonly assigns a utility value to each of them. 
If there is uncertainty as to what the outcome will be but knowledge about the distribution of the uncertainty, then under the von Neumann–Morgenstern axioms the optimal decision maximizes the expected utility (a probability–weighted average of utility over all possible outcomes of a decision). Sometimes, the equivalent problem of minimizing the expected value of loss is considered, where loss is (–1) times utility. Another equivalent problem is minimizing expected regret. 
""Utility"" is only an arbitrary term for quantifying the desirability of a particular decision outcome and not necessarily related to ""usefulness."" For example, it may well be the optimal decision for someone to buy a sports car rather than a station wagon, if the outcome in terms of another criterion (e.g., effect on personal image) is more desirable, even given the higher cost and lack of versatility of the sports car.
The problem of finding the optimal decision is a mathematical optimization problem. In practice, few people verify that their decisions are optimal, but instead use heuristics to make decisions that are ""good enough""—that is, they engage in satisficing. 
A more formal approach may be used when the decision is important enough to motivate the time it takes to analyze it, or when it is too complex to solve with more simple intuitive approaches, such as many available decision options and a complex decision–outcome relationship.",Optimal decision,1,0
426,1292142,"In the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith.In the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum variance. A non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. In practical terms, optimal experiments can reduce the costs of experimentation.
The optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. Specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments.",Optimal design,1,0
427,160986,"In statistics, the kth order statistic of a statistical sample is equal to its kth-smallest value. Together with rank statistics, order statistics are among the most fundamental tools in non-parametric statistics and inference.
Important special cases of the order statistics are the minimum and maximum value of a sample, and (with some qualifications discussed below) the sample median and other sample quantiles.
When using probability theory to analyze order statistics of random samples from a continuous distribution, the cumulative distribution function is used to reduce the analysis to the case of order statistics of the uniform distribution.",Order statistic,1,0
428,361924,"Order theory is a branch of mathematics that investigates the intuitive notion of order using binary relations. It provides a formal framework for describing statements such as ""this is less than that"" or ""this precedes that"". This article introduces the field and provides basic definitions. A list of order-theoretic terms can be found in the order theory glossary.",Order theory,1,0
429,1651906,"In statistics, ordinary least squares (OLS) is a type of linear least squares method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by the principle of least squares: minimizing the sum of the squares of the differences between the observed dependent variable (values of the variable being observed) in the given dataset and those predicted by the linear function of the independent variable.
Geometrically, this is seen as the sum of the squared distances, parallel to the axis of the dependent variable, between each data point in the set and the corresponding point on the regression surface—the smaller the differences, the better the model fits the data. The resulting estimator can be expressed by a simple formula, especially in the case of a simple linear regression, in which there is a single regressor on the right side of the regression equation.
The OLS estimator is consistent when the regressors are exogenous, and—by the Gauss–Markov theorem—optimal in the class of linear unbiased estimators when the errors are homoscedastic and serially uncorrelated. Under these conditions, the method of OLS provides minimum-variance mean-unbiased estimation when the errors have finite variances. Under the additional assumption that the errors are normally distributed, OLS is the maximum likelihood estimator.",Ordinary least squares,1,1
430,33543938,"Mathematics is a field of study that investigates topics such as number, space, structure, and change.",Outline of mathematics,1,0
431,28651,"Statistics  is a field of inquiry that studies the collection, analysis, interpretation, and presentation of data. It is applicable to a wide variety of academic disciplines, from the physical and social sciences to the humanities; it is also used and misused for making informed decisions in all areas of business and government.",Outline of statistics,1,0
432,554994,"In null-hypothesis significance testing, the p-value is the probability of obtaining test results at least as extreme as the result actually observed, under the assumption that the null hypothesis is correct. A very small p-value means that such an extreme observed outcome would be very unlikely under the null hypothesis. Reporting p-values of statistical tests is common practice in academic publications of many quantitative fields. Since the precise meaning of p-value is hard to grasp, misuse is widespread and has been a major topic in metascience.

",P-value,1,0
433,223366,"Parametric statistics is a branch of statistics which assumes that sample data comes from a population that can be adequately modeled by a probability distribution that has a fixed set of parameters.  Conversely a non-parametric model does not assume an explicit (finite-parametric) mathematical form for the distribution when modeling the data. However, it may make some assumptions about that distribution, such as continuity or symmetry.
Most well-known statistical methods are parametric. Regarding nonparametric (and semiparametric) models, Sir David Cox has said, ""These typically involve fewer assumptions of structure and distributional form but usually contain strong assumptions about independencies"".",Parametric statistics,1,0
434,16862495,"In time series analysis, the partial autocorrelation function (PACF) gives the partial correlation of a stationary time series with its own lagged values, regressed the values of the time series at all shorter lags. It contrasts with the autocorrelation function, which does not control for other lags.
This function plays an important role in data analysis aimed at identifying the extent of the lag in an autoregressive (AR) model. The use of this function was introduced as part of the Box–Jenkins approach to time series modelling, whereby plotting the partial autocorrelative functions one could determine the appropriate lags p in an AR (p) model or in an extended ARIMA (p,d,q) model.

",Partial autocorrelation function,1,0
435,8771567,"In probability theory and statistics, partial correlation measures the degree of association between two random variables, with the effect of a set of controlling random variables removed. If we are interested in finding to what extent there is a numerical relationship between two variables of interest, using their correlation coefficient will give misleading results if there is another, confounding, variable that is numerically related to both variables of interest. This misleading information can be avoided by controlling for the confounding variable, which is done by computing the partial correlation coefficient. This is precisely the motivation for including other right-side variables in a multiple regression; but while multiple regression gives unbiased results for the effect size, it does not give a numerical value of a measure of the strength of the relationship between the two variables of interest.
For example, if we have economic data on the consumption, income, and wealth of various individuals and we wish to see if there is a relationship between consumption and income, failing to control for wealth when computing a correlation coefficient between consumption and income would give a misleading result, since income might be numerically related to wealth which in turn might be numerically related to consumption; a measured correlation between consumption and income might actually be contaminated by these other correlations. The use of a partial correlation avoids this problem.
Like the correlation coefficient, the partial correlation coefficient takes on a value in the range from –1 to 1. The value –1 conveys a perfect negative correlation controlling for some variables (that is, an exact linear relationship in which higher values of one variable are associated with lower values of the other); the value 1 conveys a perfect positive linear relationship, and the value 0 conveys that there is no linear relationship.
The partial correlation coincides with the conditional correlation if the random variables are jointly distributed as the multivariate normal, other elliptical, multivariate hypergeometric, multivariate negative hypergeometric, multinomial or Dirichlet distribution, but not in general otherwise.",Partial correlation,1,0
436,3446092,"The partition of sums of squares is a concept that permeates much of inferential statistics and descriptive statistics. More properly, it is the partitioning of sums of squared deviations or errors. Mathematically, the sum of squared deviations is an unscaled, or unadjusted measure of dispersion (also called variability). When scaled for the number of degrees of freedom, it estimates the variance, or spread of the observations about their mean value. Partitioning of the sum of squared deviations into various components allows the overall variability in a dataset to be ascribed to different types or sources of variability, with the relative importance of each being quantified by the size of each component of the overall sum of squares.",Partition of sums of squares,1,0
437,126706,"Pattern recognition is the automated recognition of patterns and regularities in data. It has applications in statistical data analysis, signal processing, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. Pattern recognition has its origins in statistics and engineering; some modern approaches to pattern recognition include the use of machine learning, due to the increased availability of big data and a new abundance of processing power. These activities can be viewed as two facets of the same field of application, and they have undergone substantial development over the past few decades.
Pattern recognition systems are commonly trained from labeled ""training"" data. When no labeled data are available, other algorithms can be used to discover previously unknown patterns. KDD and data mining have a larger focus on unsupervised methods and stronger connection to business use. Pattern recognition focuses more on the signal and also takes acquisition and Signal Processing into consideration. It originated in engineering, and the term is popular in the context of computer vision: a leading computer vision conference is named Conference on Computer Vision and Pattern Recognition.
In machine learning, pattern recognition is the assignment of a label to a given input value. In statistics, discriminant analysis was introduced for this same purpose in 1936. An example of pattern recognition is classification, which attempts to assign each input value to one of a given set of classes (for example, determine whether a given email is ""spam"" or ""non-spam""). Pattern recognition is a more general problem that encompasses other types of output as well. Other examples are regression, which assigns a real-valued output to each input; sequence labeling, which assigns a class to each member of a sequence of values (for example, part of speech tagging, which assigns a part of speech to each word in an input sentence); and parsing, which assigns a parse tree to an input sentence, describing the syntactic structure of the sentence.Pattern recognition algorithms generally aim to provide a reasonable answer for all possible inputs and to perform ""most likely"" matching of the inputs, taking into account their statistical variation. This is opposed to pattern matching algorithms, which look for exact matches in the input with pre-existing patterns. A common example of a pattern-matching algorithm is regular expression matching, which looks for patterns of a given sort in textual data and is included in the search capabilities of many text editors and word processors.",Pattern recognition,1,1
438,221708,"In statistics, the Pearson correlation coefficient (PCC, pronounced ) ― also known as Pearson's r, the Pearson product-moment correlation coefficient (PPMCC), the bivariate correlation, or colloquially simply as the correlation coefficient ― is a measure of linear correlation between two sets of data. It is the ratio between the covariance of two variables and the product of their standard deviations; thus, it is essentially a normalized measurement of the covariance, such that the result always has a value between −1 and 1. As with covariance itself, the measure can only reflect a linear correlation of variables, and ignores many other types of relationships or correlations. As a simple example, one would expect the age and height of a sample of teenagers from a high school to have a Pearson correlation coefficient significantly greater than 0, but less than 1 (as 1 would represent an unrealistically perfect correlation).",Pearson correlation coefficient,1,1
439,1804023,The Pearson distribution is a family of continuous probability distributions. It was first published by Karl Pearson in 1895 and subsequently extended by him in 1901 and 1916 in a series of articles on biostatistics.,Pearson distribution,1,0
442,354907,"In statistics, a k-th percentile (percentile score or centile) is a score below which a given percentage k of scores in its frequency distribution falls (exclusive definition) or a score at or below which a given percentage falls (inclusive definition). For example, the 50th percentile (the median) is the score below which (exclusive) or at or below which (inclusive) 50% of the scores in the distribution may be found. 
Percentiles are expressed in the same unit of measurement as the input scores; for example, if the scores refer to human weight, the corresponding percentiles will be expressed in kilograms or pounds.
The percentile score and the percentile rank are related terms.  The percentile rank of a score is the percentage of scores in its distribution that are less than it, an exclusive definition, and one that can be expressed with a single, simple formula.
Percentile scores and percentile ranks are often used in the reporting of test scores from norm-referenced tests, but, as just noted, they are not the same. For percentile rank, a score is given and a percentage is computed.  Percentile ranks are exclusive.  If the percentile rank for a specified score is 90%, then 90% of the scores were lower.  In contrast, for percentiles a percentage is given and a corresponding score is determined, which can be either exclusive or inclusive. The score for a specified percentage (e.g., 90th) indicates a score below which (exclusive definition) or at or below which (inclusive definition) other scores in the distribution fall.
The 25th percentile is also known as the first quartile (Q1), the 50th percentile as the median or second quartile (Q2), and the 75th percentile as the third quartile (Q3).

",Percentile,1,0
443,149281,"Performance art is an artwork or art exhibition created through actions executed by the artist or other participants. It may be witnessed live or through documentation, spontaneously developed or written, and is traditionally presented to a public in a fine art context in an interdisciplinary mode. Also known as artistic action, it has been developed through the years as a genre of its own in which art is presented live. It had an important and fundamental role in 20th century avant-garde art.It involves four basic elements: time, space, body, and presence of the artist, and the relation between the creator and the public. The actions, generally developed in art galleries and museums, can take place in the street, any kind of setting or space and during any time period. Its goal is to generate a reaction, sometimes with the support of improvisation and a sense of aesthetics. The themes are commonly linked to life experiences of the artist themselves, or the need of denunciation or social criticism and with a spirit of transformation.The term ""performance art"" and ""performance"" became widely used in the 1970s, even though the history of performance in visual arts dates back to futurist productions and cabarets from the 1910s. The main pioneers of performance art include Carolee Schneemann, Marina Abramović, Ana Mendieta, Chris Burden, Hermann Nitsch, Joseph Beuys, Nam June Paik, Yves Klein and Vito Acconci. Some of the main exponents more recently are Tania Bruguera, Abel Azcona, Regina José Galindo, Tehching Hsieh, Marta Minujín and Petr Pavlensky. The discipline is linked to happening, the Fluxus movement, body art and conceptual art.

",Performance art,1,0
444,2468117,"A permutation test (also called re-randomization test) is an exact statistical hypothesis test making use of the proof by contradiction.
A permutation test involves two or more samples. The null hypothesis is that all samples come from the same distribution 
  
    
      
        
          H
          
            0
          
        
        :
        F
        =
        G
      
    
    {\displaystyle H_{0}:F=G}
  . Under the null hypothesis, the distribution of the test statistic is obtained by calculating all possible values of the test statistic under possible rearrangements of the observed data. Permutation tests are, therefore, a form of resampling.
Permutation tests can be understood as surrogate data testing where the surrogate data under the null hypothesis are obtained through permutations of the original data.In other words, the method by which treatments are allocated to subjects in an experimental design is mirrored in the analysis of that design. If the labels are exchangeable under the null hypothesis, then the resulting tests yield exact significance levels; see also exchangeability. Confidence intervals can then be derived from the tests. The theory has evolved from the works of Ronald Fisher and E. J. G. Pitman in the 1930s.
Permutation tests should not be confused with randomized tests.

",Permutation test,1,0
445,46439,"The philosophy of mathematics is the branch of philosophy that studies the assumptions, foundations, and implications of mathematics. It aims to understand the nature and methods of mathematics, and find out the place of mathematics in people's lives. The logical and structural nature of mathematics itself makes this study both broad and unique among its philosophical counterparts. 
The philosophy of mathematics has two major themes: mathematical realism and mathematical anti-realism.",Philosophy of mathematics,1,0
446,6879051,A line chart or line plot or line graph or curve chart is a type of chart which displays information as a series of data points called 'markers' connected by straight line segments.   It is a basic type of chart common in many fields. It is similar to a scatter plot except that the measurement points are ordered (typically by their x-axis value) and joined with straight line segments.  A line chart is often used to visualize a trend in data over intervals of time – a time series – thus the line is often drawn chronologically. In these cases they are known as run charts.,Line chart,1,0
447,7576966,"Pierre de Fermat (French: [pjɛʁ də fɛʁma]; between 31 October and 6 December 1607 – 12 January 1665) was a French mathematician who is given credit for early developments that led to infinitesimal calculus, including his technique of adequality. In particular, he is recognized for his discovery of an original method of finding the greatest and the smallest ordinates of curved lines, which is analogous to that of differential calculus, then unknown, and his research into number theory. He made notable contributions to analytic geometry, probability, and optics. He is best known for his Fermat's principle for light propagation and his Fermat's Last Theorem in number theory, which he described in a note at the margin of a copy of Diophantus' Arithmetica. He was also a lawyer at the Parlement of Toulouse, France.",Pierre de Fermat,1,0
448,6081205,"In statistics, a pivotal quantity or pivot is a function of observations and unobservable parameters such that the function's probability distribution does not depend on the unknown parameters (including nuisance parameters). A pivot quantity need not be a statistic—the function and its value can depend on the parameters of the model, but its distribution must not. If it is a statistic, then it is known as an ancillary statistic.
More formally, let 
  
    
      
        X
        =
        (
        
          X
          
            1
          
        
        ,
        
          X
          
            2
          
        
        ,
        …
        ,
        
          X
          
            n
          
        
        )
      
    
    {\displaystyle X=(X_{1},X_{2},\ldots ,X_{n})}
   be a random sample from a distribution that depends on a parameter (or vector of parameters) 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . Let 
  
    
      
        g
        (
        X
        ,
        θ
        )
      
    
    {\displaystyle g(X,\theta )}
   be a random variable whose distribution is the same for all 
  
    
      
        θ
      
    
    {\displaystyle \theta }
  . Then 
  
    
      
        g
      
    
    {\displaystyle g}
   is called a pivotal quantity (or simply a pivot).
Pivotal quantities are commonly used for normalization to allow data from different data sets to be compared. It is relatively easy to construct pivots for location and scale parameters: for the former we form differences so that location cancels, for the latter ratios so that scale cancels.
Pivotal quantities are fundamental to the construction of test statistics, as they allow the statistic to not depend on parameters – for example, Student's t-statistic is for a normal distribution with unknown variance (and mean). They also provide one method of constructing confidence intervals, and the use of pivotal quantities improves performance of the bootstrap. In the form of ancillary statistics, they can be used to construct frequentist prediction intervals (predictive confidence intervals).",Pivotal quantity,1,0
449,3763850,"In statistics, resampling is any of a variety of methods for doing one of the following:

Estimating the precision of sample statistics (medians, variances, percentiles) by using subsets of available data (jackknifing) or drawing randomly with replacement from a set of data points (bootstrapping)
Permutation tests (also re-randomization tests) are exact tests: Exchanging labels on data points when performing significance tests
Validating models by using random subsets (bootstrapping, cross validation)

",Resampling (statistics),1,1
450,160379,"In statistics, point estimation involves the use of sample data to calculate a single value (known as a point estimate since it identifies a point in some parameter space) which is to serve as a ""best guess"" or ""best estimate"" of an unknown population parameter (for example, the population mean). More formally, it is the application of a point estimator to the data to obtain a point estimate.
Point estimation can be contrasted with interval estimation: such interval estimates are typically either confidence intervals, in the case of frequentist inference, or credible intervals, in the case of Bayesian inference. More generally, a point estimator can be contrasted with a set estimator. Examples are given by confidence sets or credible sets. A point estimator can also be contrasted with a distribution estimator. Examples are given by confidence distributions, randomized estimators, and Bayesian posteriors.",Point estimation,1,0
451,2708102,"In statistics, Poisson regression is a generalized linear model form of regression analysis used to model count data and contingency tables. Poisson regression assumes the response variable Y has a Poisson distribution, and assumes the logarithm of its expected value can be modeled by a linear combination of unknown parameters. A Poisson regression model is sometimes known as a log-linear model, especially when used to model contingency tables.
Negative binomial regression is a popular generalization of Poisson regression because it loosens the highly restrictive assumption that the variance is equal to the mean made by the Poisson model. The traditional negative binomial regression model is based on the Poisson-gamma mixture distribution. This model is popular because it models the Poisson heterogeneity with a gamma distribution.
Poisson regression models are generalized linear models with the logarithm as the (canonical) link function, and the Poisson distribution function as the assumed probability distribution of the response.

",Poisson regression,1,0
452,24388,"Political science is the scientific study of politics. It is a social science dealing with systems of governance and power, and the analysis of political activities, political thought, political behavior, and associated constitutions and laws.Modern political science can generally be divided into the three subdisciplines of comparative politics, international relations, and political theory. Other notable subdisciplines are public policy and administration, domestic politics and government, political economy, and political methodology. Furthermore, political science is related to, and draws upon, the fields of economics, law, sociology, history, philosophy, human geography, political anthropology, and psychology.
Political science is methodologically diverse and appropriates many methods originating in psychology, social research, and political philosophy. Approaches include positivism, interpretivism, rational choice theory, behaviouralism, structuralism, post-structuralism, realism, institutionalism, and pluralism. Political science, as one of the social sciences, uses methods and techniques that relate to the kinds of inquires sought: primary sources, such as historical documents and official records, secondary sources, such as scholarly journal articles, survey research, statistical analysis, case studies, experimental research, and model building.",Political science,1,0
453,21893202,"In statistics, polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x. Polynomial regression fits a nonlinear relationship between the value of x and the corresponding conditional mean of y, denoted E(y |x). Although polynomial regression fits a nonlinear model to the data, as a statistical estimation problem it is linear, in the sense that the regression function E(y | x) is linear in the unknown parameters that are estimated from the data.  For this reason, polynomial regression is considered to be a special case of multiple linear regression.
The explanatory (independent) variables resulting from the polynomial expansion of the ""baseline"" variables are known as higher-degree terms. Such variables are also used in classification settings.",Polynomial regression,1,1
454,4021127,"Berlin is the most populous city in the European Union, as calculated by city-proper population (not metropolitan area).",Berlin population statistics,1,0
456,357672,"The posterior probability is a type of conditional probability that results from updating the prior probability with information summarized by the likelihood, through an application of Bayes' theorem. From an epistemological perspective, the posterior probability contains everything there is to know about an uncertain proposition (such as a scientific hypothesis, or parameter values), given prior knowledge and a mathematical model describing the observations available at a particular time. After the arrival of new information, the current posterior probability may serve as the prior in another round of Bayesian updating.
In the context of Bayesian statistics, the posterior probability distribution usually describes the epistemic uncertainty about statistical parameters conditional on a collection of observed data. From a given posterior distribution, various point and interval estimates can be derived, such as the maximum a posteriori (MAP) or the highest posterior density interval (HPDI). But while conceptually simple, the posterior distribution is generally not tractable and therefore needs to be either analytically or numerically approximated.

",Posterior probability,1,0
457,238695,"The statistical power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis (
  
    
      
        
          H
          
            0
          
        
      
    
    {\displaystyle H_{0}}
  ) when a specific alternative hypothesis (
  
    
      
        
          H
          
            1
          
        
      
    
    {\displaystyle H_{1}}
  ) is true. It is commonly denoted by 
  
    
      
        1
        −
        β
      
    
    {\displaystyle 1-\beta }
  , and represents the chances of a ""true positive"" detection conditional on the actual existence of an effect to detect. Statistical power ranges from 0 to 1, and as the power of a test increases, the probability 
  
    
      
        β
      
    
    {\displaystyle \beta }
   of making a type II error by wrongly failing to reject the null hypothesis decreases.

",Power of a test,1,1
459,536062,"In statistical inference, specifically predictive inference, a prediction interval is an estimate of an interval in which a future observation will fall, with a certain probability, given what has already been observed. Prediction intervals are often used in regression analysis.
Prediction intervals are used in both frequentist statistics and Bayesian statistics: a prediction interval bears the same relationship to a future observation that a frequentist confidence interval or Bayesian credible interval bears to an unobservable population parameter: prediction intervals predict the distribution of individual future points, whereas confidence intervals and credible intervals of parameters predict the distribution of estimates of the true population mean or other quantity of interest that cannot be observed.",Prediction interval,1,0
460,76340,"The principal components of a collection of points in a real coordinate space are a sequence of 
  
    
      
        p
      
    
    {\displaystyle p}
   unit vectors, where the 
  
    
      
        i
      
    
    {\displaystyle i}
  -th vector is the direction of a line that best fits the data while being orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   vectors. Here, a best-fitting line is defined as one that minimizes the average squared distance from the points to the line. These directions constitute an orthonormal basis in which different individual dimensions of the data are linearly uncorrelated. Principal component analysis (PCA) is the process of computing the principal components and using them to perform a change of basis on the data, sometimes using only the first few principal components and ignoring the rest.
In data analysis, the first principal component of a set of  
  
    
      
        p
      
    
    {\displaystyle p}
   variables, presumed to be jointly normally distributed, is the derived variable formed as a linear combination of the original variables that explains the most variance. The second principal component explains the most variance in what is left once the effect of the first component is removed, and we may proceed through  
  
    
      
        p
      
    
    {\displaystyle p}
   iterations until all the variance is explained. PCA is most commonly used when many of the variables are highly correlated with each other and it is desirable to reduce their number to an independent set.
PCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. The 
  
    
      
        i
      
    
    {\displaystyle i}
  -th principal component can be taken as a direction orthogonal to the first 
  
    
      
        i
        −
        1
      
    
    {\displaystyle i-1}
   principal components that maximizes the variance of the projected data.
For either objective, it can be shown that the principal components are eigenvectors of the data's covariance matrix. Thus, the principal components are often computed by eigendecomposition of the data covariance matrix or singular value decomposition of the data matrix. PCA is the simplest of the true eigenvector-based multivariate analyses and is closely related to factor analysis. Factor analysis typically incorporates more domain specific assumptions about the underlying structure and solves eigenvectors of a slightly different matrix. PCA is also related to canonical correlation analysis (CCA). CCA defines coordinate systems that optimally describe the cross-covariance between two datasets while PCA defines a new orthogonal coordinate system that optimally describes variance in a single dataset. Robust and L1-norm-based variants of standard PCA have also been proposed.",Principal component analysis,1,1
462,472877,"In Bayesian statistical inference, a prior probability distribution, often simply called the prior, of an uncertain quantity is the probability distribution that would express one's beliefs about this quantity before some evidence is taken into account. For example, the prior could be the probability distribution representing the relative proportions of voters who will vote for a particular politician in a future election. The unknown quantity may be a parameter of the model or a latent variable rather than an observable variable.
Bayes' theorem calculates the renormalized pointwise product of the prior and the likelihood function, to produce the posterior probability distribution, which is the conditional distribution of the uncertain quantity given the data.
Similarly, the prior probability of a random event or an uncertain proposition is the unconditional probability that is assigned before any relevant evidence is taken into account.
Priors can be created using a number of methods.: 27–41  A prior can be determined from past information, such as previous experiments. A prior can be elicited from the purely subjective assessment of an experienced expert. An uninformative prior can be created to reflect a balance among outcomes when no information is available. Priors can also be chosen according to some principle, such as symmetry or maximizing entropy given constraints; examples are the Jeffreys prior or Bernardo's reference prior.  When a family of  conjugate priors exists, choosing a prior from that family simplifies calculation of the posterior distribution.
Parameters of prior distributions are a kind of hyperparameter. For example, if one uses a beta distribution to model the distribution of the parameter p of a Bernoulli distribution, then:

p is a parameter of the underlying system (Bernoulli distribution), and
α and β are parameters of the prior distribution (beta distribution); hence hyperparameters.Hyperparameters themselves may have hyperprior distributions expressing beliefs about their values. A Bayesian model with more than one level of prior like this is called a hierarchical Bayes model.",Prior probability,1,0
463,9499804,"Probabilistic design is a discipline within engineering design. It deals primarily with the consideration of the effects of random variability upon the performance of an engineering system during the design phase. Typically, these effects are related to quality and reliability. Thus, probabilistic design is a tool that is mostly used in areas that are  concerned with quality and reliability. For example, product design, quality control, systems engineering, machine design, civil engineering (particularly useful in limit state design) and manufacturing. It differs from the classical approach to design by assuming a small probability of failure instead of using the safety factor.",Probabilistic design,1,0
464,43487,"In probability theory, a probability density function (PDF), or density of a continuous random variable, is a function whose value at any given sample (or point) in the sample space (the set of possible values taken by the random variable) can be interpreted as providing a relative likelihood that the value of the random variable would be close to that sample. Probabililty density  is the  probability per unit length,  in other words, while the absolute likelihood for a continuous random variable to take on any particular value is 0 (since there is an infinite set of possible values to begin with), the value of the PDF at two different samples can be used to infer, in any particular draw of the random variable, how much more likely it is that the random variable would be close to one sample compared to the other sample.
In a more precise sense, the PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range—that is, it is given by the area under the density function but above the horizontal axis and between the lowest and greatest values of the range. The probability density function is nonnegative everywhere, and its integral over the entire space is equal to 1.
The terms ""probability distribution function"" and ""probability function"" have also sometimes been used to denote the probability density function. However, this use is not standard among probabilists and statisticians. In other sources, ""probability distribution function"" may be used when the probability distribution is defined as a function over general sets of values or it may refer to the cumulative distribution function, or it may be a probability mass function (PMF) rather than the density. ""Density function"" itself is also used for the probability mass function, leading to further confusion. In general though, the PMF is used in the context of discrete random variables (random variables that take values on a countable set), while the PDF is used in the context of continuous random variables.

",Probability density function,1,1
466,23538,"The word probability has been used in a variety of ways since it was first applied to the mathematical study of games of chance. Does probability measure the real, physical, tendency of something to occur, or is it a measure of how strongly one believes it will occur, or does it draw on both these elements? In answering such questions, mathematicians interpret the probability values of probability theory.
There are two broad categories of probability interpretations which can be called ""physical"" and ""evidential"" probabilities. Physical probabilities, which are also called objective or frequency probabilities, are associated with random physical systems such as roulette wheels, rolling dice and radioactive atoms. In such systems, a given type of event (such as a die yielding a six) tends to occur at a persistent rate, or ""relative frequency"", in a long run of trials. Physical probabilities either explain, or are invoked to explain, these stable frequencies. The two main kinds of theory of physical probability are frequentist accounts (such as those of Venn, Reichenbach and von Mises) and propensity accounts (such as those of Popper, Miller, Giere and Fetzer).Evidential probability, also called Bayesian probability, can be assigned to any statement whatsoever, even when no random process is involved, as a way to represent its subjective plausibility, or the degree to which the statement is supported by the available evidence. On most accounts, evidential probabilities are considered to be degrees of belief, defined in terms of dispositions to gamble at certain odds. The four main evidential interpretations are the classical (e.g. Laplace's) interpretation, the subjective interpretation (de Finetti and Savage), the epistemic or inductive interpretation (Ramsey, Cox) and the logical interpretation (Keynes and Carnap). There are also evidential interpretations of probability covering groups, which are often labelled as 'intersubjective' (proposed by Gillies and Rowbottom).Some interpretations of probability are associated with approaches to statistical inference, including theories of estimation and hypothesis testing. The physical interpretation, for example, is taken by followers of ""frequentist"" statistical methods, such as Ronald Fisher, Jerzy Neyman and Egon Pearson. Statisticians of the opposing Bayesian school typically accept the frequency interpretation when it makes sense (although not as a definition), but there's less agreement regarding physical probabilities. Bayesians consider the calculation of evidential probabilities to be both valid and necessary in statistics. This article, however, focuses on the interpretations of probability rather than theories of statistical inference.
The terminology of this topic is rather confusing, in part because probabilities are studied within a variety of academic fields. The word ""frequentist"" is especially tricky. To philosophers it refers to a particular theory of physical probability, one that has more or less been abandoned. To scientists, on the other hand, ""frequentist probability"" is just another name for physical (or objective) probability. Those who promote Bayesian inference view ""frequentist statistics"" as an approach to statistical inference that is based on the frequency interpretation of probability, usually relying on the law of large numbers and characterized by what is called 'Null Hypothesis Significance Testing' (NHST). Also the word ""objective"", as applied to probability, sometimes means exactly what ""physical"" means here, but is also used of evidential probabilities that are fixed by rational constraints, such as logical and epistemic probabilities.

It is unanimously agreed that statistics depends somehow on probability. But, as to what probability is and how it is connected with statistics, there has seldom been such complete disagreement and breakdown of communication since the Tower of Babel. Doubtless, much of the disagreement is merely terminological and would disappear under sufficiently sharp analysis.",Probability interpretations,1,0
468,5352267,"Proportional hazards models are a class of survival models in statistics.  Survival models relate the time that passes, before some event occurs, to one or more covariates that may be associated with that quantity of time.  In a proportional hazards model, the unique effect of a unit increase in a covariate is multiplicative with respect to the hazard rate. For example, taking a drug may halve one's hazard rate for a stroke occurring, or, changing the material from which a manufactured component is constructed may double its hazard rate for failure.  Other types of survival models such as accelerated failure time models do not exhibit proportional hazards.  The accelerated failure time model describes a situation where the biological or mechanical life history of an event is accelerated (or decelerated).

",Proportional hazards model,1,0
469,50148,"The prosecutor's fallacy is a fallacy of statistical reasoning involving a test for an occurrence, such as a DNA match. A positive result in the test may paradoxically be more likely to be an erroneous result than an actual occurrence, even if the test is very accurate. The fallacy is named because it is typically used by a prosecutor to exaggerate the probability of a criminal defendant's guilt. The fallacy can be used to support other claims as well – including the innocence of a defendant.
For instance, if a perpetrator were known to have the same blood type as a given defendant and 10% of the population to share that blood type, then one version of the prosecutor's fallacy would be to claim that, on that basis alone, the probability that the defendant is guilty is 90%. However, this conclusion is only close to correct if the defendant was selected as the main suspect based on robust evidence discovered prior to the blood test and unrelated to it (the blood match may then be an ""unexpected coincidence""). Otherwise, the reasoning presented is flawed, as it overlooks the high prior probability (that is, prior to the blood test) that he is a random innocent person. Assume, for instance, that 1000 people live in the town where the murder occurred. This means that 100 people live there who have the perpetrator's blood type; therefore, the true probability that the defendant is guilty – based on the fact that his blood type matches that of the killer – is only 1%, far less than the 90% argued by the prosecutor.
At its heart, therefore, the fallacy involves assuming that the prior probability of a random match is equal to the probability that the defendant is innocent. When using it, a prosecutor questioning an expert witness may ask: ""The odds of finding this evidence on an innocent man are so small that the jury can safely disregard the possibility that this defendant is innocent, correct?"" The claim assumes that the probability that evidence is found on an innocent man is the same as the probability that a man is innocent given that evidence was found on him, which is not true. Whilst the former is usually small (approximately 10% in the previous example) due to good forensic evidence procedures, the latter (99% in that example) does not directly relate to it and will often be much higher, since, in fact, it depends on the likely quite high prior odds of the defendant being a random innocent person.
Mathematically, the fallacy results from misunderstanding the concept of a conditional probability, which is defined as the probability that an event A occurs given that event B is known – or assumed – to have occurred, and it is written as P(A|B). The error is based on assuming that P(A|B) = P(B|A), where A represents the event of finding evidence on the defendant, and B the event that the defendant is innocent. But this equality is not true: in fact, although P(A|B) is usually very small, P(B|A) may still be much higher.",Prosecutor's fallacy,1,0
470,9190286,"In natural and social science research, a protocol is most commonly a predefined procedural method in the design and implementation of an experiment. Protocols are written whenever it is desirable to standardize a laboratory method to ensure successful replication of results by others in the same laboratory or by other laboratories. Additionally, and by extension, protocols have the advantage of facilitating the assessment of experimental results through peer review. In addition to detailed procedures, equipment, and instruments, protocols will also contain study objectives, reasoning for experimental design, reasoning for chosen sample sizes, safety precautions, and how results were calculated and reported, including statistical analysis and any rules for predefining and documenting excluded data to avoid bias.Similarly, a protocol may refer to the procedural methods of health organizations, commercial laboratories, manufacturing plants, etc. to ensure their activities (e.g., blood testing at a hospital, testing of certified reference materials at a calibration laboratory, and manufacturing of transmission gears at a facility) are consistent to a specific standard, encouraging safe use and accurate results.Finally, in the field of social science, a protocol may also refer to a ""descriptive record"" of observed events or a ""sequence of behavior"" of one or more organisms, recorded during or immediately after an activity (e.g., how an infant reacts to certain stimuli or how gorillas behave in natural habitat) to better identify ""consistent patterns and cause-effect relationships."" These protocols may take the form of hand-written journals or electronically documented media, including video and audio capture.

",Protocol (science),1,0
471,23545,"Psychological statistics is application of formulas, theorems, numbers and laws to psychology. 
Statistical methods for psychology include development and application statistical theory and methods for modeling psychological data. 
These methods include psychometrics, factor analysis, experimental designs, and Bayesian statistics. The article also discusses journals in the same field.",Psychological statistics,1,0
472,24982,"Psychometrics is a field of study within psychology concerned with the theory and technique of measurement. Psychometrics generally refers to specialized fields within psychology and education devoted to testing, measurement, assessment, and related activities. Psychometrics is concerned with the objective measurement of latent constructs that cannot be directly observed. Examples of latent constructs include intelligence, introversion, mental disorders, and educational achievement. The levels of individuals on nonobservable latent variables are inferred through mathematical modeling based on what is observed from individuals' responses to items on tests and scales.Practitioners are described as psychometricians, although not all who engage in psychometric research go by this title. Psychometricians usually possess a specific qualification such as degrees or certifications, and most are psychologists with advanced graduate training in psychometrics and measurement theory. In addition to traditional, academic institutions, practitioners also work for organizations such as the Educational Testing Service and Psychological Corporation. Some psychometric researchers focus on the construction and validation of assessment instruments including surveys, scales, and open- or close-ended questionnaires. Others focus on research relating to measurement theory (e.g., item response theory; intraclass correlation) or specialize as learning and development professionals.",Psychometrics,1,0
473,41613,"Quality control (QC) is a process by which entities review the quality of all factors involved in production. ISO 9000 defines quality control as ""A part of quality management focused on fulfilling quality requirements"".This approach places emphasis on three aspects (enshrined in standards such as ISO 9001):
Elements such as controls, job management, defined and well managed processes, performance and integrity criteria, and identification of records
Competence, such as knowledge, skills, experience, and qualifications
Soft elements, such as personnel, integrity, confidence, organizational culture, motivation, team spirit, and quality relationships.Inspection is a major component of quality control, where physical product is examined visually (or the end results of a service are analyzed). Product inspectors will be provided with lists and descriptions of unacceptable product defects such as cracks or surface blemishes for example.

",Quality control,1,0
474,11864322,"A quasi-experiment is an empirical interventional study used to estimate the causal impact of an intervention on target population without random assignment. Quasi-experimental research shares similarities with the traditional experimental design or randomized controlled trial, but it specifically lacks the element of random assignment to treatment or control. Instead, quasi-experimental designs typically allow the researcher to control the assignment to the treatment condition, but using some criterion other than random assignment (e.g., an eligibility cutoff mark).Quasi-experiments are subject to concerns regarding internal validity, because the treatment and control groups may not be comparable at baseline. In other words, it may not be possible to convincingly demonstrate a causal link between the treatment condition and observed outcomes. This is particularly true if there are confounding variables that cannot be controlled or accounted for.With random assignment, study participants have the same chance of being assigned to the intervention group or the comparison group. As a result, differences between groups on both observed and unobserved characteristics would be due to chance, rather than to a systematic factor related to treatment (e.g., illness severity). Randomization itself does not guarantee that groups will be equivalent at baseline. Any change in characteristics post-intervention is likely attributable to the intervention. 

",Quasi-experiment,1,0
475,956483,"A questionnaire is a research instrument that consists of a set of questions (or other types of prompts) for the purpose of gathering information from respondents through survey or statistical study. A research questionnaire is typically a mix of close-ended questions and open-ended questions. Open-ended, long-term questions offer the respondent the ability to elaborate on their thoughts. The Research questionnaire was developed by the Statistical Society of London in 1838.Although questionnaires are often designed for statistical analysis of the responses, this is not always the case.
Questionnaires have advantages over some other types of surveys in that they are cheap, do not require as much effort from the questioner as verbal or telephone surveys, and often have standardized answers that make it simple to compile data. However, such standardized answers may frustrate users as the possible answers may not accurately represent their desired responses. Questionnaires are also sharply limited by the fact that respondents must be able to read the questions and respond to them. Thus, for some demographic groups conducting a survey by questionnaire may not be concretely feasible.",Questionnaire,1,0
476,4031859,"In statistics, a Q–Q plot (quantile-quantile plot) is a probability plot, which is a graphical method for comparing two probability distributions by plotting their quantiles against each other. First, the set of intervals for the quantiles is chosen. A point (x, y) on the plot corresponds to one of the quantiles of the second distribution (y-coordinate) plotted against the same quantile of the first distribution (x-coordinate). Thus the line is a parametric curve with the parameter which is the number of the interval for the quantile.
If the two distributions being compared are similar, the points in the Q–Q plot will approximately lie on the line y = x.  If the distributions are linearly related, the points in the Q–Q plot will approximately lie on a line, but not necessarily on the line y = x.  Q–Q plots can also be used as a graphical means of estimating parameters in a location-scale family of distributions.
A Q–Q plot is used to compare the shapes of distributions, providing a graphical view of how properties such as location, scale, and skewness are similar or different in the two distributions.  Q–Q plots can be used to compare collections of data, or theoretical distributions.  The use of Q–Q plots to compare two samples of data can be viewed as a non-parametric approach to comparing their underlying distributions.  A Q–Q plot is generally a more powerful approach to do this than the common technique of comparing histograms of the two samples, but requires more skill to interpret.  Q–Q plots are commonly used to compare a data set to a theoretical model. This can provide an assessment of ""goodness of fit"" that is graphical, rather than reducing to a numerical summary.  Q–Q plots are also used to compare two theoretical distributions to each other. Since Q–Q plots compare distributions, there is no need for the values to be observed as pairs, as in a scatter plot, or even for the numbers of values in the two groups being compared to be equal.
The term ""probability plot"" sometimes refers specifically to a Q–Q plot, sometimes to a more general class of plots, and sometimes to the less commonly used P–P plot. The probability plot correlation coefficient plot (PPCC plot) is a quantity derived from the idea of Q–Q plots, which measures the agreement of a fitted distribution with observed data and which is sometimes used as a means of fitting a distribution to data.

",Q–Q plot,1,0
477,376707,"R is a programming language for statistical computing and graphics supported by the R Core Team and the R Foundation for Statistical Computing. Created by statisticians Ross Ihaka and Robert Gentleman, R is used among data miners, bioinformaticians and statisticians for data analysis and developing statistical software. Users have created packages to augment the functions of the R language.
According to user surveys and studies of scholarly literature databases, R is one of the most commonly used programming languages used in data mining. As of March 2022, R ranks 11th in the TIOBE index, a measure of programming language popularity, in which the language peaked in 8th place in August 2020.The official R software environment is an open-source free software environment within the GNU package, available under the GNU General Public License. It is written primarily in C, Fortran, and R itself (partially self-hosting). Precompiled executables are provided for various operating systems. R has a command line interface. Multiple third-party graphical user interfaces are also available, such as RStudio, an integrated development environment, and Jupyter, a notebook interface.

",R (programming language),1,0
478,8432961,"A radar chart is a graphical method of displaying multivariate data in the form of a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point. The relative position and angle of the axes is typically uninformative, but various heuristics, such as algorithms that plot data as the maximal total area, can be applied to sort the variables (axes) into relative positions that reveal distinct correlations, trade-offs, and a multitude of other comparative measures.The radar chart is also known as web chart, spider chart, spider graph, spider web chart, star chart, star plot, cobweb chart, irregular polygon, polar chart, or Kiviat diagram. It is equivalent to a parallel coordinates plot, with the axes arranged radially.

",Radar chart,1,0
479,2940730,"Random assignment or random placement is an experimental technique for assigning human participants or animal subjects to different groups in an experiment (e.g., a treatment group versus a control group) using randomization, such as by a chance procedure (e.g., flipping a coin) or a random number generator. This ensures that each participant or subject has an equal chance of being placed in any group. Random assignment of participants helps to ensure that any differences between and within the groups are not systematic at the outset of the experiment. Thus, any differences between groups recorded at the end of the experiment can be more confidently attributed to the experimental procedures or treatment.Random assignment, blinding, and controlling are key aspects of the design of experiments because they help ensure that the results are not spurious or deceptive via confounding. This is why randomized controlled trials are vital in clinical research, especially ones that can be double-blinded and placebo-controlled.
Mathematically, there are distinctions between randomization, pseudorandomization, and quasirandomization, as well as between random number generators and pseudorandom number generators. How much these differences matter in experiments (such as clinical trials) is a matter of trial design and statistical rigor, which affect evidence grading. Studies done with pseudo- or quasirandomization are usually given nearly the same weight as those with true randomization but are viewed with a bit more caution.",Random assignment,1,1
481,160361,"In statistics, quality assurance, and survey methodology, sampling is the selection of a subset (a statistical sample) of individuals from within a statistical population to estimate characteristics of the whole population. Statisticians attempt to collect samples that are representative of the population in question. Sampling has lower costs and faster data collection than measuring the entire population and can provide insights in cases where it is infeasible to measure an entire population. 
Each observation measures one or more properties (such as weight, location, colour) of independent objects or individuals. In survey sampling, weights can be applied to the data to adjust for the sample design, particularly in stratified sampling. Results from probability theory and statistical theory are employed to guide the practice. In business and medical research, sampling is widely used for gathering information about a population. Acceptance sampling is used to determine if a production lot of material meets the governing specifications.",Sampling (statistics),1,1
482,25685,"A random variable (also called random quantity, aleatory variable, or stochastic variable) is a mathematical formalization of a quantity or object which depends on random events. It is a mapping or a function from possible events in a sample space to a measurable space, often the real numbers.

Informally, randomness typically represents some fundamental element of chance, such as in the roll of a dice; it may also represent uncertainty, such as measurement error. However, the interpretation of probability is philosophically complicated, and even in specific cases is not always straightforward. The purely mathematical analysis of random variables is independent of such interpretational difficulties, and can be based upon a rigorous axiomatic setup.
In the formal mathematical language of measure theory, a random variable is defined as a measurable function from a probability measure space (called the sample space) to a measurable space. This allows consideration of the pushforward measure, which is called the distribution of the random variable; the distribution is thus a probability measure on the set of all possible values of the random variable. It is possible for two random variables to have identical distributions but to differ in significant ways; for instance, they may be independent.
It is common to consider the special cases of discrete random variables and absolutely continuous random variables, corresponding to whether a random variable is valued in a discrete set (such as a finite set) or in an interval of real numbers. There are other important possibilities, especially in the theory of stochastic processes, wherein it is natural to consider random sequences or random functions. Sometimes a random variable is taken to be automatically valued in the real numbers, with more general random quantities instead being called random elements.
According to George Mackey, Pafnuty Chebyshev was the first person ""to think systematically in terms of random variables"".",Random variable,1,1
484,9296426,"A randomness test (or test for randomness), in data evaluation, is a test used to analyze the distribution of a set of data to see if it can be described as random (patternless).  In stochastic modeling, as in some computer simulations, the hoped-for randomness of potential input data can be verified, by a formal test for randomness, to show that the data are valid for use in simulation runs.  In some cases, data reveals an obvious non-random pattern, as with so-called ""runs in the data"" (such as expecting random 0–9 but finding ""4 3 2 1 0 4 3 2 1..."" and rarely going above 4).  If a selected set of data fails the tests, then parameters can be changed or other randomized data can be used which does pass the tests for randomness.",Randomness test,1,0
486,163180,"A randomized controlled trial (or randomized control trial; RCT) is a form of scientific experiment used to control factors not under direct experimental control. Examples of RCTs are clinical trials that compare the effects of drugs, surgical techniques, medical devices, diagnostic procedures or other medical treatments.
Participants who enroll in RCTs differ from one another in known and unknown ways that can influence study outcomes, and yet cannot be directly controlled. By randomly allocating participants among compared treatments, an RCT enables statistical control over these influences. Provided it is designed well, conducted properly, and enrolls enough participants, an RCT may achieve sufficient control over these confounding factors to deliver a useful comparison of the treatments studied.",Randomized controlled trial,1,0
487,6033300,"In science, randomized experiments are the experiments that allow the greatest reliability and validity of statistical estimates of treatment effects. Randomization-based inference is especially important in experimental design and in survey sampling.",Randomized experiment,1,0
488,27588,"In statistics, the range of a set of data is the difference between the largest and smallest values.
Difference here is specific, the range of a set of data is the result of  subtracting the sample maximum and minimum.
However, in descriptive statistics, this concept of range has a more complex meaning. The range is the size of the smallest interval (statistics) which contains all the data and provides an indication of statistical dispersion. It is measured in the same units as the data. Since it only depends on two of the observations, it is most useful in representing the dispersion of small data sets.",Range (statistics),1,0
489,3316627,"In statistics, a rank correlation is any of several statistics that measure an ordinal association—the relationship between rankings of different ordinal variables or different rankings of the same variable, where a ""ranking"" is the assignment of the ordering labels ""first"", ""second"", ""third"", etc. to different observations of a particular variable.  A rank correlation coefficient measures the degree of similarity between two rankings, and can be used to assess the significance of the relation between them.  For example, two common nonparametric methods of significance that use rank correlation are the Mann–Whitney U test and the Wilcoxon signed-rank test.",Rank correlation,1,0
490,1482394,"A ranking is a relationship between a set of items such that, for any two items, the first is either ""ranked higher than"", ""ranked lower than"" or ""ranked equal to"" the second.
In mathematics, this is known as a weak order or total preorder of objects. It is not necessarily a total order of objects because two different objects can have the same ranking. The rankings themselves are totally ordered. For example, materials are totally preordered by hardness, while degrees of hardness are totally ordered. If two items are the same in rank it is considered a tie.
By reducing detailed measures to a sequence of ordinal numbers, rankings make it possible to evaluate complex information according to certain criteria. Thus, for example, an Internet search engine may rank the pages it finds according to an estimation of their relevance, making it possible for the user quickly to select the pages they are likely to want to see.
Analysis of data obtained by ranking commonly requires non-parametric statistics.",Ranking,1,0
491,400010,"In statistics, the Rao–Blackwell theorem, sometimes referred to as the Rao–Blackwell–Kolmogorov theorem, is a result which characterizes the transformation of an arbitrarily crude estimator into an estimator that is optimal by the mean-squared-error criterion or any of a variety of similar criteria.
The Rao–Blackwell theorem states that if g(X) is any kind of estimator of a parameter θ, then the conditional expectation of g(X) given T(X), where T is a sufficient statistic, is typically a better estimator of θ, and is never worse. Sometimes one can very easily construct a very crude estimator g(X), and then evaluate that conditional expected value to get an estimator that is in various senses optimal.
The theorem is named after Calyampudi Radhakrishna Rao and David Blackwell.  The process of transforming an estimator using the Rao–Blackwell theorem is sometimes called Rao–Blackwellization. The transformed estimator is called the Rao–Blackwell estimator.

",Rao–Blackwell theorem,1,0
492,26478,"In mathematics, real analysis is the branch of mathematical analysis that studies the behavior of real numbers, sequences and series of real numbers, and real functions. Some particular properties of real-valued sequences and functions that real analysis studies include convergence, limits, continuity, smoothness, differentiability and integrability.
Real analysis is distinguished from complex analysis, which deals with the study of complex numbers and their functions.",Real analysis,1,0
493,53937,"A real data type is a data type used in a computer program to represent an approximation of a real number.
Because the real numbers are not countable, computers cannot represent them exactly using a finite amount of information.
Most often, a computer will use a rational approximation to a real number.",Real data type,1,0
494,26446,"Recreational mathematics is mathematics carried out for recreation (entertainment) rather than as a strictly research and application-based professional activity or as a part of a student's formal education. Although it is not necessarily limited to being an endeavor for amateurs, many topics in this field require no knowledge of advanced mathematics. Recreational mathematics involves mathematical puzzles and games, often appealing to children and untrained adults, inspiring their further study of the subject.The Mathematical Association of America (MAA) includes recreational mathematics as one of its seventeen Special Interest Groups, commenting:

Recreational mathematics is not easily defined because it is more than mathematics done as a diversion or playing games that involve mathematics. Recreational mathematics is inspired by deep ideas that are hidden in puzzles, games, and other forms of play. The aim of the SIGMAA on Recreational Mathematics (SIGMAA-Rec) is to bring together enthusiasts and researchers in the myriad of topics that fall under recreational math. We will share results and ideas from our work, show that real, deep mathematics is there awaiting those who look, and welcome those who wish to become involved in this branch of mathematics.
Mathematical competitions (such as those sponsored by mathematical associations) are also categorized under recreational mathematics.",Recreational mathematics,1,0
495,826997,"In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships between a dependent variable (often called the 'outcome' or 'response' variable, or a 'label' in machine learning parlance) and one or more independent variables (often called 'predictors', 'covariates', 'explanatory variables' or 'features'). The most common form of regression analysis is linear regression, in which one finds the line (or a more complex linear combination) that most closely fits the data according to a specific mathematical criterion. For example, the method of ordinary least squares computes the unique line (or hyperplane) that minimizes the sum of squared differences between the true data and that line (or hyperplane). For specific mathematical reasons (see linear regression), this allows the researcher to estimate the conditional expectation (or population average value) of the dependent variable when the independent variables take on a given set of values. Less common forms of regression use slightly different procedures to estimate alternative location parameters (e.g., quantile regression or Necessary Condition Analysis) or estimate the conditional expectation across a broader collection of non-linear models (e.g., nonparametric regression).
Regression analysis is primarily used for two conceptually distinct purposes. 
First, regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning. 
Second, in some situations regression analysis can be used to infer causal relationships between the independent and dependent variables. Importantly, regressions by themselves only reveal relationships between a dependent variable and a collection of independent variables in a fixed dataset. To use regressions for prediction or to infer causal relationships, respectively, a researcher must carefully justify why existing relationships have predictive power for a new context or why a relationship between two variables has a causal interpretation. The latter is especially important when researchers hope to estimate causal relationships using  observational data.",Regression analysis,1,1
496,15935860,"In statistics, regression validation is the process of deciding whether the numerical results quantifying hypothesized relationships between variables, obtained from regression analysis, are acceptable as descriptions of the data. The validation process can involve analyzing the goodness of fit of the regression, analyzing whether the regression residuals are random, and checking whether the model's predictive performance deteriorates substantially when applied to data that were not used in model estimation.",Regression validation,1,0
497,1724836,"Reliability engineering is a sub-discipline of systems engineering that emphasizes the ability of equipment to function without failure. Reliability describes the ability of a system or component to function under stated conditions for a specified period of time. Reliability is closely related to availability, which is typically described as the ability of a component or system to function at a specified moment or interval of time.
The reliability function is theoretically defined as the probability of success at time t, which is denoted R(t). This probability is estimated from detailed (physics of failure) analysis, previous data sets or through reliability testing and reliability modelling. Availability, testability, maintainability and maintenance are often defined as a part of ""reliability engineering"" in reliability programs. Reliability often plays the key role in the cost-effectiveness of systems.
Reliability engineering deals with the prediction, prevention and management of high levels of ""lifetime"" engineering uncertainty and risks of failure. Although stochastic parameters define and affect reliability, reliability is not only achieved by mathematics and statistics. ""Nearly all teaching and literature on the subject emphasize these aspects, and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement."" For example, it is easy to represent ""probability of failure"" as a symbol or value in an equation, but it is almost impossible to predict its true magnitude in practice, which is massively multivariate, so having the equation for reliability does not begin to equal having an accurate predictive measurement of reliability.
Reliability engineering relates closely to Quality Engineering, safety engineering and system safety, in that they use common methods for their analysis and may require input from each other. It can be said that a system must be reliably safe.
Reliability engineering focuses on costs of failure caused by system downtime, cost of spares, repair equipment, personnel, and cost of warranty claims.",Reliability engineering,1,0
498,10306262,"In engineering, science, and statistics, replication is the repetition of an experimental condition so that the variability associated with the phenomenon can be estimated. ASTM, in standard E1847, defines replication as ""the repetition of the set of all the treatment combinations to be compared in an experiment. Each of the repetitions is called a replicate.""
Replication is not the same as repeated measurements of the same item: they are dealt with differently in statistical experimental design and data analysis.
For proper sampling, a process or batch of products should be in reasonable statistical control; inherent random variation is present but variation due to assignable (special) causes is not. Evaluation or testing of a single item does not allow for item-to-item variation and may not represent the batch or process.  Replication is needed to account for this variation among items and treatments.

",Replication (statistics),1,0
499,44984325,"The replication crisis (also called the replicability crisis and the reproducibility crisis) is an ongoing methodological crisis in which it has been found that the results of many scientific studies are difficult or impossible to reproduce. Because the reproducibility of empirical results is an essential part of the scientific method, such failures undermine the credibility of theories building on them and potentially call into question  substantial parts of scientific knowledge.
The replication crisis is frequently discussed in relation to psychology and medicine, where considerable efforts have been undertaken to re-investigate classic results, to determine both their reliability and, if found unreliable, the reasons for the failure. Data strongly indicate that other natural, and social sciences are affected as well.The phrase replication crisis was coined in the early 2010s as part of a growing awareness of the problem. Considerations around causes and remedies have given rise to a new scientific discipline called metascience, which uses methods of empirical research to examine empirical research practice.
Since empirical research involves both obtaining and analyzing data, considerations about its reproducibility fall into two categories. The validation of the analysis and interpretation of the data obtained in a study runs under the term reproducibility in the narrow sense. The task of repeating the experiment or observational study to obtain new, independent data with the goal of reaching the same or similar conclusions as an original study is called replication.",Replication crisis,1,0
501,2473303,"In statistics, the residual sum of squares (RSS),  also known as the sum of squared residuals (SSR) or the sum of squared estimate of errors (SSE),  is the sum of the squares of residuals (deviations predicted from actual empirical values of data).  It is a measure of the discrepancy between the data and an estimation model, such as a linear regression. A small RSS indicates a tight fit of the model to the data. It is used as an optimality criterion in parameter selection and model selection.
In general, total sum of squares = explained sum of squares + residual sum of squares.  For a proof of this in the multivariate ordinary least squares (OLS) case, see partitioning in the general OLS model.",Residual sum of squares,1,0
502,2713327,"In robust statistics, robust regression is a form of regression analysis designed to overcome some limitations of traditional parametric and non-parametric methods. Regression analysis seeks to find the relationship between one or more independent variables and a dependent variable. Certain widely used methods of regression, such as ordinary least squares, have favourable properties if their underlying assumptions are true, but can give misleading results if those assumptions are not true; thus ordinary least squares is said to be not robust to violations of its assumptions. Robust regression methods are designed to be not overly affected by violations of assumptions by the underlying data-generating process.
In particular, least squares estimates for regression models are highly sensitive to outliers. While there is no precise definition of an outlier, outliers are observations that do not follow the pattern of the other observations. This is not normally a problem if the outlier is simply an extreme observation drawn from the tail of a normal distribution, but if the outlier results from non-normal measurement error or some other violation of standard ordinary least squares assumptions, then it compromises the validity of the regression results if a non-robust regression technique is used.",Robust regression,1,0
503,2885691,"Robust statistics is statistics with good performance for data drawn from a wide range of probability distributions, especially for distributions that are not normal.  Robust statistical methods have been developed for many common problems,  such as estimating location, scale, and regression parameters. One motivation is to produce statistical methods that are not unduly affected by outliers. Another motivation is to provide methods with good performance when there are small departures from a parametric distribution. For example, robust methods work well for mixtures of two normal distributions with different standard-deviations; under this model, non-robust methods like a t-test work poorly.",Robust statistics,1,0
504,140824,"Sir Ronald Aylmer Fisher  (17 February 1890 – 29 July 1962) was a British polymath who was active as a mathematician, statistician, biologist, geneticist, and academic. For his work in statistics, he has been described as ""a genius who almost single-handedly created the foundations for modern statistical science"" and ""the single most important figure in 20th century statistics"". In genetics, his work used mathematics to combine Mendelian genetics and natural selection; this contributed to the revival of Darwinism in the early 20th-century revision of the theory of evolution known as the modern synthesis. For his contributions to biology, Fisher has been called ""the greatest of Darwin’s successors"".Fisher held strong views on race and eugenics, insisting on racial differences. Although he was clearly a eugenist and advocated for the legalization of voluntary sterilization of those with heritable mental disabilities, there is some debate as to whether Fisher supported scientific racism (see Ronald Fisher § Views on Race), and he did not directly advocate for racially discriminatory policies. Notably, he was a dissenting voice in the 1950 UNESCO statement The Race Question.: 27  In his own words: ""Available scientific knowledge provides a firm basis for believing that the groups of mankind differ in their innate capacity for intellectual and emotional development"".: 56  He was the Galton Professor of Eugenics at University College London and editor of the Annals of Eugenics.From 1919, he worked at the Rothamsted Experimental Station for 14 years; there, he analysed its immense body of data from crop experiments since the 1840s, and developed the analysis of variance (ANOVA). He established his reputation there in the following years as a biostatistician.
Together with J. B. S. Haldane and Sewall Wright, Fisher is known as one of the three principal founders of population genetics. He outlined Fisher's principle, the Fisherian runaway and sexy son hypothesis theories of sexual selection.  His contributions to statistics include promoting the method of maximum likelihood and deriving the properties of maximum likelihood estimators, fiducial inference, the derivation of various sampling distributions, founding principles of the design of experiments, and much more.",Ronald Fisher,1,0
505,8648608,"The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample or population values) predicted by a model or an estimator and the values observed. The RMSD represents the square root of the second sample moment of the differences between predicted values and observed values or the quadratic mean of these differences. These deviations are called residuals when the calculations are performed over the data sample that was used for estimation and are called errors (or prediction errors) when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various data points into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular dataset and not between datasets, as it is scale-dependent.RMSD is always non-negative, and a value of 0 (almost never achieved in practice) would indicate a perfect fit to the data. In general, a lower RMSD is better than a higher one. However, comparisons across different types of data would be invalid because the measure is dependent on the scale of the numbers used.
RMSD is the square root of the average of squared errors. The effect of each error on RMSD is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSD. Consequently, RMSD is sensitive to outliers.

",Root-mean-square deviation,1,0
506,6392749,"A run chart, also known as a run-sequence plot is a graph that displays observed data in a time sequence.  Often, the data displayed represent some aspect of the output or performance of a manufacturing or other business process. It is therefore a form of line chart.",Run chart,1,0
507,990677,"SAS (previously ""Statistical Analysis System"") is a statistical software suite developed by SAS Institute for  data management, advanced analytics, multivariate analysis, business intelligence, criminal investigation, and predictive analytics.
SAS was developed at North Carolina State University from 1966 until 1976, when SAS Institute was incorporated. SAS was further developed in the 1980s and 1990s with the addition of new statistical procedures, additional components and the introduction of JMP. A point-and-click interface was added in version 9 in 2004. A social media analytics product was added in 2010.

",SAS (software),1,1
508,28692,"Sabermetrics (or originally as SABRmetrics) is the empirical analysis of baseball, especially baseball statistics that measure in-game activity.
Sabermetricians collect and summarize the relevant data from this in-game activity to answer specific questions. The term is derived from the acronym SABR, which stands for the Society for American Baseball Research, founded in 1971. The term ""sabermetrics"" was coined by Bill James, who is one of its pioneers and is often considered its most prominent advocate and public face.

",Sabermetrics,1,0
510,10005756,"The sample mean (or ""empirical mean"") and the sample covariance are statistics computed from a sample of data on one or more random variables.
The sample mean is the average value (or mean value) of a sample of numbers taken from a larger population of numbers, where ""population"" indicates not number of people but the entirety of relevant data, whether collected or not. A sample of 40 companies' sales from the Fortune 500 might be used for convenience instead of looking at the population, all 500 companies' sales. The sample mean is used as an estimator for the population mean, the average value in the entire population, where the estimate is more likely to be close to the population mean if the sample is large and representative. The reliability of the sample mean is estimated using the standard error, which in turn is calculated using the variance of the sample. If the sample is random, the standard error falls with the size of the sample and the sample mean's distribution approaches the normal distribution as the sample size increases.
The term ""sample mean"" can also be used to refer to a vector of average values when the statistician is looking at the values of several variables in the sample, e.g. the sales, profits, and employees of a sample of Fortune 500 companies. In this case, there is not just a sample variance for each variable but a sample variance-covariance matrix (or simply covariance matrix) showing also the relationship between each pair of variables. This would be a 3×3 matrix when 3 variables are being considered. The sample covariance is useful in judging the reliability of the sample means as estimators and is also useful as an estimate of the population covariance matrix.
Due to their ease of calculation and other desirable characteristics, the sample mean and sample covariance are widely used in statistics to  represent the location and dispersion of the distribution of values in the sample, and to estimate the values for the population.",Sample mean and covariance,1,1
513,1776839,"Sample size determination is the act of choosing the number of observations or replicates to include in a statistical sample. The sample size is an important feature of any empirical study in which the goal is to make inferences about a population from a sample. In practice, the sample size used in a study is usually determined based on the cost, time, or convenience of collecting the data, and the need for it to offer sufficient statistical power. In complicated studies there may be several different sample sizes: for example, in a stratified survey there would be different sizes for each stratum. In a census, data is sought for an entire population, hence the intended sample size is equal to the population. In experimental design, where a study may be divided into different treatment groups, there may be different sample sizes for each group.
Sample sizes may be chosen in several ways:

using experience –  small samples, though sometimes unavoidable, can result in wide confidence intervals and risk of errors in statistical hypothesis testing.
using a target variance for an estimate to be derived from the sample eventually obtained, i.e. if a high precision is required (narrow confidence interval) this translates to a low target variance of the estimator.
using a target for the power of a statistical test to be applied once the sample is collected.
using a confidence level, i.e. the larger the required confidence level, the larger the sample size (given a constant precision requirement).

",Sample size determination,1,1
515,26703,"A statistic (singular) or sample statistic is any quantity computed from values in a sample which is considered for a statistical purpose. Statistical purposes include estimating a population parameter, describing a sample, or evaluating a hypothesis. The average (or mean) of sample values is a statistic. The term statistic is used both for the function and for the value of the function on a given sample. When a statistic is being used for a specific purpose, it may be referred to by a name indicating its purpose.
When a statistic is used for estimating a population parameter, the statistic is called an estimator. A population parameter is any characteristic of a population under study, but when it is not feasible to directly measure the value of a population parameter, statistical methods are used to infer the likely value of the parameter on the basis of a statistic computed from a sample taken from the population. For example, the sample mean is an unbiased estimator of the population mean. This means that the expected value of the sample mean equals the true population mean.A descriptive statistic is used to summarize the sample data. A test statistic is used in statistical hypothesis testing. Note that a single statistic can be used for multiple purposes – for example the sample mean can be used to estimate the population mean, to describe a sample data set, or to test a hypothesis.",Statistic,1,0
516,32344,"In probability theory and statistics, variance is the expectation of the squared deviation of a random variable from its population mean or sample mean. Variance is a measure of dispersion, meaning it is a measure of how far a set of numbers is spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself, and it is often represented by 
  
    
      
        
          σ
          
            2
          
        
      
    
    {\displaystyle \sigma ^{2}}
  , 
  
    
      
        
          s
          
            2
          
        
      
    
    {\displaystyle s^{2}}
  , 
  
    
      
        Var
        ⁡
        (
        X
        )
      
    
    {\displaystyle \operatorname {Var} (X)}
  , 
  
    
      
        V
        (
        X
        )
      
    
    {\displaystyle V(X)}
  , or 
  
    
      
        
          V
        
        (
        X
        )
      
    
    {\displaystyle \mathbb {V} (X)}
  .An advantage of variance as a measure of dispersion is that it is more amenable to algebraic manipulation than other measures of dispersion such as the expected absolute deviation; for example, the variance of a sum of uncorrelated random variables is equal to the sum of their variances. A disadvantage of the variance for practical applications is that, unlike the standard deviation, its units differ from the random variable, which is why the standard deviation is more commonly reported as a measure of dispersion once the calculation is finished.
There are two distinct concepts that are both called ""variance"". One, as discussed above, is part of a theoretical probability distribution and is defined by an equation. The other variance is a characteristic of a set of observations. When variance is calculated from observations, those observations are typically measured from a real world system. If all possible observations of the system are present then the calculated variance is called the population variance. Normally, however, only a subset is available, and the variance calculated from this is called the sample variance. The variance calculated from a sample is considered an estimate of the full population variance. There are multiple ways to calculate an estimate of the population variance, as discussed in the section below.
The two kinds of variance are closely related. To see how, consider that a theoretical probability distribution can be used as a generator of hypothetical observations. If an infinite number of observations are generated using a distribution, then the sample variance calculated from that infinite set will match the value calculated using the distribution's equation for variance.",Variance,1,0
518,520670,"In statistics, a sampling distribution or finite-sample distribution is the probability distribution of a given random-sample-based statistic. If an arbitrarily large number of samples, each involving multiple observations (data points), were separately used in order to compute one value of a statistic (such as, for example, the sample mean or sample variance) for each sample, then the sampling distribution is the probability distribution of the values that the statistic takes on. In many contexts, only one sample is observed, but the sampling distribution can be found theoretically.
Sampling distributions are important in statistics because they provide a major simplification en route to statistical inference. More specifically, they allow analytical considerations to be based on the probability distribution of a statistic, rather than on the joint probability distribution of all the individual sample values.",Sampling distribution,1,0
519,769503,"In probability theory and statistics, a scale parameter is a special kind of numerical parameter of a parametric family of probability distributions. The larger the scale parameter, the more spread out the distribution.

",Scale parameter,1,0
520,412544,"A scatter plot (also called a scatterplot, scatter graph, scatter chart, scattergram, or scatter diagram) is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data. If the points are coded (color/shape/size), one additional variable can be displayed.
The data are displayed as a collection of points, each having the value of one variable determining the position on the horizontal axis and the value of the other variable determining the position on the vertical axis.

",Scatter plot,1,0
521,29507,"Scientific American, informally abbreviated SciAm or sometimes SA, is an American popular science magazine. Many famous scientists, including Albert Einstein, have contributed articles to it. In print since 1845, it is the oldest continuously published monthly magazine in the United States, although it did not become monthly until 1921. Scientific American is owned by Springer Nature, which in turn is a subsidiary of Holtzbrinck Publishing Group.

",Scientific American,1,0
522,1640288,"A scientific control is an experiment or observation designed to minimize the effects of variables other than the independent variable (i.e. confounding variables). This increases the reliability of the results, often through a comparison between control measurements and the other measurements. Scientific controls are a part of the scientific method.",Scientific control,1,0
523,1599160,"In statistics, the score test assesses constraints on statistical parameters based on the gradient of the likelihood function—known as the score—evaluated at the hypothesized parameter value under the null hypothesis. Intuitively, if the restricted estimator is near the maximum of the likelihood function, the score should not differ from zero by more than sampling error. While the finite sample distributions of score tests are generally unknown, they have an asymptotic χ2-distribution under the null hypothesis as first proved by C. R. Rao in 1948, a fact that can be used to determine statistical significance.
Since function maximization subject to equality constraints is most conveniently done using a Lagrangean expression of the problem, the score test can be equivalently understood as a test of the magnitude of the Lagrange multipliers associated with the constraints where, again, if the constraints are non-binding at the maximum likelihood, the vector of Lagrange multipliers should not differ from zero by more than sampling error. The equivalence of these two approaches was first shown by S. D. Silvey in 1959, which led to the name Lagrange multiplier test that has become more commonly used, particularly in econometrics, since Breusch and Pagan's much-cited 1980 paper.The main advantage of the score test over the Wald test and likelihood-ratio test is that the score test only requires the computation of the restricted estimator. This makes testing feasible when the unconstrained maximum likelihood estimate is a boundary point in the parameter space. Further, because the score test only requires the estimation of the likelihood function under the null hypothesis, it is less specific than the other two tests about the precise nature of the alternative hypothesis.",Score test,1,0
524,7579959,"Seasonal adjustment or deseasonalization is a statistical method for removing the seasonal component of a time series. It is usually done when wanting to analyse the trend, and cyclical deviations from trend, of a time series independently of the seasonal components. Many economic phenomena have seasonal cycles, such as agricultural production, (crop yields fluctuate with the seasons) and consumer consumption (increased personal spending leading up to Christmas). It is necessary to adjust for this component in order to understand underlying trends in the economy, so official statistics are often adjusted to remove seasonal components. Typically, seasonally adjusted data is reported for unemployment rates to reveal the underlying trends and cycles in labor markets.",Seasonal adjustment,1,0
525,4536125,"In statistics, semiparametric regression includes regression models that combine parametric  and nonparametric models. They are often used in situations where the fully nonparametric model may not perform well or when the researcher wants to use a parametric model but the functional form with respect to a subset of the regressors or the density of the errors is not known. Semiparametric regression models are a particular type of semiparametric modelling and, since semiparametric models contain a parametric component, they rely on parametric assumptions and may be misspecified and inconsistent, just like a fully parametric model.",Semiparametric regression,1,0
526,27553,"Set theory is the branch of mathematical logic that studies sets, which can be informally described as collections of objects. Although objects of any kind can be collected into a set, set theory, as a branch of mathematics, is mostly concerned with those that are relevant to mathematics as a whole.
The modern study of set theory was initiated by the German mathematicians Richard Dedekind and Georg Cantor in the 1870s. In particular, Georg Cantor is commonly considered the founder of set theory. The non-formalized systems investigated during this early stage go under the name of naive set theory. After the discovery of paradoxes within naive set theory (such as Russell's paradox, Cantor's paradox and Burali-Forti paradox) various axiomatic systems were proposed in the early twentieth century, of which Zermelo–Fraenkel set theory (with or without the axiom of choice) is still the best-known and most studied.
Set theory is commonly employed as a foundational system for the whole of mathematics, particularly in the form of Zermelo–Fraenkel set theory with the axiom of choice. Besides its foundational role, set theory also provides the framework to develop a mathematical theory of infinity, and has various applications in computer science (such as in the theory of relational algebra), philosophy and formal semantics. Its foundational appeal, together with its paradoxes, its implications for the concept of infinity and its multiple applications, have made set theory an area of major interest for logicians and philosophers of mathematics. Contemporary research into set theory covers a vast array of topics, ranging from the structure of the real number line to the study of the consistency of large cardinals.

",Set theory,1,0
527,3099367,"A diversity index is a quantitative measure that reflects how many different types (such as species) there are in a dataset (a community), and that can simultaneously take into account the phylogenetic relations among the individuals distributed among those types, such as richness, divergence or evenness. These indices are statistical  representations of biodiversity in different aspects (richness, evenness, and dominance).",Diversity index,1,0
528,9330700,"In statistics, the concept of the shape of a probability distribution arises in questions of finding an appropriate distribution to use to model the statistical properties of a population, given a sample from that population. The shape of a distribution may be considered either descriptively, using terms such as ""J-shaped"", or numerically, using quantitative measures such as skewness and kurtosis.
Considerations of the shape of a distribution arise in statistical data analysis, where simple quantitative descriptive statistics and plotting techniques such as histograms can lead on to the selection of a particular family of distributions for modelling purposes.",Shape of a probability distribution,1,0
529,9210865,"In probability theory and statistics, a shape parameter (also known as form parameter) is a kind of numerical parameter of a parametric family of probability distributions
that is neither a location parameter nor a scale parameter (nor a function of these, such as a rate parameter). Such a parameter must affect the shape of a distribution rather than simply shifting it (as a location parameter does) or stretching/shrinking it (as a scale parameter does).
For example, ""peakedness"" refers to how round the main peak is.",Shape parameter,1,0
530,2690983,"The Shapiro–Wilk test is a test of normality in frequentist statistics. It was published in 1965 by Samuel Sanford Shapiro and Martin Wilk.

",Shapiro–Wilk test,1,0
531,3319766,"The sign test is a statistical method to test for consistent differences between pairs of observations, such as the weight of subjects before and after treatment. Given pairs of observations (such as weight pre- and post-treatment) for each subject, the sign test determines if one member of the pair (such as pre-treatment) tends to be greater than (or less than) the other member of the pair (such as post-treatment).
The paired observations may be designated x and y. For comparisons of paired observations (x,y), the sign test is most useful if comparisons can only be expressed as x > y, x = y, or x < y. If, instead, the observations can be expressed as numeric quantities (x = 7, y = 18), or as ranks (rank of x = 1st, rank of y = 8th), then the paired t-test
or the Wilcoxon signed-rank test will usually have greater power than the sign test to detect consistent differences.
If X and Y are quantitative variables, the sign test can be used to test the hypothesis that the difference between the X and Y has zero median, assuming continuous distributions of the two random variables X and Y, in the situation when we can draw paired samples from X and Y.The sign test can also test if the median of a collection of numbers is significantly greater than or less than a specified value. For example, given a list of student grades in a class, the sign test can determine if the median grade is significantly different from, say, 75 out of 100.
The sign test is a non-parametric test which makes very few assumptions about the nature of the distributions under test – this means that it has very general applicability but may lack the statistical power of the alternative tests.
The two conditions for the paired-sample sign test are that a sample must be randomly selected from each population, and the samples must be dependent, or paired. 
Independent samples cannot be meaningfully paired. Since the test is nonparametric, the samples need not come from normally distributed populations. Also, the test works for left-tailed, right-tailed, and two-tailed tests.",Sign test,1,0
532,2593771,"In statistics, simple linear regression is a linear regression model with a single explanatory variable. That is, it concerns two-dimensional sample points with one independent variable and one dependent variable (conventionally, the x and y coordinates in a Cartesian coordinate system) and finds a linear function (a non-vertical straight line) that, as accurately as possible, predicts the dependent variable values as a function of the independent variable.
The adjective simple refers to the fact that the outcome variable is related to a single predictor.
It is common to make the additional stipulation that the ordinary least squares (OLS) method should be used: the accuracy of each predicted value is measured by its squared residual (vertical distance between the point of the data set and the fitted line), and the goal is to make the sum of these squared deviations as small as possible. Other regression methods that can be used in place of ordinary least squares include least absolute deviations (minimizing the sum of absolute values of residuals) and the Theil–Sen estimator (which chooses a line whose slope is the median of the slopes determined by pairs of sample points).  Deming regression (total least squares) also finds a line that fits a set of two-dimensional sample points, but (unlike ordinary least squares, least absolute deviations, and median slope regression) it is not really an instance of simple linear regression, because it does not separate the coordinates into one dependent and one independent variable and could potentially return a vertical line as its fit.
The remainder of the article assumes an ordinary least squares regression.
In this case, the slope of the fitted line is equal to the correlation between y and x corrected by the ratio of standard deviations of these variables. The intercept of the fitted line is such that the line passes through the center of mass (x, y) of the data points.",Simple linear regression,1,0
533,250436,"Simultaneous equations models are a type of statistical model in which the dependent variables are functions of other dependent variables, rather than just independent variables. This means some of the explanatory variables are jointly determined with the dependent variable, which in economics usually is the consequence of some underlying equilibrium mechanism. Take the typical supply and demand model: whilst typically one would determine the quantity supplied and demanded to be a function of the price set by the market, it is also possible for the reverse to be true, where producers observe the quantity that consumers demand and then set the price.Simultaneity poses challenges for the estimation of the statistical parameters of interest, because the Gauss–Markov assumption of strict exogeneity of the regressors is violated. And while it would be natural to estimate all simultaneous equations at once, this often leads to a computationally costly non-linear optimization problem even for the simplest system of linear equations. This situation prompted the development, spearheaded by the Cowles Commission in the 1940s and 1950s, of various techniques that estimate each equation in the model seriatim, most notably limited information maximum likelihood and two-stage least squares.",Simultaneous equations model,1,0
534,28212,"In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive, zero, negative, or undefined.
For a unimodal distribution, negative skew commonly indicates that the tail is on the left side of the distribution, and positive skew indicates that the tail is on the right. In cases where one tail is long but the other tail is fat, skewness does not obey a simple rule. For example, a zero value means that the tails on both sides of the mean balance out overall; this is the case for a symmetric distribution, but can also be true for an asymmetric distribution where one tail is long and thin, and the other is short but fat.

",Skewness,1,0
535,373212,"Social research is a research conducted by social scientists following a systematic plan. Social research methodologies can be classified as quantitative and qualitative.
Quantitative designs approach social phenomena through quantifiable evidence, and often rely on statistical analysis of many cases (or across intentionally designed treatments in an experiment) to create valid and reliable general claims. Related to quantity.
Qualitative designs emphasize understanding of social phenomena through direct observation, communication with participants, or analysis of texts, and may stress contextual subjective accuracy over generality. Related to quality.While methods may be classified as quantitative or qualitative, most methods contain elements of both. For example, qualitative data analysis often involves a fairly structured approach to coding the raw data into systematic information, and quantifying intercoder reliability. Thus, there is often a more complex relationship between ""qualitative"" and ""quantitative"" approaches than would be suggested by drawing a simple distinction between them.
Social scientists employ a range of methods in order to analyse a vast breadth of social phenomena: from census survey data derived from millions of individuals, to the in-depth analysis of a single agent's social experiences; from monitoring what is happening on contemporary streets, to the investigation of ancient historical documents. Methods rooted in classical sociology and statistics have formed the basis for research in other disciplines, such as political science, media studies, program evaluation and market research.",Social research,1,0
536,26781,"Social science is one of the branches of science, devoted to the study of societies and the relationships among individuals within those societies. The term was formerly used to refer to the field of sociology, the original ""science of society"", established in the 19th century. In addition to sociology, it now encompasses a wide array of academic disciplines, including anthropology, archaeology, economics, human geography, linguistics, management science, communication science, political science and psychology.Positivist social scientists use methods resembling those of the natural sciences as tools for understanding society, and so define science in its stricter modern sense. Interpretivist social scientists, by contrast, may use social critique or symbolic interpretation rather than constructing empirically falsifiable theories, and thus treat science in its broader sense. In modern academic practice, researchers are often eclectic, using multiple methodologies (for instance, by combining both quantitative and qualitative research). The term social research has also acquired a degree of autonomy as practitioners from various disciplines share the same goals and methods.",Social science,1,0
537,285943,"Social statistics is the use of statistical measurement systems to study human behavior in a social environment.  This can be accomplished through polling a group of people, evaluating a subset of data obtained about a group of people, or by observation and statistical analysis of a set of data that relates to people and their behaviors.

",Social statistics,1,0
538,18717981,"Sociology is a social science that focuses on society, human social behavior, patterns of social relationships, social interaction, and aspects of culture associated with everyday life. It uses various methods of empirical investigation and critical analysis: 3–5  to develop a body of knowledge about social order and social change.: 32–40  While some sociologists conduct research that may be applied directly to social policy and welfare, others focus primarily on refining the theoretical understanding of social processes and phenomenological method. Subject matter can range from micro-level analyses of society (i.e. of individual interaction and agency) to macro-level analyses (i.e. of systems and the social structure).Traditional focuses of sociology include social stratification, social class, social mobility, religion, secularization, law, sexuality, gender, and deviance. As all spheres of human activity are affected by the interplay between social structure and individual agency, sociology has gradually expanded its focus to other subjects and institutions, such as health and the institution of medicine; economy; military; punishment and systems of control; the Internet; sociology of education; social capital; and the role of social activity in the development of scientific knowledge.
The range of social scientific methods has also expanded, as social researchers draw upon a variety of qualitative and quantitative techniques. The linguistic and cultural turns of the mid-20th century, especially, have led to increasingly interpretative, hermeneutic, and philosophical approaches towards the analysis of society. Conversely, the turn of the 21st century has seen the rise of new analytically, mathematically, and computationally rigorous techniques, such as agent-based modelling and social network analysis.Social research has influence throughout various industries and sectors of life, such as among politicians, policy makers, and legislators; educators; planners; administrators; developers; business magnates and managers; social workers; non-governmental organizations; and non-profit organizations, as well as individuals interested in resolving social issues in general. As such, there is often a great deal of crossover between social research, market research, and other statistical fields.",Sociology,1,0
539,3190431,"Spatial analysis or spatial statistics includes any of the formal techniques  which studies entities using their topological, geometric, or geographic properties. Spatial analysis includes a variety of techniques, many still in their early development, using different analytic approaches and applied in fields as diverse as astronomy, with its studies of the placement of galaxies in the cosmos, to chip fabrication engineering, with its use of ""place and route"" algorithms to build complex wiring structures. In a more restricted sense, spatial analysis is the technique applied to structures at the human scale, most notably in the analysis of geographic data or transcriptomics data.
Complex issues arise in spatial analysis, many of which are neither clearly defined nor completely resolved, but form the basis for current research. The most fundamental of these is the problem of defining the spatial location of the entities being studied.
Classification of the techniques of spatial analysis is difficult because of the large number of different fields of research involved, the different fundamental approaches which can be chosen, and the many forms the data can take.",Spatial analysis,1,1
541,235623,"In statistics, Spearman's rank correlation coefficient or Spearman's ρ, named after Charles Spearman and often denoted by the Greek letter 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   (rho) or as 
  
    
      
        
          r
          
            s
          
        
      
    
    {\displaystyle r_{s}}
  , is a nonparametric measure of rank correlation (statistical dependence between the rankings of two variables). It assesses how well the relationship between two variables can be described using a monotonic function.
The Spearman correlation between two variables is equal to the Pearson correlation between the rank values of those two variables; while Pearson's correlation assesses linear relationships, Spearman's correlation assesses monotonic relationships (whether linear or not). If there are no repeated data values, a perfect Spearman correlation of +1 or −1 occurs when each of the variables is a perfect monotone function of the other.
Intuitively, the Spearman correlation between two variables will be high when observations have a similar (or identical for a correlation of 1) rank (i.e. relative position label of the observations within the variable: 1st, 2nd, 3rd, etc.) between the two variables, and low when observations have a dissimilar (or fully opposed for a correlation of −1) rank between the two variables.
Spearman's coefficient is appropriate for both continuous and discrete ordinal variables. Both Spearman's 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   and Kendall's 
  
    
      
        τ
      
    
    {\displaystyle \tau }
   can be formulated as special cases of a more general correlation coefficient.",Spearman's rank correlation coefficient,1,1
543,49607,"Jargon is the specialized terminology associated with a particular field or area of activity. Jargon is normally employed in a particular communicative context and may not be well understood outside that context. The context is usually a particular occupation (that is, a certain trade, profession, vernacular or academic field), but any ingroup can have jargon. The main trait that distinguishes jargon from the rest of a language is special vocabulary—including some words specific to it and often different senses or meanings of words, that outgroups would tend to take in another sense—therefore misunderstanding that communication attempt. Jargon is sometimes understood as a form of technical slang and then distinguished from the official terminology used in a particular field of activity.The terms jargon, slang and argot are not consistently differentiated in the literature; different authors interpret these concepts in varying ways. According to one definition, jargon differs from slang in being secretive in nature; according to another understanding, it is specifically associated with professional and technical circles. Some sources, however, treat these terms as synonymous. In Russian linguistics, jargon is classified as an expressive form of language, while secret languages are referred to as argots. One thing is true about these terms: they come from a desire to differentiate between groups of people.

",Jargon,1,0
544,13070117,"In statistical signal processing, the goal of spectral density estimation (SDE) is to estimate the spectral density (also known as the power spectral density) of a random signal from a sequence of time samples of the signal. Intuitively speaking, the spectral density characterizes the frequency content of the signal. One purpose of estimating the spectral density is to detect any periodicities in the data, by observing peaks at the frequencies corresponding to these periodicities.
Some SDE techniques assume that a signal is composed of a limited (usually small) number of generating frequencies plus noise and seek to find the location and intensity of the generated frequencies.  Others make no assumption on the number of components and seek to estimate the whole generating spectrum.",Spectral density estimation,1,0
545,27590,"In statistics, the standard deviation is a measure of the amount of variation or dispersion of a set of values. A low standard deviation indicates that the values tend to be close to the mean (also called the expected value) of the set, while a high standard deviation indicates that the values are spread out over a wider range.
Standard deviation may be abbreviated SD, and is most commonly represented in mathematical texts and equations by the lower case Greek letter σ (sigma), for the population standard deviation, or the Latin letter s, for the sample standard deviation.
The standard deviation of a random variable, sample, statistical population, data set, or probability distribution is the square root of its variance. It is algebraically simpler, though in practice less robust, than the average absolute deviation. A useful property of the standard deviation is that, unlike the variance, it is expressed in the same unit as the data.
The standard deviation of a population or sample and the standard error of a statistic (e.g., of the sample mean) are quite different, but related. The sample mean's standard error is the standard deviation of the set of means that would be found by drawing an infinite number of repeated samples from the population and computing a mean for each sample.  The mean's standard error turns out to equal the population standard deviation divided by the square root of the sample size, and is estimated by using the sample standard deviation divided by the square root of the sample size. For example, a poll's standard error (what is reported as the margin of error of the poll), is the expected standard deviation of the estimated mean if the same poll were to be conducted multiple times. Thus, the standard error estimates the standard deviation of an estimate, which itself measures how much the estimate depends on the particular sample that was taken from the population. 
In science, it is common to report both the standard deviation of the data (as a summary statistic) and the standard error of the estimate (as a measure of potential error in the findings). By convention, only effects more than two standard errors away from a null expectation are considered  ""statistically significant"", a safeguard against spurious conclusion that is really due to random sampling error.  
When only a sample of data from a population is available, the term standard deviation of the sample or sample standard deviation can refer to either the above-mentioned quantity as applied to those data, or to a modified quantity that is an unbiased estimate of the population standard deviation (the standard deviation of the entire population).",Standard deviation,1,0
546,552520,"The standard error (SE) of a statistic (usually an estimate of a parameter) is the standard deviation of its sampling distribution or an estimate of that standard deviation. If the statistic is the sample mean, it is called the standard error of the mean (SEM).The sampling distribution of a mean is generated by repeated sampling from the same population and recording of the sample means obtained. This forms a distribution of different means, and this distribution has its own mean and variance. Mathematically, the variance of the sampling distribution obtained is equal to the variance of the population divided by the sample size. This is because as the sample size increases, sample means cluster more closely around the population mean. 
Therefore, the relationship between the standard error of the mean and the standard deviation is such that, for a given sample size, the standard error of the mean equals the standard deviation divided by the square root of the sample size. In other words, the standard error of the mean is a measure of the dispersion of sample means around the population mean. 
In regression analysis, the term ""standard error"" refers either to the square root of the reduced chi-squared statistic or the standard error for a particular regression coefficient (as used in, say, confidence intervals).",Standard error,1,1
548,357356,"The Stanford Encyclopedia of Philosophy (SEP) combines an online encyclopedia of philosophy with peer-reviewed publication of original papers in philosophy, freely accessible to Internet users. It is maintained by Stanford University. Each entry is written and maintained by an expert in the field, including professors from many academic institutions worldwide. Authors contributing to the encyclopedia give Stanford University the permission to publish the articles, but retain the copyright to those articles.",Stanford Encyclopedia of Philosophy,1,0
549,65704,"Stanley Smith Stevens (November 4, 1906 – January 18, 1973) was an American psychologist who founded Harvard's Psycho-Acoustic Laboratory, studying psychoacoustics, and he is credited with the introduction of Stevens's power law. Stevens authored a milestone textbook, the 1400+ page Handbook of Experimental Psychology (1951). He was also one of the founding organizers of the Psychonomic Society.  In 1946 he introduced a theory of levels of measurement widely used by scientists but whose use in some areas of statistics has been criticized. In addition, Stevens played a key role in the development of the use of operational definitions in psychology.A Review of General Psychology survey, published in 2002, ranked Stevens as the 52nd most cited psychologist of the 20th century.

",Stanley Smith Stevens,1,0
550,329898,"In mathematics and statistics, a stationary process (or a strict/strictly stationary process or strong/strongly stationary process) is a stochastic process whose unconditional joint probability distribution does not change when shifted in time. Consequently, parameters such as mean and variance also do not change over time. To get an intuition of stationarity, one can imagine a frictionless pendulum. It swings back and forth in an oscillatory motion, yet the amplitude and frequency remain constant. Although the pendulum is moving, the process is stationary as its ""statistics"" are constant (frequency and amplitude). However, if a force were to be applied to the pendulum (for example, friction with the air), either the frequency or amplitude would change, thus making the process non-stationary.Since stationarity is an assumption underlying many statistical procedures used in time series analysis, non-stationary data are often transformed to become stationary. The most common cause of violation of stationarity is a trend in the mean, which can be due either to the presence of a unit root or of a deterministic trend. In the former case of a unit root, stochastic shocks have permanent effects, and the process is not mean-reverting. In the latter case of a deterministic trend, the process is called a trend-stationary process, and stochastic shocks have only transitory effects after which the variable tends toward a deterministically evolving (non-constant) mean.
A trend stationary process is not strictly stationary, but can easily be transformed into a stationary process by removing the underlying trend, which is solely a function of time. Similarly, processes with one or more unit roots can be made stationary through differencing. An important type of non-stationary process that does not include a trend-like behavior is a cyclostationary process, which is a stochastic process that varies cyclically with time.
For many applications strict-sense stationarity is too restrictive. Other forms of stationarity such as wide-sense stationarity or N-th-order stationarity are then employed. The definitions for different kinds of stationarity are not consistent among different authors (see Other terminology).",Stationary process,1,0
552,1645218,"Statistical Methods for Research Workers is a classic book on statistics, written by the statistician R. A. Fisher. It is considered by some to be one of the 20th century's most influential books on statistical methods, together with his The Design of Experiments (1935). It was originally published in 1925, by Oliver & Boyd (Edinburgh); the final and posthumous 14th edition was published in 1970.",Statistical Methods for Research Workers,1,0
553,1579244,"In statistics, classification is the problem of identifying which of a set of categories (sub-populations) an observation (or observations) belongs to.  Examples are assigning a given email to the ""spam"" or ""non-spam"" class, and assigning a diagnosis to a given patient based on observed characteristics of the patient (sex, blood pressure, presence or absence of certain symptoms, etc.).
Often, the individual observations are analyzed into a set of quantifiable properties, known variously as explanatory variables or features.  These properties may variously be categorical (e.g. ""A"", ""B"", ""AB"" or ""O"", for blood type), ordinal (e.g. ""large"", ""medium"" or ""small""), integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
An algorithm that implements classification, especially in a concrete implementation, is known as a classifier.  The term ""classifier"" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable.  In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), and the possible categories to be predicted are classes.  Other fields may use different terminology: e.g. in community ecology, the term ""classification"" normally refers to cluster analysis.",Statistical classification,1,1
554,542306,"A consultant (from Latin: consultare ""to deliberate"") is a professional (also known as expert, specialist, see variations of meaning below) who provides advice and other purposeful activities in an area of specialization.Consulting services generally fall under the domain of professional services.",Consultant,1,0
555,901126,"Kaohsiung Metro (Chinese: 高雄大眾捷運系統, 高雄捷運) is a rapid transit and light rail system covering the metropolitan area of Kaohsiung, Taiwan. Its rapid transit network is known as Kaohsiung Mass Rapid Transit or Kaohsiung MRT. Construction of the MRT started in October 2001. The MRT opened in 2008 and the Circular light rail in 2015. Kaohsiung Metro is operated by the Kaohsiung Rapid Transit Corporation (KRTC; Chinese: 高雄捷運公司) under a BOT contract the company signed with the Kaohsiung City Government.
Two Kaohsiung Metro stations, Formosa Boulevard and Central Park, were ranked among the top 50 most beautiful subway systems in the world by Metrobits.org in 2011. In 2012, the two stations respectively are ranked as the 2nd and the 4th among the top 15 most beautiful subway stops in the world by BootsnAll.The system uses romanizations derived from Tongyong Pinyin.",Kaohsiung Metro,1,0
556,35269346,"In statistics, groups of individual data points may be classified as belonging to any of various statistical data types, e.g. categorical (""red"", ""blue"", ""green""), real number (1.68, -5, 1.7e+6), odd number(1,3,5) etc.  The data type is a fundamental component of the semantic content of the variable, and controls which sorts of probability distributions can logically be used to describe the variable, the permissible operations on the variable, the type of regression analysis used to predict the variable, etc.  The concept of data type is similar to the concept of level of measurement, but more specific: For example, count data require a different distribution (e.g. a Poisson distribution or binomial distribution) than non-negative real-valued data require, but both fall under the same level of measurement (a ratio scale).
Various attempts have been made to produce a taxonomy of levels of measurement. The psychophysicist Stanley Smith Stevens defined nominal, ordinal, interval, and ratio scales. Nominal measurements do not have meaningful rank order among values, and permit any one-to-one transformation. Ordinal measurements have imprecise differences between consecutive values, but have a meaningful order to those values, and permit any order-preserving transformation. Interval measurements have meaningful distances between measurements defined, but the zero value is arbitrary (as in the case with longitude and temperature measurements in degree Celsius or degree Fahrenheit), and permit any linear transformation. Ratio measurements have both a meaningful zero value and the distances between different measurements defined, and permit any rescaling transformation.
Because variables conforming only to nominal or ordinal measurements cannot be reasonably measured numerically, sometimes they are grouped together as categorical variables, whereas ratio and interval measurements are grouped together as quantitative variables, which can be either discrete or continuous, due to their numerical nature. Such distinctions can often be loosely correlated with data type in computer science, in that dichotomous categorical variables may be represented with the Boolean data type, polytomous categorical variables with arbitrarily assigned integers in the integral data type, and continuous variables with the real data type involving floating point computation. But the mapping of computer science data types to statistical data types depends on which categorization of the latter is being implemented. 
Other categorizations have been proposed. For example, Mosteller and Tukey (1977) distinguished grades, ranks, counted fractions, counts, amounts, and balances. Nelder (1990) described continuous counts, continuous ratios, count ratios, and categorical modes of data. See also Chrisman (1998), van den Berg (1991).The issue of whether or not it is appropriate to apply different kinds of statistical methods to data obtained from different kinds of measurement procedures is complicated by issues concerning the transformation of variables and the precise interpretation of research questions. ""The relationship between the data and what they describe merely reflects the fact that certain kinds of statistical statements may have truth values which are not invariant under some transformations. Whether or not a transformation is sensible to contemplate depends on the question one is trying to answer"" (Hand, 2004, p. 82).",Statistical data type,1,0
557,446216,"Decision theory (or the theory of choice; not to be confused with choice theory) is a branch of applied probability theory concerned with the theory of making decisions based on assigning probabilities to various factors and assigning numerical consequences to the outcome.There are three branches of decision theory:

Normative decision theory: Concerned with the identification of optimal decisions, where optimality is often determined by considering an ideal decision-maker who is able to calculate with perfect accuracy and is in some sense fully rational.
Prescriptive decision theory: Concerned with describing observed behaviors through the use of conceptual models, under the assumption that those making the decisions are behaving under some consistent rules.
Descriptive decision theory: Analyzes how individuals actually make the decisions that they do.Decision theory is closely related to the field of game theory and is an interdisciplinary topic, studied by economists, mathematicians, data scientists, psychologists, biologists, political and other social scientists, philosophers and computer scientists.
Empirical applications of this theory are usually done with the help of statistical and econometric methods.",Decision theory,1,0
558,23773247,"In statistics, dispersion (also called variability, scatter, or spread) is the extent to which a distribution is stretched or squeezed. Common examples of measures of statistical dispersion are the variance, standard deviation, and interquartile range. For instance, when the variance of data in a set is large, the data is widely scattered. On the other hand, when the variance is small, the data in the set is clustered.
Dispersion is contrasted with location or central tendency, and together they are the most used properties of distributions.",Statistical dispersion,1,0
559,5187620,"In statistics, probability theory, and information theory, a statistical distance quantifies the distance between two statistical objects, which can be two random variables, or two probability distributions or samples, or the distance can be between an individual sample point and a population or a wider sample of points.
A distance between populations can be interpreted as measuring the distance between two probability distributions and hence they are essentially measures of distances between probability measures. Where statistical distance measures relate to the differences between random variables, these may have statistical dependence, and hence these distances are not directly related to measures of distances between probability measures.  Again, a measure of distance between random variables may relate to the extent of dependence between them, rather than to their individual values.
Statistical distance measures are not typically metrics, and they need not be symmetric. Some types of distance measures, which generalize squared distance, are referred to as (statistical) divergences.",Statistical distance,1,0
560,15934463,"Statistical graphics, also known as statistical graphical techniques, are graphics used in the field of statistics for data visualization.",Statistical graphics,1,0
563,3223282,"Statistical literacy is the ability to understand and reason with statistics and data. The abilities to understand and reason with data, or arguments that use data, are necessary for citizens to understand material presented in publications such as newspapers, television, and the Internet. However, scientists also need to develop statistical literacy so that they can both produce rigorous and reproducible research and consume it. Numeracy is an element of being statistically literate and in some models of statistical literacy, or for some populations (e.g., students in kindergarten through 12th grade/end of secondary school), it is a prerequisite skill. Being statistically literate is sometimes taken to include having the abilities to both critically evaluate statistical material and appreciate the relevance of statistically-based approaches to all aspects of life in general or to the evaluating, design, and/or production of scientific work.",Statistical literacy,1,0
564,28481,"In physics, statistical mechanics is a mathematical framework that applies statistical methods and probability theory to large assemblies of microscopic entities. It does not assume or postulate any natural laws, but explains the macroscopic behavior of nature from the behavior of such ensembles.
Statistical mechanics arose out of the development of classical thermodynamics, a field for which it was successful in explaining macroscopic physical properties—such as temperature, pressure, and heat capacity—in terms of microscopic parameters that fluctuate about average values and are characterized by probability distributions. This established the fields of statistical thermodynamics and statistical physics.
The founding of the field of statistical mechanics is generally credited to three physicists:

Ludwig Boltzmann, who developed the fundamental interpretation of entropy in terms of a collection of microstates
James Clerk Maxwell, who developed models of probability distribution of such states
Josiah Willard Gibbs, who coined the name of the field in 1884While classical thermodynamics is primarily concerned with thermodynamic equilibrium, statistical mechanics has been applied in non-equilibrium statistical mechanics to the issues of microscopically modeling the speed of irreversible processes that are driven by imbalances. Examples of such processes include chemical reactions and flows of particles and heat. The fluctuation–dissipation theorem is the basic knowledge obtained from applying non-equilibrium statistical mechanics to study the simplest non-equilibrium situation of a steady state current flow in a system of many particles.

",Statistical mechanics,1,0
565,27576,"A statistical model is a mathematical model that embodies a set of statistical assumptions concerning the generation of sample data (and similar data from a larger population). A statistical model represents, often in considerably idealized form, the data-generating process.A statistical model is usually specified as a mathematical relationship between one or more random variables and other non-random variables.  As such, a statistical model is ""a formal representation of a theory"" (Herman Adèr quoting Kenneth Bollen).All statistical hypothesis tests and all statistical estimators are derived via statistical models. More generally, statistical models are part of the foundation of statistical inference.",Statistical model,1,1
566,413291,"In statistics, as opposed to its general use in mathematics, a parameter is any measured quantity of a statistical population that summarises or describes an aspect of the population, such as a mean or a standard deviation. If a population exactly follows a known and defined distribution, for example the normal distribution, then a small set of parameters can be measured which completely describes the population, and can be considered to define a probability distribution for the purposes of extracting samples from this population.
A parameter is to a population as a statistic is to a sample; that is to say, a parameter describes the true value calculated from the full population, whereas a statistic is an estimated measurement of the parameter based on a subsample. Thus a ""statistical parameter"" can be more specifically referred to as a population parameter.

",Statistical parameter,1,0
567,27585,"In statistics, a population is a set of similar items or events which is of interest for some question or experiment.  A statistical population can be a group of existing objects (e.g. the set of all stars within the Milky Way galaxy) or a hypothetical and potentially infinite group of objects conceived as a generalization from experience (e.g. the set of all possible hands in a game of poker). A common aim of statistical analysis is to produce information about some chosen population.In statistical inference, a subset of the population (a statistical sample) is chosen to represent the population in a statistical analysis. Moreover, the statistical sample must be unbiased and accurately model the population (every unit of the population has an equal chance of selection). The ratio of the size of this statistical sample to the size of the population is called a sampling fraction. It is then possible to estimate the population parameters using the appropriate sample statistics.

",Statistical population,1,0
569,593680,"Statistical process control (SPC) is a method of quality control which employs statistical methods to monitor and control a process. This helps to ensure that the process operates efficiently, producing more specification-conforming products with less waste (rework or scrap). SPC can be applied to any process where the ""conforming product"" (product meeting specifications) output can be measured. Key tools used in SPC include run charts, control charts, a focus on continuous improvement, and the design of experiments. An example of a process where SPC is applied is manufacturing lines.
SPC must be practiced in two phases: The first phase is the initial establishment of the process, and the second phase is the regular production use of the process. In the second phase, a decision of the period to be examined must be made, depending upon the change in 5M&E conditions (Man, Machine, Material, Method, Movement, Environment) and wear rate of parts used in the manufacturing process (machine parts, jigs, and fixtures).
An advantage of SPC over other methods of quality control, such as ""inspection"", is that it emphasizes early detection and prevention of problems, rather than the correction of problems after they have occurred.
In addition to reducing waste, SPC can lead to a reduction in the time required to produce the product. SPC makes it less likely the finished product will need to be reworked or scrapped.",Statistical process control,1,0
570,160995,"In statistical hypothesis testing, a result has statistical significance when it is very unlikely to have occurred given the null hypothesis. More precisely, a study's defined significance level, denoted by 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  , is the probability of the study rejecting the null hypothesis, given that the null hypothesis is true; and the p-value of a result, 
  
    
      
        p
      
    
    {\displaystyle p}
  , is the probability of obtaining a result at least as extreme, given that the null hypothesis is true. The result is statistically significant, by the standards of the study, when 
  
    
      
        p
        ≤
        α
      
    
    {\displaystyle p\leq \alpha }
  . The significance level for a study is chosen before data collection, and is typically set to 5% or much lower—depending on the field of study.In any experiment or observation that involves drawing a sample from a population, there is always the possibility that an observed effect would have occurred due to sampling error alone. But if the p-value of an observed effect is less than (or equal to) the significance level, an investigator may conclude that the effect reflects the characteristics of the whole population, thereby rejecting the null hypothesis.This technique for testing the statistical significance of results was developed in the early 20th century. The term significance does not imply importance here, and the term statistical significance is not the same as research significance, theoretical significance, or practical significance. For example, the term clinical significance refers to the practical importance of a treatment effect.

",Statistical significance,1,0
571,10854000,"Energy statistics refers to collecting, compiling, analyzing and disseminating data on commodities such as coal, crude oil, natural gas, electricity, or renewable energy sources (biomass, geothermal, wind or solar energy), when they are used for the energy they contain. Energy is the capability of some substances, resulting from their physico-chemical properties, to do work or produce heat. Some energy commodities, called fuels, release their energy content as heat when they burn. This heat could be used to run an internal or external combustion engine.

The need to have statistics on energy commodities became obvious during the 1973 oil crisis that brought tenfold increase in petroleum prices. Before the crisis, to have accurate data on global energy supply and demand was not deemed critical. Another concern of energy statistics today is a huge gap in energy use between developed and developing countries. As the gap narrows (see picture), the pressure on energy supply increases tremendously. 
The data on energy and electricity come from three principal sources:

Energy industry
Other industries (""self-producers"")
ConsumersThe flows of and trade in energy commodities are measured both in physical units (e.g., metric tons), and, when energy balances are calculated, in energy units (e.g., terajoules or tons of oil equivalent). What makes energy statistics specific and different from other fields of economic statistics is the fact that energy commodities undergo greater number of transformations (flows) than other commodities. In these transformations energy is conserved, as defined by and within the limitations of the first and second laws of thermodynamics.",Statistical study of energy data,1,0
572,272134,"Survey methodology is ""the study of survey methods"".
As a field of applied statistics concentrating on human-research surveys, survey methodology studies the sampling of individual units from a population and associated techniques of survey data collection, such as questionnaire construction and methods for improving the number and accuracy of responses to surveys. Survey methodology targets instruments or procedures that ask one or more questions that may or may not be answered.
Researchers carry out statistical surveys with a view towards making statistical inferences about the population being studied; such inferences depend strongly on the survey questions used.  Polls about public opinion, public-health surveys, market-research surveys, government surveys and censuses all exemplify quantitative research that uses survey methodology to answer questions about a  population. Although censuses do not include a ""sample"", they do include other aspects of survey methodology, like questionnaires, interviewers, and non-response follow-up techniques. Surveys provide important information for all kinds of public-information and research fields, such as marketing research, psychology, health-care provision and sociology.

",Survey methodology,1,1
573,27579,"The theory of statistics provides a basis for the whole range of techniques, in both study design and data analysis, that are used within applications of statistics. The theory covers approaches to statistical-decision problems and to statistical inference, and the actions and deductions that satisfy  the basic principles stated for these different approaches. Within a given approach, statistical theory gives ways of comparing statistical procedures; it can find a best possible procedure within a given context for given statistical problems, or can provide guidance on the choice between alternative procedures.Apart from philosophical considerations about how to make statistical inferences and decisions, much of statistical theory consists of mathematical statistics, and is closely linked to probability theory, to utility theory, and to optimization.",Statistical theory,1,0
574,48661,"A statistician is a person who works with theoretical or applied statistics. The profession exists in both the private and public sectors. 
It is common to combine statistical knowledge with expertise in other subjects, and statisticians may work as employees or as statistical consultants.

",Statistician,1,1
576,24985094,"Statistics education is the practice of teaching and learning of statistics, along with the associated scholarly research.
Statistics is both a formal science and a practical theory of scientific inquiry, and both aspects are considered in statistics education. Education in statistics has similar concerns as does education in other mathematical sciences, like logic, mathematics, and computer science. At the same time, statistics is concerned with evidence-based reasoning, particularly with the analysis of data. Therefore, education in statistics has strong similarities to education in empirical disciplines like psychology and chemistry, in which education is closely tied to ""hands-on"" experimentation.
Mathematicians and statisticians often work in a department of mathematical sciences (particularly at colleges and small universities). Statistics courses have been sometimes taught by non-statisticians, against the recommendations of some professional organizations of statisticians and of mathematicians.
Statistics education research is an emerging field that grew out of different disciplines and is currently establishing itself as a unique field that is devoted to the improvement of teaching and learning statistics at all educational levels.",Statistics education,1,0
577,977649,"A stem-and-leaf display or stem-and-leaf plot is a device for  presenting quantitative data in a graphical format, similar to a histogram, to assist in visualizing the shape of a distribution. They evolved from Arthur Bowley's work in the early 1900s, and are useful tools in exploratory data analysis. Stemplots became more commonly used in the 1980s after the publication of John Tukey's book on exploratory data analysis in 1977. The popularity during those years is attributable to their use of monospaced (typewriter) typestyles that allowed computer technology of the time to easily produce the graphics. Modern computers' superior graphic capabilities have meant these techniques are less often used.
This plot has been implemented in Octave and R.A stem-and-leaf plot is also called a stemplot, but the latter term often refers to another chart type.  A simple stem plot may refer to plotting a matrix of y values onto a common x axis, and identifying the common x value with a vertical line, and the individual y values with symbols on the line.Unlike histograms, stem-and-leaf displays retain the original data to at least two significant digits, and put the data in order, thereby easing the move to order-based inference and non-parametric statistics.",Stem-and-leaf display,1,0
578,228001,"Stochastic calculus is a branch of mathematics that operates on stochastic processes. It allows a consistent theory of integration to be defined for integrals of stochastic processes with respect to stochastic processes. This field was created and started by the Japanese mathematician Kiyoshi Itô during World War II.
The best-known stochastic process to which stochastic calculus is applied is the Wiener process (named in honor of Norbert Wiener), which is used for modeling Brownian motion as described by Louis Bachelier in 1900 and by Albert Einstein in 1905 and other physical diffusion processes in space of particles subject to random forces.  Since the 1970s, the Wiener process has been widely applied in financial mathematics and economics to model the evolution in time of stock prices and bond interest rates.
The main flavours of stochastic calculus are the Itô calculus and its variational relative the Malliavin calculus.  For technical reasons the Itô integral is the most useful for general classes of processes, but the related Stratonovich integral is frequently useful in problem formulation (particularly in engineering disciplines). The Stratonovich integral can readily be expressed in terms of the Itô integral.  The main benefit of the Stratonovich integral is that it obeys the usual chain rule and therefore does not require Itô's lemma. This enables problems to be expressed in a coordinate system invariant form, which is invaluable when developing stochastic calculus on manifolds other than Rn.
The dominated convergence theorem does not hold for the Stratonovich integral; consequently it is very difficult to prove results without re-expressing the integrals in Itô form.",Stochastic calculus,1,0
579,8979437,"Stochastic approximation methods are a family of iterative methods typically used for root-finding problems or for optimization problems. The recursive update rules of stochastic approximation methods can be used, among other things, for solving linear systems when the collected data is corrupted by noise, or for approximating extreme values of functions which cannot be computed directly, but only estimated via noisy observations. 
In a nutshell, stochastic approximation algorithms deal with a function of the form 
  
    
      
        f
        (
        θ
        )
        =
        
          E
          
            ξ
          
        
        ⁡
        [
        F
        (
        θ
        ,
        ξ
        )
        ]
      
    
    {\textstyle f(\theta )=\operatorname {E} _{\xi }[F(\theta ,\xi )]}
  
which is the expected value of a function depending on a random variable 
  
    
      
        ξ
      
    
    {\textstyle \xi }
  . The goal is to recover properties of such a function 
  
    
      
        f
      
    
    {\textstyle f}
   without evaluating it directly. Instead, stochastic approximation algorithms use random samples of 
  
    
      
        F
        (
        θ
        ,
        ξ
        )
      
    
    {\textstyle F(\theta ,\xi )}
   to efficiently approximate properties of 
  
    
      
        f
      
    
    {\textstyle f}
   such as zeros or extrema.
Recently, stochastic approximations have found extensive applications in the  fields of statistics and machine learning, especially in settings with big data. These applications range from stochastic optimization methods and algorithms,  to online forms of the  EM algorithm, reinforcement learning via temporal differences, and deep learning, and others.
Stochastic approximation algorithms have also been used in the social sciences to describe collective dynamics: fictitious play in learning theory and consensus algorithms can be studied using their theory.The earliest, and prototypical, algorithms of this kind are the Robbins–Monro and Kiefer–Wolfowitz algorithms introduced respectively in 1951 and 1952.",Stochastic approximation,1,0
580,292222,"Stochastic (from Greek  στόχος (stókhos) 'aim, guess') refers to the property of being well described by a random probability distribution. Although stochasticity and randomness are distinct in that the former refers to a modeling approach and the latter refers to phenomena themselves, these two terms are often used synonymously. Furthermore, in probability theory, the formal concept of a stochastic process is also referred to as a random process.Stochasticity is used in many different fields, including the natural sciences such as biology, chemistry, ecology,  neuroscience, and physics, as well as technology and engineering fields such as image processing, signal processing, information theory, computer science, cryptography, and telecommunications. It is also used in finance, due to seemingly random changes in financial markets as well as in medicine, linguistics,  music, media, colour theory,  botany, manufacturing, and geomorphology. Stochastic modeling is also used in social science.

",Stochastic,1,0
581,27596,"In statistics, stratified sampling is a method of sampling from a population which can be partitioned into subpopulations.

In statistical surveys, when subpopulations within an overall population vary, it could be advantageous to sample each subpopulation (stratum) independently. Stratification is the process of dividing members of the population into homogeneous subgroups before sampling. The strata should define a partition of the population. That is, it should be collectively exhaustive and mutually exclusive: every element in the population must be assigned to one and only one stratum. Then simple random sampling is applied within each stratum. The objective is to improve the precision of the sample by reducing sampling error. It can produce a weighted mean that has less variability than the arithmetic mean of a simple random sample of the population.
In computational statistics, stratified sampling is a method of variance reduction when Monte Carlo methods are used to estimate population statistics from a known population.

",Stratified sampling,1,0
582,14528980,"In econometrics and statistics, a structural break is an unexpected change over time in the parameters of regression models, which can lead to huge forecasting errors and unreliability of the model in general. This issue was popularised by David Hendry, who argued that lack of stability of coefficients frequently caused forecast failure, and therefore we must routinely test for structural stability.  Structural stability − i.e., the time-invariance of regression coefficients − is a central issue in all applications of linear regression models.",Structural break,1,0
583,2007748,"Structural equation modeling (SEM) is a label for a diverse set of methods used by scientists in both experimental and observational research across the sciences, business, and other fields. It is used most in the social and behavioral sciences. A definition of SEM is difficult without reference to highly technical language, but a good starting place is the name itself.
SEM involves the construction of a model, an informative representation of some observable or theoretical phenomenon. In this model, different aspects of a phenomenon are theorized to be related to one another with a structure. This structure is a system of equations, but it is usually designed on paper or using a computer with arrows and symbols (also known as path notation as shown in Figure 1). The structure implies statistical and often causal relationships between variables, error terms and can include multiple equations. The equation (or equations) in SEM are mathematical and statistical properties that are implied by the model and its structural features, and then estimated with statistical algorithms (usually based on matrix algebra and generalized linear models) using experimental or observational data.
Criticisms of SEM methods hint at mathematical formulation problems, a tendency to accept models without establishing external validity, and philosophical bias inherent in the standard procedures.Although there are not always clear boundaries of what is and what is not SEM, it generally involves path models (see also path analysis) and measurement models (see also factor analysis) and always employs statistical models and computer programs to investigate the structural connections between latent variables underlying the actual variables taken from observed data.The SEM toolkit includes confirmatory factor analysis, confirmatory composite analysis, path analysis, multi-group modeling, longitudinal modeling, partial least squares path modeling, latent growth modeling and hierarchical or multi-level modeling. Use of SEM is commonly justified in the social sciences because it is a way to identify latent variables that are believed to exist, but cannot be directly observed in reality.Researchers using SEM employ software programs to estimate the strength and sign of a coefficient for each modeled arrow (the numbers shown in Figure 1 for example), and to provide diagnostic clues suggesting which indicators or model components might produce inconsistency between the model and the data. 
A hypothetical model suggesting that intelligence (as measured by four questions) can predict academic performance (as measured by SAT, ACT, and high school GPA) is shown in Figure 1. The concept of human intelligence cannot be measured directly in the way that one could measure height or weight. Instead, researchers have a theory and conceptualization of intelligence and then design measurement instruments such as a questionnaire or test that provides them with multiple indicators of intelligence. These indicators are then combined in a model to create a plausible way of measuring intelligence as a latent variable (the circle for intelligence in Figure 1) from the indicators (square boxes with scale 1–4 in Figure 1).In SEM diagrams, latent variables are commonly shown as ovals and observed variables as rectangles. The diagram above shows how error (e) influences each intelligence question and the SAT, ACT, and GPA scores, but does not influence the latent variables. When applying this model to observed data generated from the instruments, the researcher can recover a measure of intelligence and academic performance for each individual observed with the instruments with a margin of error that is implied by the instruments. The researcher can then use intelligence to test a hypothesis, for example that intelligence causes academic performance (which is another latent variable in Figure 1) defined by a path model drawing an arrow from intelligence to performance. Figure 1 is therefore a general example of a SEM involving measurement of latent variables and estimation of a hypothesized effect between at least one latent variable and another observed or latent variable (in this case latent academic performance).
A great advantage of SEM is that all of these measurements and tests occur simultaneously in one statistical estimation procedure, where the errors throughout the model are calculated using all information from the model. This means the errors are more accurate than if a researcher were to calculate each part of the model separately.

",Structural equation modeling,1,1
585,17813659,"Structured data analysis is the statistical data analysis of structured data. This can arise either in the form of an a priori structure such as multiple-choice questionnaires or in situations with the need to search for structure that fits the given data, either exactly or approximately. This structure can then be used for making comparisons, predictions, manipulations etc.",Structured data analysis (statistics),1,0
586,105375,"In probability and statistics, Student's  t-distribution (or simply the t-distribution) is any member of a family of continuous probability distributions that arise when estimating the mean of a normally distributed population in situations where the sample size is small and the population's standard deviation is unknown. It was developed by English statistician William Sealy Gosset under the pseudonym ""Student"".
The t-distribution plays a role in a number of widely used statistical analyses, including Student's t-test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the difference between two population means, and in linear regression analysis. Student's t-distribution also arises in the Bayesian analysis of data from a normal family.
If we take a sample of 
  
    
      
        n
      
    
    {\displaystyle n}
   observations from a normal distribution, then the t-distribution with 
  
    
      
        ν
        =
        n
        −
        1
      
    
    {\displaystyle \nu =n-1}
   degrees of freedom can be defined as the distribution of the location of the sample mean relative to the true mean, divided by the sample standard deviation, after multiplying by the standardizing term 
  
    
      
        
          
            n
          
        
      
    
    {\displaystyle {\sqrt {n}}}
  . In this way, the t-distribution can be used to construct a confidence interval for the true mean.
The t-distribution is symmetric and bell-shaped, like the normal distribution. However, the t-distribution has heavier tails, meaning that it is more prone to producing values that fall far from its mean. This makes it useful for understanding the statistical behavior of certain types of ratios of random quantities, in which variation in the denominator is amplified and may produce outlying values when the denominator of the ratio falls close to zero.  The Student's t-distribution is a special case of the generalized hyperbolic distribution.",Student's t-distribution,1,0
589,140841,"In statistics, a statistic is sufficient with respect to a statistical model and its associated unknown parameter if ""no other statistic that can be calculated from the same sample provides any additional information as to the value of the parameter"". In particular, a statistic is sufficient for a family of probability distributions if the sample from which it is calculated gives no additional information than the statistic, as to which of those probability distributions is the sampling distribution.
A related concept is that of linear sufficiency, which is weaker than sufficiency but can be applied in some cases where there is no sufficient statistic, although it is restricted to linear estimators. The Kolmogorov structure function deals with individual finite data; the related notion there is the algorithmic sufficient statistic.
The concept is due to Sir Ronald Fisher in 1920. Stephen Stigler noted in 1973 that the concept of sufficiency had fallen out of favor in descriptive statistics because of the strong dependence on an assumption of the distributional form (see Pitman–Koopman–Darmois theorem below), but remained very important in theoretical work.",Sufficient statistic,1,1
591,27587,"In descriptive statistics, summary statistics are used to summarize a set of observations, in order to communicate the largest amount of information as simply as possible. Statisticians commonly try to describe the observations in

a measure of location, or central tendency, such as the arithmetic mean
a measure of statistical dispersion like the standard mean absolute deviation
a measure of the shape of the distribution like skewness or kurtosis
if more than one variable is measured, a measure of statistical dependence such as a correlation coefficientA common collection of order statistics used as summary statistics are the five-number summary, sometimes extended to a seven-number summary, and the associated box plot.
Entries in an analysis of variance table can also be regarded as summary statistics.: 378 ",Summary statistics,1,0
593,27578,"In statistics, survey sampling describes the process of selecting a sample of elements from a target population to conduct a survey. 
The term ""survey"" may refer to many different types or techniques of observation.  In survey sampling it most often involves a questionnaire used to measure the characteristics and/or attitudes of people. Different ways of contacting members of a sample once they have been selected is the subject of survey data collection. The purpose of sampling is to reduce the cost and/or the amount of work that it would take to survey the entire target population.  A survey that measures the entire target population is called a census. A sample refers to a group or section of a population from which information is to be obtained
Survey samples can be broadly divided into two types: probability samples and super samples.  Probability-based samples implement a sampling plan with specified probabilities (perhaps adapted probabilities specified by an adaptive procedure). Probability-based sampling allows design-based inference about the target population. The inferences are based on a known objective probability distribution that was specified in the study protocol. Inferences from probability-based surveys may still suffer from many types of bias.
Surveys that are not based on probability sampling have greater difficulty measuring their bias or sampling error.  Surveys based on non-probability samples often fail to represent the people in the target population.In academic and government survey research, probability sampling is a standard procedure. In the United States, the Office of Management and Budget's ""List of Standards for Statistical Surveys"" states that federally funded surveys must be performed:

selecting samples using generally accepted statistical methods (e.g., probabilistic methods that can provide estimates of sampling error). Any use of nonprobability sampling methods (e.g., cut-off or model-based samples) must be justified statistically and be able to measure estimation error.
Random sampling and design-based inference are supplemented by other statistical methods, such as model-assisted sampling and model-based sampling.For example, many surveys have substantial amounts of nonresponse. Even though the units are initially chosen with known probabilities, the nonresponse mechanisms are unknown. For surveys with substantial nonresponse, statisticians have proposed statistical models with which the data sets are analyzed.
Issues related to survey sampling are discussed in several sources, including Salant and Dillman (1994).",Survey sampling,1,0
594,419259,"Survival analysis is a branch of statistics for analyzing the expected duration of time until one event occurs, such as death in biological organisms and failure in mechanical systems. This topic is called reliability theory or reliability analysis in engineering, duration analysis or duration modelling in economics, and event history analysis in sociology. Survival analysis attempts to answer certain questions, such as what is the proportion of a population which will survive past a certain time? Of those that survive, at what rate will they die or fail? Can multiple causes of death or failure be taken into account? How do particular circumstances or characteristics increase or decrease the probability of survival?
To answer such questions, it is necessary to define ""lifetime"". In the case of biological survival, death is unambiguous, but for mechanical reliability, failure may not be well-defined, for there may well be mechanical systems in which failure is partial, a matter of degree, or not otherwise localized in time. Even in biological problems, some events (for example, heart attack or other organ failure) may have the same ambiguity. The theory outlined below assumes well-defined events at specific times; other cases may be better treated by models which explicitly account for ambiguous events.
More generally, survival analysis involves the modelling of time to event data; in this context, death or failure is considered an ""event"" in the survival analysis literature – traditionally only a single event occurs for each subject, after which the organism or mechanism is dead or broken. Recurring event or repeated event models relax that assumption. The study of recurring events is relevant in systems reliability, and in many areas of social sciences and medical research.",Survival analysis,1,0
595,30876757,"The survival function is a function that gives the probability that a patient, device, or other object of interest will survive past a certain time.The survival function is also known as the survivor function or reliability function.The term reliability function is common in engineering while the term survival function is used in a broader range of applications, including human mortality. The survival function is the complementary cumulative distribution function of the lifetime. Sometimes complementary cumulative distribution functions are called survival functions in general.

",Survival function,1,0
596,552466,"The field of system identification uses statistical methods to build mathematical models of dynamical systems from measured data. System identification also includes the optimal design of experiments for efficiently generating informative data for fitting such models as well as model reduction. A common approach is to start from measurements of the behavior of the system and the external influences (inputs to the system) and try to determine a mathematical relation between them without going into many details of what is actually happening inside the system; this approach is called black box system identification.

",System identification,1,0
598,659757,"Systems Science, also referred to as Systems Research, or, simply, Systems, is an interdisciplinary field concerned with understanding systems—from simple to complex—in nature, society, cognition, engineering, technology and science itself. The field is diverse, spanning the formal, natural, social, and applied sciences.
To systems scientists, the world can be understood as a system of systems. The field aims to develop interdisciplinary foundations that are applicable in a variety of areas, such as psychology, biology, medicine, communication, business management, technology, computer science, engineering,  and social sciences.Systems science covers formal sciences such as complex systems, cybernetics, dynamical systems theory, information theory, linguistics or systems theory. It has applications in the field of the natural and social sciences and engineering, such as control theory, systems design, operations research, social systems theory, systems biology, system dynamics, human factors, systems ecology, computer science, systems engineering and systems psychology. Themes commonly stressed in system science are (a) holistic view, (b) interaction between a system and its embedding environment, and (c) complex (often subtle) trajectories of dynamic behavior that sometimes are stable (and thus reinforcing), while at various 'boundary conditions' can become wildly unstable (and thus destructive). Concerns about Earth-scale biosphere/geosphere dynamics is an example of the nature of problems to which systems science seeks to contribute meaningful insights.",Systems science,1,0
599,1416975,"A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to perform the hypothesis test.  In general, a test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis, where such an alternative is prescribed, or that would characterize the null hypothesis if there is no explicitly stated alternative hypothesis.
An important property of a test statistic is that its sampling distribution under the null hypothesis must be calculable, either exactly or approximately, which allows p-values to be calculated. A test statistic shares some of the same qualities of a descriptive statistic, and many statistics can be used as both test statistics and descriptive statistics. However, a test statistic is specifically intended for use in statistical testing, whereas the main quality of a descriptive statistic is that it is easily interpretable. Some informative descriptive statistics, such as the sample range, do not make good test statistics since it is difficult to determine their sampling distribution.
Two widely used test statistics are the t-statistic and the F-test.",Test statistic,1,0
600,16876079,"The American Statistician is a quarterly peer-reviewed scientific journal covering statistics published by Taylor & Francis on behalf of the American Statistical Association. It was established in 1947. The editor-in-chief is Daniel R. Jeske, a professor at the University of California, Riverside.",The American Statistician,1,0
601,816492,"""The Correlation between Relatives on the Supposition of Mendelian Inheritance"" is a scientific paper by Ronald Fisher which was published in the Transactions of the Royal Society of Edinburgh in 1918, (volume 52, pages 399–433).  In it, Fisher puts forward the ""infinitesimal model"", a genetics conceptual model showing that continuous variation amongst phenotypic traits could be the result of Mendelian inheritance.  The paper also contains the first use of the statistical term variance.",The Correlation between Relatives on the Supposition of Mendelian Inheritance,1,0
603,499172,"The Genetical Theory of Natural Selection is a book by Ronald Fisher which combines Mendelian genetics with Charles Darwin's theory of natural selection, with Fisher being the first to argue that ""Mendelism therefore validates Darwinism"" and stating with regard to mutations that ""The vast majority of large mutations are deleterious; small mutations are both far more frequent and more likely to be useful"", thus refuting orthogenesis. First published in 1930 by The Clarendon Press, it is one of the most important books of the modern synthesis, and helped define population genetics. It is commonly cited in biology books, outlining many concepts that are still considered important such as Fisherian runaway, Fisher's principle, reproductive value, Fisher's fundamental theorem of natural selection, Fisher's geometric model, the sexy son hypothesis, mimicry and the evolution of dominance. It was dictated to his wife in the evenings as he worked at Rothamsted Research in the day.",The Genetical Theory of Natural Selection,1,0
604,30402,"In theoretical computer science and mathematics, the theory of computation is the branch that deals with what problems can be solved on a model of computation, using an algorithm, how efficiently they can be solved or to what degree (e.g., approximate solutions versus precise ones).  The field is divided into three major branches: automata theory and formal languages, computability theory, and computational complexity theory, which are linked by the question: ""What are the fundamental capabilities and limitations of computers?"".In order to perform a rigorous study of computation, computer scientists work with a mathematical abstraction of computers called a model of computation. There are several models in use, but the most commonly examined is the Turing machine. Computer scientists study the Turing machine because it is simple to formulate, can be analyzed and used to prove results, and because it represents what many consider the most powerful possible ""reasonable"" model of computation (see Church–Turing thesis). It might seem that the potentially infinite memory capacity is an unrealizable attribute, but any decidable problem solved by a Turing machine will always require only a finite amount of memory. So in principle, any problem that can be solved (decided) by a Turing machine can be solved by a computer that has a finite amount of memory.

",Theory of computation,1,1
605,173419,"Time domain refers to the analysis of mathematical functions, physical signals or time series of economic or environmental data, with respect to time.  In the time domain, the signal or function's value is known for all real numbers, for the case of continuous time, or at various separate instants in the case of discrete time. An oscilloscope is a tool commonly used to visualize real-world signals in the time domain.  A time-domain graph shows how a signal changes with time, whereas a frequency-domain graph shows how much of the signal lies within each given frequency band over a range of frequencies.
Though most precisely referring to time in physics, the term time domain may occasionally informally refer to position in space when dealing with spatial frequencies, as a substitute for the more precise term spatial domain.",Time domain,1,0
606,406624,"In mathematics, a time series is a series of data points indexed (or listed or graphed) in time order.  Most commonly, a time series is a sequence taken at successive equally spaced points in time. Thus it is a sequence of discrete-time data. Examples of time series are heights of ocean tides, counts of sunspots, and the daily closing value of the Dow Jones Industrial Average.
A time series is very frequently plotted via a run chart (which is a temporal line chart). Time series are used in statistics, signal processing, pattern recognition, econometrics, mathematical finance, weather forecasting, earthquake prediction, electroencephalography, control engineering, astronomy, communications engineering, and largely in any domain of applied science and engineering which involves temporal measurements.
Time series analysis comprises methods for analyzing time series data in order to extract meaningful statistics and other characteristics of the data. Time series forecasting is the use of a model to predict future values based on previously observed values. While regression analysis is often employed in such a way as to test relationships between one or more different time series, this type of analysis is not usually called ""time series analysis"", which refers in particular to relationships between different points in time within a single series. Interrupted time series analysis is used to detect changes in the evolution of a time series from before to after some intervention which may affect the underlying variable.
Time series data have a natural temporal ordering.  This makes time series analysis distinct from cross-sectional studies, in which there is no natural ordering of the observations (e.g. explaining people's wages by reference to their respective education levels, where the individuals' data could be entered in any order).  Time series analysis is also distinct from spatial data analysis where the observations typically relate to geographical locations (e.g. accounting for house prices by the location as well as the intrinsic characteristics of the houses). A stochastic model for a time series will generally reflect the fact that observations close together in time will be more closely related than observations further apart. In addition, time series models will often make use of the natural one-way ordering of time so that values for a given period will be expressed as deriving in some way from past values, rather than from future values (see time reversibility).
Time series analysis can be applied to real-valued, continuous data, discrete numeric data, or discrete symbolic data (i.e. sequences of characters, such as letters and words in the English language).",Time series,1,1
608,19537780,"This is a timeline of pure and applied mathematics history. It is divided here into three stages, corresponding to stages in the development of mathematical notation: a ""rhetorical"" stage in which calculations are described purely by words, a ""syncopated"" stage in which quantities and common algebraic operations are beginning to be represented by symbolic abbreviations, and finally a  ""symbolic"" stage, in which comprehensive notational systems for formulas are the norm.",Timeline of mathematics,1,0
609,833690,"A tolerance interval is a statistical interval within which, with some confidence level, a specified proportion of a sampled population falls. ""More specifically, a 100×p%/100×(1−α) tolerance interval provides limits within which at least a certain proportion (p) of the population falls with a given level of confidence (1−α)."" ""A (p, 1−α) tolerance interval (TI) based on a sample is constructed so that it would include at least a proportion p of the sampled population with confidence 1−α; such a TI is usually referred to as p-content − (1−α) coverage TI."" ""A (p, 1−α) upper tolerance limit (TL) is simply a 1−α upper confidence limit for the 100 p percentile of the population.""A tolerance interval can be seen as a statistical version of a probability interval. ""In the parameters-known case, a 95% tolerance interval and a 95% prediction interval are the same."" If we knew a population's exact parameters, we would be able to compute a range within which a certain proportion of the population falls. For example, if we know a population is normally distributed with mean 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   and standard deviation 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
  , then the interval 
  
    
      
        μ
        ±
        1.96
        σ
      
    
    {\displaystyle \mu \pm 1.96\sigma }
   includes 95% of the population (1.96 is the z-score for 95% coverage of a normally distributed population).
However, if we have only a sample from the population, we know only the sample mean 
  
    
      
        
          
            
              μ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mu }}}
   and sample standard deviation 
  
    
      
        
          
            
              σ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\sigma }}}
  , which are only estimates of the true parameters. In that case, 
  
    
      
        
          
            
              μ
              ^
            
          
        
        ±
        1.96
        
          
            
              σ
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mu }}\pm 1.96{\hat {\sigma }}}
   will not necessarily include 95% of the population, due to variance in these estimates. A tolerance interval bounds this variance by introducing a confidence level 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , which is the confidence with which this interval actually includes the specified proportion of the population. For a normally distributed population, a z-score can be transformed into a ""k factor"" or tolerance factor for a given 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   via lookup tables or several approximation formulas. ""As the degrees of freedom approach infinity, the prediction and tolerance intervals become equal.""",Tolerance interval,1,0
610,29954,"In mathematics, topology (from the Greek words τόπος, 'place, location', and λόγος, 'study') is concerned with the properties of a geometric object that are preserved under continuous deformations, such as stretching, twisting, crumpling, and bending; that is, without closing holes, opening holes, tearing, gluing, or passing through itself.
A topological space is a set endowed with a structure, called a topology, which allows defining continuous deformation of subspaces, and, more generally, all kinds of continuity. Euclidean spaces, and, more generally, metric spaces are examples of a topological space, as any distance or metric defines a topology. The deformations that are considered in topology are homeomorphisms and homotopies. A property that is invariant under such deformations is a topological property. Basic examples of topological properties are: the dimension, which allows distinguishing between a line and a surface; compactness, which allows distinguishing between a line and a circle; connectedness, which allows distinguishing a circle from two non-intersecting circles.
The ideas underlying topology go back to Gottfried Leibniz, who in the 17th century envisioned the geometria situs and analysis situs. Leonhard Euler's Seven Bridges of Königsberg problem and polyhedron formula are arguably the field's first theorems. The term topology was introduced by Johann Benedict Listing in the 19th century, although it was not until the first decades of the 20th century that the idea of a topological space was developed.",Topology,1,0
611,477060,"Linear trend estimation is a statistical technique to aid interpretation of data. When a series of measurements of a process are treated as, for example, a sequences or time series, trend estimation can be used to make and justify statements about tendencies in the data, by relating the measurements to the times at which they occurred. This model can then be used to describe the behaviour of the observed data, without explaining it.
In particular, it may be useful to determine if measurements exhibit an increasing or decreasing trend which is statistically distinguished from random behaviour. Some examples are determining the trend of the daily average temperatures at a given location from winter to summer, and determining the trend in a global temperature series over the last 100 years. In the latter case, issues of homogeneity are important (for example, about whether the series is equally reliable throughout its length).",Linear trend estimation,1,0
612,575863,"The True Value Company is an American wholesaler with over 4,500 independent retail locations worldwide.A number of different retail identities are supported by True Value Company, including True Value (hardware and home center stores), Grand Rental Station, Party Central, and Taylor Rental (equipment and party rental), Induserve Supply (commercial supplies), Home & Garden Showplace (nursery and garden center stores) as well as affiliate stores that purchase from True Value Company but do not use any of the national store identities.The wholesaler supports its retailers through 13 regional distribution centers and approximately 2,500 employees in 60 countries. The corporate headquarters are located in the O'Hare neighborhood of Chicago.Historically it was a cooperative owned by retailers, but in 2018 it was purchased by ACON Investments.",True Value,1,0
613,14653762,"Tukey's range test, also known as Tukey's test, Tukey method, Tukey's honest significance test, or Tukey's HSD (honestly significant difference) test, is a single-step multiple comparison procedure and statistical test. It can be used to find means that are significantly different from each other. 
Named after John Tukey, it compares all possible pairs of means, and is based on a studentized range distribution (q) (this distribution is similar to the distribution of t from the t-test. See below).Tukey's test compares the means of every treatment to the means of every other treatment; that is, it applies simultaneously to the set of all pairwise comparisons

  
    
      
        
          μ
          
            i
          
        
        −
        
          μ
          
            j
          
        
        
      
    
    {\displaystyle \mu _{i}-\mu _{j}\,}
  and identifies any difference between two means that is greater than the expected standard error. The confidence coefficient for the set, when all sample sizes are equal, is exactly  
  
    
      
        1
        −
        α
      
    
    {\displaystyle 1-\alpha }
   for any 
  
    
      
        0
        ≤
        α
        ≤
        1
      
    
    {\displaystyle 0\leq \alpha \leq 1}
  . For unequal sample sizes, the confidence coefficient is greater than 1 − α. In other words, the Tukey method is conservative when there are unequal sample sizes.
A common mistaken belief is that the Tukey hsd should only be used following a significant ANOVA. The ANOVA is not necessary because the Tukey test controls the Type I error rate on its own.",Tukey's range test,1,0
614,5657877,"In statistical hypothesis testing, a type I error is the mistaken rejection of an actually true null hypothesis (also known as a ""false positive"" finding or conclusion; example: ""an innocent person is convicted""), while a type II error is the failure to reject a null hypothesis that is actually false (also known as a ""false negative"" finding or conclusion; example: ""a guilty person is not convicted""). Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is a statistical impossibility if the outcome is not determined by a known, observable causal process.
By selecting a low threshold (cut-off) value and modifying the alpha (α) level, the quality of the hypothesis test can be increased. The knowledge of type I errors and type II errors is widely used in medical science, biometrics and computer science.Intuitively, type I errors can be thought of as errors of commission, i.e. the researcher unluckily concludes that something is the fact. For instance, consider a study where researchers compare a drug with a placebo. If the patients who are given the drug get better than the patients given the placebo by chance, it may appear that the drug is effective, but in fact the conclusion is incorrect.
In reverse, type II errors are errors of omission.  In the example above, if the patients who got the drug did not get better at a higher rate than the ones who got the placebo, but this was a random fluke, that would be a type II error. The consequence of a type II error depends on the size and direction of the missed determination and the circumstances. An expensive cure for one in a million patients may be inconsequential even if it truly is a cure.",Type I and type II errors,1,1
617,40282,"In mathematics, logic, and computer science, a type theory is the formal presentation of a specific type system, and in general type theory is the academic study of type systems. Some type theories serve as alternatives to set theory as a foundation of mathematics. Two influential type theories that were proposed as foundations are Alonzo Church's typed λ-calculus and Per Martin-Löf's intuitionistic type theory.  Most computerized proof-writing systems use a type theory for their foundation.  A common one is Thierry Coquand's Calculus of Inductive Constructions.",Type theory,1,0
618,16811057,"In statistical theory, a U-statistic is a class of statistics that is especially important in estimation theory; the letter ""U"" stands for unbiased.   In elementary statistics, U-statistics arise naturally in producing minimum-variance unbiased estimators.
The theory of U-statistics allows a minimum-variance unbiased estimator to be derived from each unbiased estimator of an estimable parameter (alternatively, statistical functional) for large classes of probability distributions. An estimable parameter is a measurable function of the population's cumulative probability distribution: For example, for every probability distribution, the population median is an estimable parameter. The theory of U-statistics applies to general classes of probability distributions.
Many statistics originally derived for particular parametric families have been recognized as U-statistics for general distributions. In non-parametric statistics, the theory of U-statistics is used to establish for statistical procedures (such as estimators and tests) and estimators relating to the asymptotic normality and to the variance (in finite samples) of such quantities. The theory has been used to study more general statistics as well as stochastic processes, such as random graphs.Suppose that a problem involves independent and identically-distributed random variables and that estimation of a certain parameter is required. Suppose that a simple unbiased estimate can be constructed based on only a few observations: this defines the basic estimator based on a given number of observations. For example, a single observation is itself an unbiased estimate of the mean and a pair of observations can be used to derive an unbiased estimate of the variance. The U-statistic based on this estimator is defined as the average (across all combinatorial selections of the given size from the full set of observations) of the basic estimator applied to the sub-samples.
Sen (1992) provides a review of the paper by Wassily Hoeffding (1948), which introduced U-statistics and set out the theory relating to them, and in doing so Sen outlines the importance U-statistics have in statistical theory. Sen says, “The impact of Hoeffding (1948) is overwhelming at the present time and is very likely to continue in the years to come.” Note that the theory of U-statistics is not limited to the case of independent and identically-distributed random variables or to scalar random-variables.",U-statistic,1,0
621,15433591,"In statistical hypothesis testing, a uniformly most powerful (UMP) test is a hypothesis test which has the greatest power 
  
    
      
        1
        −
        β
      
    
    {\displaystyle 1-\beta }
   among all possible tests of a given size α. For example, according to the Neyman–Pearson lemma, the likelihood-ratio test is UMP for testing simple (point) hypotheses.",Uniformly most powerful test,1,0
622,45200,"Universal algebra (sometimes called general algebra) is the field of mathematics that studies algebraic structures themselves, not examples (""models"") of algebraic structures.
For instance, rather than take particular groups as the object of study, in universal algebra one takes the class of groups as an object of study.",Universal algebra,1,0
623,52029,"University College London, which operates as UCL, is a public research university in London, United Kingdom. It is a member institution of the federal University of London, and is the second-largest university in the United Kingdom by total enrolment and the largest by postgraduate enrolment.
Established in 1826, as London University, by founders inspired by the radical ideas of Jeremy Bentham, UCL was the first university institution to be established in London, and the first in England to be entirely secular and to admit students regardless of their religion. UCL also makes contested claims to being the third-oldest university in England and the first to admit women. In 1836, UCL became one of the two founding colleges of the University of London, which was granted a royal charter in the same year. It has grown through mergers, including with the Institute of Ophthalmology (in 1995), the Institute of Neurology (in 1997), the Royal Free Hospital Medical School (in 1998), the Eastman Dental Institute (in 1999), the School of Slavonic and East European Studies (in 1999), the School of Pharmacy (in 2012) and the Institute of Education (in 2014).
UCL has its main campus in the Bloomsbury area of central London, with a number of institutes and teaching hospitals elsewhere in central London and satellite campuses at Queen Elizabeth Olympic Park in Stratford, east London and in Doha, Qatar. UCL is organised into 11 constituent faculties, within which there are over 100 departments, institutes and research centres. UCL operates several museums and collections in a wide range of fields, including the Petrie Museum of Egyptian Archaeology and the Grant Museum of Zoology and Comparative Anatomy, and administers the annual Orwell Prize in political writing. In 2019/20, UCL had around 43,840 students and 16,400 staff (including around 7,100 academic staff and 840 professors) and had a total income of £1.54 billion, of which £468 million was from research grants and contracts. The university generates around £10 billion annually for the UK economy, primarily through the spread of its research and knowledge (£4 billion) and the impact of its own spending (£3 billion).UCL is a member of numerous academic organisations, including the Russell Group and the League of European Research Universities, and is part of UCL Partners, the world's largest academic health science centre. It is considered part of the ""golden triangle"" of research-intensive universities in southeast England. UCL has publishing and commercial activities including UCL Press, UCL Business and UCL Consultants.
UCL has many notable alumni, including the respective ""Fathers of the Nation"" of India, Kenya and Mauritius, the founders of Ghana, modern Japan and Nigeria, the inventor of the telephone, and one of the co-discoverers of the structure of DNA. UCL academics discovered five of the naturally occurring noble gases, discovered hormones, invented the vacuum tube, and made several foundational advances in modern statistics. As of 2020, 34 Nobel Prize winners and three Fields medallists have been affiliated with UCL as alumni, faculty or researchers.",University College London,1,0
624,59158299,"Up-and-down designs (UDDs) are a family of statistical experiment designs used in dose-finding experiments in science, engineering, and medical research. Dose-finding experiments have binary responses: each individual outcome can be described as one of two possible values, such as success vs. failure or toxic vs. non-toxic. Mathematically the binary responses are coded as 1 and 0. The goal of dose-finding experiments is to estimate the strength of treatment (i.e., the ""dose"") that would trigger the ""1"" response a pre-specified proportion of the time. This dose can be envisioned as a percentile of the distribution of response thresholds. An example where dose-finding is used is in an experiment to estimate the LD50 of some toxic chemical with respect to mice.

Dose-finding designs are sequential and response-adaptive: the dose at a given point in the experiment depends upon previous outcomes, rather than be fixed a priori. Dose-finding designs are generally more efficient for this task than fixed designs, but their properties are harder to analyze, and some require specialized design software. UDDs use a discrete set of doses rather than vary the dose continuously. They are relatively simple to implement, and are also among the best understood dose-finding designs. Despite this simplicity, UDDs generate random walks with intricate properties. The original UDD aimed to find the median threshold by increasing the dose one level after a ""0"" response, and decreasing it one level after a ""1"" response. Hence the name ""up-and-down"". Other UDDs break this symmetry in order to estimate percentiles other than the median, or are able to treat groups of subjects rather than one at a time.
UDDs were developed in the 1940s by several research groups independently. The 1950s and 1960s saw rapid diversification with UDDs targeting percentiles other than the median, and expanding into numerous applied fields. The 1970s to early 1990s saw little UDD methods research, even as the design continued to be used extensively. A revival of UDD research since the 1990s has provided deeper understanding of UDDs and their properties, and new and better estimation methods.UDDs are still used extensively in the two applications for which they were originally developed: psychophysics where they are used to estimate sensory thresholds and are often known as fixed forced-choice staircase procedures, and explosive sensitivity testing, where the median-targeting UDD is often known as the Bruceton test. UDDs are also very popular in toxicity and anesthesiology research. They are also considered a viable choice for Phase I clinical trials.",Up-and-Down Designs,1,0
625,30694795,"V-statistics are a class of statistics named for Richard von Mises who developed their asymptotic distribution theory in a fundamental paper in 1947. V-statistics are closely related to U-statistics (U for ""unbiased"") introduced by Wassily Hoeffding in 1948.  A V-statistic is a statistical function (of a sample) defined by a particular statistical functional of a probability distribution.",V-statistic,1,0
626,15915853,"Named after the Dutch mathematician Bartel Leendert van der Waerden, the Van der Waerden test is a statistical test that k population distribution functions are equal. The Van der Waerden test converts the ranks from a standard Kruskal-Wallis one-way analysis of variance to quantiles of the standard normal distribution (details given below). These are called normal scores and the test is computed from these normal scores.
The k population version of the test is an extension of the test for two populations published by Van der Waerden (1952,1953).",Van der Waerden test,1,0
627,3728109,"In mathematics, a variable (from Latin variabilis, ""changeable"") is a symbol and placeholder for (historically) a quantity that may change, or (nowadays) any mathematical object. In particular, a variable may represent a number, a vector, a matrix, a function, the argument of a function, a set, or an element of a set.Algebraic computations with variables as if they were explicit numbers solve a range of problems in a single computation. For example, the quadratic formula solves every quadratic equation by substituting the numeric values of the coefficients of the given equation for the variables that represent them. In mathematical logic, a variable is either a symbol representing an unspecified term of the theory (a meta-variable), or a basic object of the theory that is manipulated without referring to its possible intuitive interpretation.

",Variable (mathematics),1,0
628,2399016,"Vector autoregression (VAR) is a statistical model used to capture the relationship between multiple quantities as they change over time. VAR is a type of stochastic process model. VAR models generalize the single-variable (univariate) autoregressive model by allowing for multivariate time series. VAR models are often used in economics and the natural sciences.
Like the autoregressive model, each variable has an equation modelling its evolution over time. This equation includes the variable's lagged (past) values, the lagged values of the other variables in the model, and an error term. VAR models do not require as much knowledge about the forces influencing a variable as do structural models with simultaneous equations. The only prior knowledge required is a list of variables which can be hypothesized to affect each other over time.",Vector autoregression,1,0
629,17164795,"A violin plot is a method of plotting numeric data. It is similar to a box plot, with the addition of a rotated kernel density plot on each side.Violin plots are similar to box plots, except that they also show the probability density of the data at different values, usually smoothed by a kernel density estimator. Typically a violin plot will include all the data that is in a box plot: a marker for the median of the data; a box or marker indicating the interquartile range; and possibly all sample points, if the number of samples is not too high.
Violin plots are available as extensions to a number of software packages such as DataVisualization on CRAN  and the md-plot package on PyPI.A violin plot is more informative than a plain box plot. While a box plot only shows summary statistics such as mean/median and interquartile ranges, the violin plot shows the full distribution of the data. The difference is particularly useful when the data distribution is multimodal (more than one peak). In this case a violin plot shows the presence of different peaks, their position and relative amplitude.
Like box plots, violin plots are used to represent comparison of a variable distribution (or sample distribution) across different ""categories"" (for example, temperature distribution compared between day and night, or distribution of car prices compared across different car makers).
A violin plot can have multiple layers. For instance, the outer shape represents all possible results. The next layer inside might represent the values that occur 95% of the time.  The next layer (if it exists) inside might represent the values that occur 50% of the time.
Although more informative than box plots, they are less popular. Because of their unpopularity, they may be harder to understand for readers not familiar with them. In this case, a more accessible alternative is to plot a series of stacked histograms or kernel density distributions.
Violin plots are available as extensions to a number of software packages, including the R packages vioplot, wvioplot, caroline, UsingR, lattice and ggplot2, the Stata add-on command vioplot, and the Python libraries matplotlib, Plotly, ROOT and Seaborn, a graph type in Origin, IGOR Pro,Julia statistical plotting package StatsPlots.jl and DistributionChart in Mathematica.",Violin plot,1,0
630,1600352,"In statistics, the Wald test (named after Abraham Wald) assesses constraints on statistical parameters based on the weighted distance between the unrestricted estimate and its hypothesized value under the null hypothesis, where the weight is the precision of the estimate. Intuitively, the larger this weighted distance, the less likely it is that the constraint is true. While the finite sample distributions of Wald tests are generally unknown, it has an asymptotic χ2-distribution under the null hypothesis, a fact that can be used to determine statistical significance.Together with the Lagrange multiplier test and the likelihood-ratio test, the Wald test is one of three classical approaches to hypothesis testing. An advantage of the Wald test over the other two is that it only requires the estimation of the unrestricted model, which lowers the computational burden as compared to the likelihood-ratio test. However, a major disadvantage is that (in finite samples) it is not invariant to changes in the representation of the null hypothesis; in other words, algebraically equivalent expressions of non-linear parameter restriction can lead to different values of the test statistic. That is because the Wald statistic is derived from a Taylor expansion, and different ways of writing equivalent nonlinear expressions lead to nontrivial differences in the corresponding Taylor coefficients. Another aberration, known as the Hauck–Donner effect, can occur in binomial models when the estimated (unconstrained) parameter is close to the boundary of the parameter space—for instance a fitted probability being extremely close to zero or one—which results in the Wald test no longer monotonically increasing in the distance between the unconstrained and constrained parameter.",Wald test,1,0
631,50903,"A wavelet is a wave-like oscillation with an amplitude that begins at zero, increases or decreases, and then returns to zero one or more times. Wavelets are termed a ""brief oscillation"". A taxonomy of wavelets has been established, based on the number and direction of its pulses. Wavelets are imbued with specific properties that make them useful for signal processing.

For example, a wavelet could be created to have a frequency of Middle C and a short duration of roughly one tenth of a second. If this wavelet were to be convolved with a signal created from the recording of a melody, then the resulting signal would be useful for determining when the Middle C note appeared in the song. Mathematically, a wavelet correlates with a signal if a portion of the signal is similar. Correlation is at the core of many practical wavelet applications.
As a mathematical tool, wavelets can be used to extract information from many different kinds of data, including – but not limited to – audio signals and images. Sets of wavelets are needed to analyze data fully. ""Complementary"" wavelets decompose a signal without gaps or overlaps so that the decomposition process is mathematically reversible. Thus, sets of complementary wavelets are useful in wavelet based compression/decompression algorithms where it is desirable to recover the original information with minimal loss.
In formal terms, this representation is a wavelet series representation of a square-integrable function with respect to either a complete, orthonormal set of basis functions, or an overcomplete set or frame of a vector space, for the Hilbert space of square integrable functions. This is accomplished through coherent states.
In classical physics, the diffraction phenomenon is described by the Huygens–Fresnel principle that treats each point in a propagating wavefront as a collection of individual spherical wavelets. The characteristic bending pattern is most pronounced when a wave from a coherent source (such as a laser) encounters a slit/aperture that is comparable in size to its wavelength. This is due to the addition, or interference, of different points on the wavefront (or, equivalently, each wavelet) that travel by paths of different lengths to the registering surface. Multiple, closely spaced openings (e.g., a diffraction grating), can result in a complex pattern of varying intensity.",Wavelet,1,1
632,23538754,"The Wayback Machine is a digital archive of the World Wide Web founded by the Internet Archive, a nonprofit based in San Francisco, California. Created in 1996 and launched to the public in 2001, it allows the user to go ""back in time"" and see how websites looked in the past. Its founders, Brewster Kahle and Bruce Gilliat, developed the Wayback Machine to provide ""universal access to all knowledge"" by preserving archived copies of defunct web pages.Launched on May 10, 1996, the Wayback Machine had more than 38.2 million records at the end of 2009. As of July 2022, the machine had saved more than 700 billion web pages. More than one million web pages are added daily.

In 1996, Brewster Kahle, founder of the Internet Archive, and Bruce Gilliat, a graduate student at the Massachusetts Institute of Technology (MIT), developed the Wayback Machine as a tool for creating a universally accessible digital library, supporting the Internet Archive's mission of universal access to all knowledge.

",Wayback Machine,1,0
633,49522576,"In statistics, Whittle likelihood is an approximation to the likelihood function of a stationary Gaussian time series. It is named after the mathematician and statistician Peter Whittle, who introduced it in his PhD thesis in 1951.
It is commonly used in time series analysis and signal processing for parameter estimation and signal detection.",Whittle likelihood,1,0
634,1954987,"The Wilcoxon signed-rank test is a non-parametric statistical hypothesis test used either to test the location of a population based on a sample of data, or to compare the locations of two populations using two matched samples. The one-sample version serves a purpose similar to that of the one-sample Student's t-test. For two matched samples, it is a paired difference test like the paired Student's t-test (also known as the ""t-test for matched pairs"" or ""t-test for dependent samples""). The Wilcoxon test can be a good alternative to the t-test when population means are not of interest; for example, when one wishes to test whether a population's median is nonzero, or whether there is a better than 50% chance that a sample from one population is greater than a sample from another population.",Wilcoxon signed-rank test,1,0
635,105651,"William Sealy Gosset (13 June 1876 – 16 October 1937) was an English statistician, chemist and brewer who served as Head Brewer of Guinness and Head Experimental Brewer of Guinness and was a pioneer of modern statistics. He pioneered small sample experimental design and analysis with an economic approach to the logic of uncertainty. Gosset published under the pen name Student and developed most famously Student's t-distribution – originally called Student's ""z"" – and ""Student's test of statistical significance"".",William Sealy Gosset,1,0
636,29272078,"World Statistics Day is an international day to celebrate statistics.  Created by the United Nations Statistical Commission, it was first celebrated on 20 October 2010.  The day is celebrated every five years.As of 2010, 103 countries celebrate a national Statistics Day, including 51 African countries that jointly celebrate African Statistics Day annually on 18 November. India celebrates its statistics day on 29 June, the birthday of the statistician Prasanta Chandra Mahalanobis.  The Royal Statistical Society in the UK also launched its getstats statistical literacy campaign on the same day at 20:10 (on 20.10.2010).",World Statistics Day,1,0
637,221722,"In statistics, the standard score is the number of standard deviations by which the value of a raw score (i.e., an observed value or data point) is above or below the mean value of what is being observed or measured. Raw scores above the mean have positive standard scores, while those below the mean have negative standard scores. 
It is calculated by subtracting the population mean from an individual raw score and then dividing the difference by the population standard deviation. This process of converting a raw score into a standard score is called standardizing or normalizing (however, ""normalizing"" can refer to many types of ratios; see normalization for more). 
Standard scores are most commonly called z-scores; the two terms may be used interchangeably, as they are in this article. Other equivalent terms in use include z-values, normal scores, standardized variables and pull in high energy physics.Computing a z-score requires knowledge of the mean and standard deviation of the complete population to which a data point belongs; if one only has a sample of observations from the population, then the analogous computation using the sample mean and sample standard deviation yields the t-statistic.

",Standard score,1,0
638,332213,"A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Z-tests test the mean of a distribution. For each significance level in the confidence interval, the Z-test has a single critical value (for example, 1.96 for 5% two tailed) which makes it more convenient than the Student's t-test whose critical values are defined by the sample size (through the corresponding degrees of freedom). Both the Z test and Student's t-test have similarities in that they both help determine the significance of a set of data. However, the z-test is rarely used in practice because the population deviation is difficult to determine.

",Z-test,1,0
639,24714635,"The AAAI Conference on Artificial Intelligence (AAAI) is one of the leading international academic conference in artificial intelligence held annually. Along with ICML, NeurIPS and ICLR, it is one of the primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the Association for the Advancement of Artificial Intelligence. Precise dates vary from year to year, but paper submissions are generally due at the end of August to beginning of September, and the conference is generally held during the following February. The first AAAI was held in 1980 at Stanford University, Stanford California.During AAAI-20 conference AI pioneers and 2018 Turing Award winners Yann LeCun and Yoshua Bengio among eight other researchers were honored as the AAAI 2020 Fellows.Along with other conferences such as NeurIPS, ICML, AAAI uses artificial intelligence algorithm to assign papers to reviewers.",AAAI Conference on Artificial Intelligence,0,1
640,3192516,"The ACM Computing Classification System (CCS) is a subject classification system for computing devised by the Association for Computing Machinery (ACM). The system is comparable to the Mathematics Subject Classification (MSC) in scope, aims, and structure, being used by the various ACM journals to organize subjects by area.",ACM Computing Classification System,0,1
641,24559562,"ACM Computing Surveys is a quarterly peer-reviewed scientific journal published by the Association for Computing Machinery. It publishes survey articles and tutorials related to computer science and computing. The journal was established in 1969 with William S. Dorn as founding editor-in-chief.According to the Journal Citation Reports, the journal has a 2020 impact factor of 10.282. In a 2008 ranking of computer science journals, ACM Computing Surveys received the highest rank ""A*"".

",ACM Computing Surveys,0,1
642,50785023,"In artificial intelligence (AI) and philosophy, AI alignment and the AI control problem are aspects of how to build AI systems such that they will aid rather than harm their creators. One particular concern is that humanity will have to solve the control problem before a superintelligent AI system is created, as a poorly designed superintelligence might rationally decide to seize control over its environment and refuse to permit its creators to modify it after launch. In addition, some scholars argue that solutions to the control problem, alongside other advances in AI safety engineering, might also find applications in existing non-superintelligent AI.Major approaches to the control problem include alignment, which aims to align AI goal systems with human values, and capability control, which aims to reduce an AI system's capacity to harm humans or gain control. Capability control proposals are generally not considered reliable or sufficient to solve the control problem, but rather as potentially valuable supplements to alignment efforts.",AI alignment,0,1
643,813176,"An AI takeover is a hypothetical scenario in which an artificial intelligence (AI) becomes the dominant form of intelligence on Earth, as computer programs or robots effectively take the control of the planet away from the human species. Possible scenarios include replacement of the entire human workforce, takeover by a superintelligent AI, and the popular notion of a robot uprising. Some public figures, such as Stephen Hawking and Elon Musk, have advocated research into precautionary measures to ensure future superintelligent machines remain under human control.

",AI takeover,0,1
644,3548574,"In the history of artificial intelligence, an AI winter is a period of reduced funding and interest in artificial intelligence research. The term was coined by analogy to the idea of a nuclear winter. The field has experienced several hype cycles, followed by disappointment and criticism, followed by funding cuts, followed by renewed interest years or decades later.
The term first appeared in 1984 as the topic of a public debate at the annual meeting of AAAI (then called the ""American Association of Artificial Intelligence""). It is a chain reaction that begins with pessimism in the AI community, followed by pessimism in the press, followed by a severe cutback in funding, followed by the end of serious research. At the meeting, Roger Schank and Marvin Minsky—two leading AI researchers who had survived the ""winter"" of the 1970s—warned the business community that enthusiasm for AI had spiraled out of control in the 1980s and that disappointment would certainly follow. Three years later, the billion-dollar AI industry began to collapse.Hype is common in many emerging technologies, such as the railway mania or the dot-com bubble. The AI winter was a result of such hype, due to over-inflated promises by developers, unnaturally high expectations from end-users, and extensive promotion in the media. Despite the rise and fall of AI's reputation, it has continued to develop new and successful technologies. AI researcher Rodney Brooks would complain in 2002 that ""there's this stupid myth out there that AI has failed, but AI is around you every second of the day."" In 2005, Ray Kurzweil agreed: ""Many observers still think that the AI winter was the end of the story and that nothing since has come of the AI field. Yet today many thousands of AI applications are deeply embedded in the infrastructure of every industry.""Enthusiasm and optimism about AI has generally increased since its low point in the early 1990s. Beginning about 2012, interest in artificial intelligence (and especially the sub-field of machine learning) from the research and corporate communities led to a dramatic increase in funding and investment.",AI winter,0,1
645,3874825,"AT&T Labs is the research & development division of AT&T. It employs some 1800 people in various locations, including: Bedminster NJ; Middletown, NJ; Manhattan, NY; Warrenville, IL; Austin, TX; Dallas, TX; Atlanta, GA; San Francisco, CA; San Ramon, CA; and Redmond, WA . AT&T Labs – Research, the 450-person research division of AT&T Labs, is based in the Bedminster, Middletown, San Francisco, and Manhattan locations.
AT&T Labs traces its history from AT&T Bell Labs. Much research is in areas traditionally associated with networks and systems, ranging from the physics of optical transmission to foundational topics in computing and communications. Other research areas address the technical challenges of large operational networks and the resulting large data sets.",AT&T Labs,0,1
646,5033373,"Action selection is a way of characterizing the most basic problem of intelligent systems: what to do next. In artificial intelligence and computational cognitive science, ""the action selection problem"" is typically associated with intelligent agents and animats—artificial systems that exhibit complex behaviour in an agent environment. The term is also sometimes used in ethology or animal behavior.
One problem for understanding action selection is determining the level of abstraction used for specifying an ""act"". At the most basic level of abstraction, an atomic act could be anything from contracting a muscle cell to provoking a war. Typically for any one action-selection mechanism, the set of possible actions is predefined and fixed.
Most researchers working in this field place high demands on their agents:

The acting agent typically must select its action in dynamic and unpredictable environments.
The agents typically act in real time; therefore they must make decisions in a timely fashion.
The agents are normally created to perform several different tasks. These tasks may conflict for resource allocation (e.g. can the agent put out a fire and deliver a cup of coffee at the same time?)
The environment the agents operate in may include humans, who may make things more difficult for the agent (either intentionally or by attempting to assist.)
The agents themselves are often intended to model animals or humans, and animal/human behaviour is quite complicated.For these reasons action selection is not trivial and attracts a good deal of research.",Action selection,0,1
647,14179835,"In artificial neural networks, the activation function of a node defines the output of that node given an input or set of inputs. 
A standard integrated circuit can be seen as a digital network of activation functions that can be ""ON"" (1) or ""OFF"" (0), depending on input. This is similar to the linear perceptron in neural networks. However, only nonlinear activation functions allow such networks to compute nontrivial problems using only a small number of nodes, and such activation functions are called nonlinearities.

",Activation function,0,1
648,28801798,"Active learning is a special case of machine learning in which a learning algorithm can interactively query a user (or some other information source) to label new data points with the desired outputs. In statistics literature, it is sometimes also called optimal experimental design. The information source is also called teacher or oracle.
There are situations in which unlabeled data is abundant but manual labeling is expensive. In such a scenario, learning algorithms can actively query the user/teacher for labels. This type of iterative supervised learning is called active learning. Since the learner chooses the examples, the number of examples to learn a concept can often be much lower than the number required in normal supervised learning. With this approach, there is a risk that the algorithm is overwhelmed by uninformative examples.  Recent developments are dedicated to multi-label active learning, hybrid active learning and active learning in a single-pass (on-line) context, combining concepts from the field of machine learning (e.g. conflict and ignorance) with adaptive, incremental learning policies in the field of online machine learning.
Large-scale active learning projects may benefit from crowdsourcing frameworks such as Amazon Mechanical Turk that include many humans in the active learning loop.",Active learning (machine learning),0,1
649,23924341,An adaptive website is a website that builds a model of user activity and modifies the information and/or presentation of information to the user in order to better address the user's needs.,Adaptive website,0,1
650,45049676,"Adversarial machine learning is the study of the attacks on machine learning algorithms, and of the defenses against such attacks. A recent survey exposes the fact that practitioners report a dire need for better protecting machine learning systems in industrial applications.To understand, note that most machine learning techniques are mostly designed to work on specific problem sets, under the assumption that the training and test data are generated from the same statistical distribution (IID). However, this assumption is often dangerously violated in practical high-stake applications, where users may purposely supply fabricated data that violates the statistical assumption.
Some of the most common threat models in adversarial machine learning include evasion attacks, data poisoning attacks, Byzantine attacks and model extraction.",Adversarial machine learning,0,1
651,233942,"Affective computing is the study and development of systems and devices that can recognize, interpret, process, and simulate human affects. It is an interdisciplinary field spanning computer science, psychology, and cognitive science. While some core ideas in the field may be traced as far back as to early philosophical inquiries into emotion, the more modern branch of computer science originated with Rosalind Picard's 1995 paper on affective computing and her book Affective Computing published by MIT Press. One of the motivations for the research is the ability to give machines emotional intelligence, including to simulate empathy. The machine should interpret the emotional state of humans and adapt its behavior to them, giving an appropriate response to those emotions.

",Affective computing,0,1
652,5906926,"Alan Mackworth is a professor of computer science and Canada Research Chair in Artificial Intelligence in the Department of Computer Science at the University of British Columbia. He is known as ""The Founding Father"" of RoboCup. He is a former president of the Association for the Advancement of Artificial Intelligence (AAAI).",Alan Mackworth,0,1
653,1208,"Alan Mathison Turing  (; 23 June 1912 – 7 June 1954) was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. He is widely considered to be the father of theoretical computer science and artificial intelligence.Born in Maida Vale, London, Turing was raised in southern England. He graduated at King's College, Cambridge, with a degree in mathematics. Whilst he was a fellow at Cambridge, he published a proof demonstrating that some purely mathematical yes–no questions can never be answered by computation and defined a Turing machine, and went on to prove the halting problem for Turing machines is undecidable. In 1938, he obtained his PhD from the Department of Mathematics at Princeton University. During the Second World War, Turing worked for the Government Code and Cypher School (GC&CS) at Bletchley Park, Britain's codebreaking centre that produced Ultra intelligence. For a time he led Hut 8, the section that was responsible for German naval cryptanalysis. Here, he devised a number of techniques for speeding the breaking of German ciphers, including improvements to the pre-war Polish bombe method, an electromechanical machine that could find settings for the Enigma machine. Turing played a crucial role in cracking intercepted coded messages that enabled the Allies to defeat the Axis powers in many crucial engagements, including the Battle of the Atlantic.After the war, Turing worked at the National Physical Laboratory, where he designed the Automatic Computing Engine (ACE), one of the first designs for a stored-program computer. In 1948, Turing joined Max Newman's Computing Machine Laboratory, at the Victoria University of Manchester, where he helped develop the Manchester computers and became interested in mathematical biology. He wrote a paper on the chemical basis of morphogenesis and predicted oscillating chemical reactions such as the Belousov–Zhabotinsky reaction, first observed in the 1960s. Despite these accomplishments, Turing was never fully recognised in Britain during his lifetime because much of his work was covered by the Official Secrets Act.Turing was prosecuted in 1952 for homosexual acts. He accepted hormone treatment with DES, a procedure commonly referred to as chemical castration, as an alternative to prison. Turing died on 7 June 1954, 16 days before his 42nd birthday, from cyanide poisoning. An inquest determined his death as a suicide, but it has been noted that the known evidence is also consistent with accidental poisoning.
Following a public campaign in 2009, the British Prime Minister Gordon Brown made an official public apology on behalf of the British government for ""the appalling way [Turing] was treated"". Queen Elizabeth II granted a posthumous pardon in 2013. The term ""Alan Turing law"" is now used informally to refer to a 2017 law in the United Kingdom that retroactively pardoned men cautioned or convicted under historical legislation that outlawed homosexual acts.Turing has an extensive legacy with statues of him and many things named after him, including an annual award for computer science innovations. He appears on the current Bank of England £50 note, which was released on 23 June 2021, to coincide with his birthday. A 2019 BBC series, as voted by the audience, named him the greatest person of the 20th century.",Alan Turing,0,1
654,50568903,"Alex Graves is a research scientist at DeepMind. He did a BSc in Theoretical Physics at Edinburgh and obtained a PhD in AI under Jürgen Schmidhuber at IDSIA. He was also a postdoc under Jürgen Schmidhuber at TU Munich and under Geoffrey Hinton at the University of Toronto.
At IDSIA, he trained long short-term memory neural networks by a novel method called connectionist temporal classification (CTC).   This method outperformed traditional speech recognition models in certain applications. In 2009, his CTC-trained LSTM was the first recurrent neural network to win pattern recognition contests, winning several competitions in connected handwriting recognition.
This method has become very popular. Google uses CTC-trained LSTM for speech recognition on the smartphone.Graves is also the creator of neural Turing machines and the closely related  differentiable neural computer.",Alex Graves (computer scientist),0,1
657,55817338,"Algorithmic bias describes systematic and repeatable errors in a computer system that create ""unfair"" outcomes, such as ""privileging"" one category over another in ways different from the intended function of the algorithm. 
Bias can emerge from many factors, including but not limited to the design of the algorithm or the unintended or unanticipated use or decisions relating to the way data is coded, collected, selected or used to train the algorithm. For example, algorithmic bias has been observed in search engine results and social media platforms. This bias can have impacts ranging from inadvertent privacy violations to reinforcing social biases of race, gender, sexuality, and ethnicity. The study of algorithmic bias is most concerned with algorithms that reflect ""systematic and unfair"" discrimination. This bias has only recently been addressed in legal frameworks, such as the European Union's General Data Protection Regulation (2018) and the proposed Artificial Intelligence Act (2021).
As algorithms expand their ability to organize society, politics, institutions, and behavior, sociologists have become concerned with the ways in which unanticipated output and manipulation of data can impact the physical world. Because algorithms are often considered to be neutral and unbiased, they can inaccurately project greater authority than human expertise (in part due to the psychological phenomenon of automation bias), and in some cases, reliance on algorithms can displace human responsibility for their outcomes. Bias can enter into algorithmic systems as a result of pre-existing cultural, social, or institutional expectations; because of technical limitations of their design; or by being used in unanticipated contexts or by audiences who are not considered in the software's initial design.
Algorithmic bias has been cited in cases ranging from election outcomes to the spread of online hate speech. It has also arisen in criminal justice, healthcare, and hiring, compounding existing racial, socioeconomic, and gender biases. The relative inability of facial recognition technology to accurately identify darker-skinned faces has been linked to multiple wrongful arrests of black men, an issue stemming from imbalanced datasets. Problems in understanding, researching, and discovering algorithmic bias persist due to the proprietary nature of algorithms, which are typically treated as trade secrets. Even when full transparency is provided, the complexity of certain algorithms poses a barrier to understanding their functioning. Furthermore, algorithms may change, or respond to input or output in ways that cannot be anticipated or easily reproduced for analysis. In many cases, even within a single website or application, there is no single ""algorithm"" to examine, but a network of many interrelated programs and data inputs, even between users of the same service.",Algorithmic bias,0,1
658,145128,"In computer science, algorithmic efficiency is a property of an algorithm which relates to the amount of computational resources used by the algorithm. An algorithm must be analyzed to determine its resource usage, and the efficiency of an algorithm can be measured based on the usage of different resources. Algorithmic efficiency can be thought of as analogous to engineering productivity for a repeating or continuous process.
For maximum efficiency it is desirable to minimize resource usage. However, different resources such as time and space complexity cannot be compared directly, so which of two algorithms is considered to be more efficient often depends on which measure of efficiency is considered most important.
For example, bubble sort and timsort are both algorithms to sort a list of items from smallest to largest. Bubble sort sorts the list in time proportional to the number of elements squared (
  
    
      
        O
        (
        
          n
          
            2
          
        
        )
      
    
    {\textstyle O(n^{2})}
  , see Big O notation), but only requires a small amount of extra memory which is constant with respect to the length of the list (
  
    
      
        O
        (
        1
        )
      
    
    {\textstyle O(1)}
  ). Timsort sorts the list in time linearithmic (proportional to a quantity times its logarithm) in the list's length (
  
    
      
        O
        (
        n
        log
        ⁡
        n
        )
      
    
    {\textstyle O(n\log n)}
  ), but has a space requirement linear in the length of the list (
  
    
      
        O
        (
        n
        )
      
    
    {\textstyle O(n)}
  ). If large lists must be sorted at high speed for a given application, timsort is a better choice; however, if minimizing the memory footprint of the sorting is more important, bubble sort is a better choice.

",Algorithmic efficiency,0,1
659,52773150,"Algorithmic transparency is the principle that the factors that influence the decisions made by algorithms should be visible, or transparent, to the people who use, regulate, and are affected by systems that employ those algorithms. Although the phrase was coined in 2016 by Nicholas Diakopoulos and Michael Koliska about the role of algorithms in deciding the content of digital journalism services, the underlying principle dates back to the 1970s and the rise of automated systems for scoring consumer credit.
The phrases ""algorithmic transparency"" and ""algorithmic accountability"" are sometimes used interchangeably – especially since they were coined by the same people – but they have subtly different meanings. Specifically, ""algorithmic transparency"" states that the inputs to the algorithm and the algorithm's use itself must be known, but they need not be fair.  ""Algorithmic accountability"" implies that the organizations that use algorithms must be accountable for the decisions made by those algorithms, even though the decisions are being made by a machine, and not by a human being.Current research around algorithmic transparency interested in both societal effects of accessing remote services running algorithms., as well as mathematical and computer science approaches that can be used to achieve algorithmic transparency In the United States, the Federal Trade Commission's Bureau of Consumer Protection studies how algorithms are used by consumers by conducting its own research on algorithmic transparency and by funding external research. In the European Union, the data protection laws that came into effect in May 2018 include a ""right to explanation"" of decisions made by algorithms, though it is unclear what this means.",Algorithmic transparency,0,1
660,59766171,"AlphaFold is an artificial intelligence (AI) program developed by Alphabet's/Google's DeepMind which performs predictions of protein structure. The program is designed as a deep learning system.AlphaFold AI software has had two major versions. A team of researchers that used AlphaFold 1 (2018) placed first in the overall rankings of the 13th Critical Assessment of Techniques for Protein Structure Prediction (CASP) in December 2018. The program was particularly successful at predicting the most accurate structure for targets rated as the most difficult by the competition organisers, where no existing template structures were available from proteins with a partially similar sequence. 
A team that used AlphaFold 2 (2020) repeated the placement in the CASP competition in November 2020. The team achieved a level of accuracy much higher than any other group. It scored above 90 for around two-thirds of the proteins in CASP's global distance test (GDT), a test that measures the degree to which a computational program predicted structure is similar to the lab experiment determined structure, with 100 being a complete match, within the distance cutoff used for calculating GDT.AlphaFold 2's results at CASP were described as ""astounding"" and transformational. Some researchers noted that the accuracy is not high enough for a third of its predictions, and that it does not reveal the mechanism or rules of protein folding for the protein folding problem to be considered solved. Nevertheless, there has been widespread respect for the technical achievement.
On 15 July 2021 the AlphaFold 2 paper was published at Nature as an advance access publication alongside open source software and a searchable database of species proteomes.

",AlphaFold,0,1
661,55981499,"AlphaZero is a computer program developed by artificial intelligence research company DeepMind to master the games of chess, shogi and go.  This algorithm uses an approach similar to AlphaGo Zero. 
On December 5, 2017, the DeepMind team released a preprint introducing AlphaZero, which within 24 hours of training achieved a superhuman level of play in these three games by defeating world-champion programs Stockfish, elmo, and the three-day version of AlphaGo Zero. In each case it made use of custom tensor processing units (TPUs) that the Google programs were optimized to use. AlphaZero was trained solely via self-play using 5,000 first-generation TPUs to generate the games and 64 second-generation TPUs to train the neural networks, all in parallel, with no access to opening books or endgame tables. After four hours of training, DeepMind estimated AlphaZero was playing chess at a higher Elo rating than Stockfish 8; after nine hours of training, the algorithm defeated Stockfish 8 in a time-controlled 100-game tournament (28 wins, 0 losses, and 72 draws). The trained algorithm played on a single machine with four TPUs. 
DeepMind's paper on AlphaZero was published in the journal Science on 7 December 2018. In 2019 DeepMind published a new paper detailing MuZero, a new algorithm able to generalise AlphaZero's work, playing both Atari and board games without knowledge of the rules or representations of the game.",AlphaZero,0,1
662,1691376,"Amazon Web Services, Inc. (AWS) is a subsidiary of Amazon that provides on-demand cloud computing platforms and APIs to individuals, companies, and governments, on a metered pay-as-you-go basis. These cloud computing web services provide distributed computing processing capacity and software tools via AWS server farms. One of these services is Amazon Elastic Compute Cloud (EC2), which allows users to have at their disposal a virtual cluster of computers, available all the time, through the Internet. AWS's virtual computers emulate most of the attributes of a real computer, including hardware central processing units (CPUs) and graphics processing units (GPUs) for processing; local/RAM memory; hard-disk/SSD storage; a choice of operating systems; networking; and pre-loaded application software such as web servers, databases, and customer relationship management (CRM).
AWS services are delivered to customers via a network of AWS server farms located throughout the world. Fees are based on a combination of usage (known as a ""Pay-as-you-go"" model), hardware, operating system, software, or networking features chosen by the subscriber required availability, redundancy, security, and service options. Subscribers can pay for a single virtual AWS computer, a dedicated physical computer, or clusters of either. Amazon provides select portions of security for subscribers (e.g. physical security of the data centers) while other aspects of security are the responsibility of the subscriber (e.g. account management, vulnerability scanning, patching). AWS operates from many global geographical regions including 6 in North America.Amazon markets AWS to subscribers as a way of obtaining large-scale computing capacity more quickly and cheaply than building an actual physical server farm. All services are billed based on usage, but each service measures usage in varying ways. As of 2021 Q4, AWS has 33% market share for cloud infrastructure while the next two competitors Microsoft Azure and Google Cloud have 21%, and 10% respectively, according to Synergy Group.",Amazon Web Services,0,1
663,2230,"In computer science, the analysis of algorithms is the process of finding the computational complexity of algorithms—the amount of time, storage, or other resources needed to execute them. Usually, this involves determining a function that relates the size of an algorithm's input to the number of steps it takes (its time complexity) or the number of storage locations it uses (its space complexity). An algorithm is said to be efficient when this function's values are small, or grow slowly compared to a growth in the size of the input. Different inputs of the same size may cause the algorithm to have different behavior, so best, worst and average case descriptions might all be of practical interest.  When not otherwise specified, the function describing the performance of an algorithm is usually an upper bound, determined from the worst case inputs to the algorithm.
The term ""analysis of algorithms"" was coined by Donald Knuth. Algorithm analysis is an important part of a broader computational complexity theory, which provides theoretical estimates for the resources needed by any algorithm which solves a given computational problem. These estimates provide an insight into reasonable directions of search for efficient algorithms.
In theoretical analysis of algorithms it is common to estimate their complexity in the asymptotic sense, i.e., to estimate the complexity function for arbitrarily large input. Big O notation, Big-omega notation and Big-theta notation are used to this end. For instance, binary search is said to run in a number of steps proportional to the logarithm of the size n of the sorted list being searched, or in O(log n), colloquially ""in logarithmic time"". Usually asymptotic estimates are used because different implementations of the same algorithm may differ in efficiency. However the efficiencies of any two ""reasonable"" implementations of a given algorithm are related by a constant multiplicative factor  called a hidden constant.
Exact (not asymptotic) measures of efficiency can sometimes be computed but they usually require certain assumptions concerning the particular implementation of the algorithm, called model of computation. A model of computation may be defined in terms of an abstract computer, e.g. Turing machine, and/or by postulating that certain operations are executed in unit time.
For example, if the sorted list to which we apply binary search has n elements, and we can guarantee that each lookup of an element in the list can be done in unit time, then at most log2(n) + 1 time units are needed to return an answer.",Analysis of algorithms,0,1
664,25277512,"Andrew Yan-Tak Ng (Chinese: 吳恩達; born 1976) is a British-born American computer scientist and technology entrepreneur focusing on machine learning and AI. Ng was a co-founder and head of Google Brain and was the former chief scientist at Baidu, building the company's Artificial Intelligence Group into a team of several thousand people.Ng is an adjunct professor at Stanford University (formerly associate professor and Director of its Stanford AI Lab or SAIL). Ng has also made substantial contributions to the field of online education as the co-founder of both Coursera and deeplearning.ai. He has successfully spearheaded many efforts to ""democratize deep learning"" teaching over 2.5 million students through his online courses. He is one of the world's most famous and influential computer scientists being named one of Time magazine's 100 Most Influential People in 2012, and Fast Company's Most Creative People in 2014. In 2018, he launched and currently heads the AI Fund, initially a $175-million investment fund for backing artificial intelligence startups. He has founded Landing AI, which provides AI-powered SaaS products.",Andrew Ng,0,1
665,15345868,"Angoss Software Corporation, headquartered in Toronto, Ontario, Canada, with offices in the United States and UK, acquired by Datawatch and now owned by Altair, was a provider of predictive analytics systems through software licensing and services. Angoss' customers represent industries including finance, insurance, mutual funds, retail, health sciences, telecom and technology. The company was founded in 1984, and publicly traded on the TSX Venture Exchange from 2008-2013 under the ticker symbol ANC.In June 2013 the private equity firm Peterson Partners acquired Angoss for $8.4 million.",Angoss,0,1
666,8190902,"In data analysis, anomaly detection (also referred to as outlier detection and sometimes as novelty detection) is generally understood to be the identification of rare items, events or observations which deviate significantly from the majority of the data and do not conform to a well defined notion of normal behaviour. Such examples may arouse suspicions of being generated by a different mechanism, or appear inconsistent with the remainder of that set of data.Anomaly detection finds application in many domains including cyber security, medicine, machine vision, statistics, neuroscience, law enforcement and financial fraud to name only a few. Anomalies were initially searched for clear rejection or omission from the data to aid statistical analysis, for example to compute the mean or standard deviation. They were also removed to better predictions from models such as linear regression, and more recently their removal aids the performance of machine learning algorithms. However, in many applications anomalies themselves are of interest and are the observations most desirous in the entire data set, which need to be identified and separated from noise or irrelevant outliers.
Three broad categories of anomaly detection techniques exist. Supervised anomaly detection techniques require a data set that has been labeled as ""normal"" and ""abnormal"" and involves training a classifier. However, this approach is rarely used in anomaly detection due to the general unavailability of labelled data and the inherent unbalanced nature of the classes. Semi-supervised anomaly detection techniques assume that some portion of the data is labelled. This may be any combination of the normal or anomalous data, but more often than not the techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model. Unsupervised anomaly detection techniques assume the data is unlabelled and are by far the most commonly used due to their wider and relevant application.

",Anomaly detection,0,1
667,18706674,"Apache Mahout is a project of the Apache Software Foundation to produce free implementations of distributed or otherwise scalable machine learning algorithms focused primarily on linear algebra. In the past, many of the implementations use the Apache Hadoop platform, however today it is primarily focused on Apache Spark. Mahout also provides Java/Scala libraries for common math operations (focused on linear algebra and statistics) and primitive Java collections. Mahout is a work in progress; a number of algorithms have been implemented.",Apache Mahout,0,1
668,42164234,"Apache Spark  is an open-source unified analytics engine for large-scale data processing. Spark provides an interface for programming clusters with implicit data parallelism and fault tolerance. Originally developed at the University of California, Berkeley's AMPLab, the Spark codebase was later donated to the Apache Software Foundation, which has maintained it since.",Apache Spark,0,1
669,49338480,"Apache SystemDS (Previously, Apache SystemML) is an open source ML system for the end-to-end data science lifecycle. 
SystemDS's distinguishing characteristics are:

Algorithm customizability via R-like and Python-like languages.
Multiple execution modes, including Standalone, Spark Batch, Spark MLContext, Hadoop Batch, and JMLC.
Automatic optimization based on data and cluster characteristics to ensure both efficiency and scalability.",Apache SystemDS,0,1
670,2571015,"Application security (short AppSec) includes all tasks that introduce a secure software development life cycle to development teams. Its final goal is to improve security practices and, through that, to find, fix and preferably prevent security issues within applications. It encompasses the whole application life cycle from requirements analysis, design, implementation, verification as well as maintenance.

",Application security,0,1
671,15893057,"Artificial intelligence (AI) has been used in applications to alleviate certain problems throughout industry and academia. AI, like electricity or computers, is a general purpose technology that has a multitude of applications. It has been used in fields of language translation, image recognition, credit scoring, e-commerce and other domains.",Applications of artificial intelligence,0,1
672,49277634,"Approximate computing is an emerging paradigm for energy-efficient and/or high-performance design. It includes a plethora of computation techniques that return a possibly inaccurate result rather than a guaranteed accurate result, and that can be used for applications where an approximate result is sufficient for its purpose. One example of such situation is for a search engine where no exact answer may exist for a certain search query and hence, many answers may be acceptable. Similarly, occasional dropping of some frames in a video application can go undetected due to perceptual limitations of humans. Approximate computing is based on the observation that in many scenarios, although performing exact computation requires large amount of resources, allowing bounded approximation can provide disproportionate gains in performance and energy, while still achieving acceptable result accuracy.  For example, in k-means clustering algorithm, allowing only 5% loss in classification accuracy can provide 50 times energy saving compared to the fully accurate classification.
The key requirement in approximate computing is that approximation can be introduced only in non-critical data, since approximating critical data (e.g., control operations) can lead to disastrous consequences, such as program crash or erroneous output.",Approximate computing,0,1
673,2052,"In computer science, an array data structure, or simply an array, is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key. An array is stored such that the position of each element can be computed from its index tuple by a mathematical formula. The simplest type of data structure is a linear array, also called one-dimensional array.
For example, an array of 10 32-bit (4-byte) integer variables, with indices 0 through 9, may be stored as 10 words at memory addresses 2000, 2004, 2008, ..., 2036, (in hexadecimal: 0x7D0, 0x7D4, 0x7D8, ..., 0x7F4) so that the element with index i has the address 2000 + (i × 4).The memory address of the first element of an array is called first address, foundation address, or base address.
Because the mathematical concept of a matrix can be represented as a two-dimensional grid, two-dimensional arrays are also sometimes called matrices. In some cases the term ""vector"" is used in computing to refer to an array, although tuples rather than vectors are the more mathematically correct equivalent. Tables are often implemented in the form of arrays, especially lookup tables; the word table is sometimes used as a synonym of array.
Arrays are among the oldest and most important data structures, and are used by almost every program. They are also used to implement many other data structures, such as lists and strings. They effectively exploit the addressing logic of computers. In most modern computers and many external storage devices, the memory is a one-dimensional array of words, whose indices are their addresses. Processors, especially vector processors, are often optimized for array operations.
Arrays are useful mostly because the element indices can be computed at run time. Among other things, this feature allows a single iterative statement to process arbitrarily many elements of an array. For that reason, the elements of an array data structure are required to have the same size and should use the same data representation. The set of valid index tuples and the addresses of the elements (and hence the element addressing formula) are usually, but not always, fixed while the array is in use.
The term array is often used to mean array data type, a kind of data type provided by most high-level programming languages that consists of a collection of values or variables that can be selected by one or more indices computed at run-time. Array types are often implemented by array structures; however, in some languages they may be implemented by hash tables, linked lists, search trees, or other data structures.
The term is also used, especially in the description of algorithms, to mean associative array or ""abstract array"", a theoretical computer science model (an abstract data type or ADT) intended to capture the essential properties of arrays.",Array data structure,0,1
674,10809677,"Arthur Lee Samuel (December 5, 1901 – July 29, 1990) was an American pioneer in the field of computer gaming and artificial intelligence. He popularized the term ""machine learning"" in 1959. The Samuel Checkers-playing Program was among the world's first successful self-learning programs, and as such a very early demonstration of the fundamental concept of artificial intelligence (AI). He was also a senior member in the TeX community who devoted much time giving personal attention to the needs of users and wrote an early TeX manual in 1983.",Arthur Samuel,0,1
675,566680,"Artificial Intelligence: A Modern Approach (AIMA) is a university textbook on artificial intelligence, written by Stuart J. Russell and Peter Norvig. It was first published in 1995 and the fourth edition of the book was released on 28 April 2020. It is used in over 1400 universities worldwide and has been called ""the most popular artificial intelligence textbook in the world"". It is considered the standard text in the field of artificial intelligence.The book is intended for an undergraduate audience but can also be used for graduate-level studies with the suggestion of adding some of the primary sources listed in the extensive bibliography.

",Artificial Intelligence: A Modern Approach,0,1
676,586357,"Artificial general intelligence (AGI) is the ability of an intelligent agent to understand or learn any intellectual task that a human being can.
It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action, although some academic sources reserve the term ""strong AI"" for computer programs that experience sentience or consciousness.In contrast to strong AI, weak AI or ""narrow AI"" is not intended to have general cognitive abilities; rather, weak AI is any program that is designed to solve exactly one problem. (Academic sources reserve ""weak AI"" for programs that do not experience consciousness or do not have a mind in the same sense people do.) Recently, it has been argued that the current research on both AI and cognitive modelling are perfectly aligned with the weak-AI hypothesis (that should not be confused with the ""general"" vs ""narrow"" AI distinction) and that the popular assumption that cognitively inspired AI systems spouse the strong AI hypothesis is  ill-posed and problematic since ""artificial models of brain and mind can be used to understand mental phenomena without pretending that they are the real phenomena that they are modelling""  (as, on the other hand, implied by the strong AI assumption).
A 2020 survey identified 72 active AGI R&D projects spread across 37 countries.",Artificial general intelligence,0,1
677,1589987,"In artificial intelligence, artificial immune systems (AIS) are a class of computationally intelligent, rule-based machine learning systems inspired by the principles and processes of the vertebrate immune system. The algorithms are typically modeled after the immune system's characteristics of learning and memory for use in problem-solving.",Artificial immune system,0,1
678,1164,"Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans. AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.The term ""artificial intelligence"" had previously been used to describe machines that mimic and display ""human"" cognitive skills that are associated with the human mind, such as ""learning"" and ""problem-solving"". This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.AI applications include advanced web search engines (e.g., Google), recommendation systems (used by YouTube, Amazon and Netflix), understanding human speech (such as Siri and Alexa), self-driving cars (e.g., Tesla), automated decision-making and competing at the highest level in strategic game systems (such as chess and Go).
As machines become increasingly capable, tasks considered to require ""intelligence"" are often removed from the definition of AI, a phenomenon known as the AI effect. For instance, optical character recognition is frequently excluded from things considered to be AI, having become a routine technology.Artificial intelligence was founded as an academic discipline in 1956, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an ""AI winter""), followed by new approaches, success and renewed funding. AI research has tried and discarded many different approaches since its founding, including simulating the brain, modeling human problem solving, formal logic, large databases of knowledge and imitating animal behavior. In the first decades of the 21st century, highly mathematical-statistical machine learning has dominated the field, and this technique has proved highly successful, helping to solve many challenging problems throughout industry and academia.The various sub-fields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. General intelligence (the ability to solve an arbitrary problem) is among the field's long-term goals. To solve these problems, AI researchers have adapted and integrated a wide range of problem-solving techniques—including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, probability and economics. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields.
The field was founded on the assumption that human intelligence ""can be so precisely described that a machine can be made to simulate it"".
This raised philosophical arguments about the mind and the ethical consequences of creating artificial beings endowed with human-like intelligence; these issues have previously been explored by myth, fiction and philosophy since antiquity. Computer scientists and philosophers have since suggested that AI may become an existential risk to humanity if its rational capacities are not steered towards beneficial goals.

",Artificial intelligence,0,1
680,349771,"An artificial neuron is a mathematical function conceived as a model of biological neurons, a neural network. Artificial neurons are elementary units in an artificial neural network. The artificial neuron receives one or more inputs (representing excitatory postsynaptic potentials and inhibitory postsynaptic potentials at neural dendrites) and sums them to produce an output (or activation, representing a neuron's action potential which is transmitted along its axon). Usually each input is separately weighted, and the sum is passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape, but they may also take the form of other non-linear functions, piecewise linear functions, or step functions. They are also often  monotonically increasing, continuous, differentiable and bounded. Non-monotonic, unbounded and oscillating activation functions with multiple zeros that outperform sigmoidal and ReLU like activation functions on many tasks have also been recently explored. The thresholding function has inspired building logic gates referred to as threshold logic; applicable to building logic circuits resembling brain processing. For example, new devices such as memristors have been extensively used to develop such logic in recent times.The artificial neuron transfer function should not be confused with a linear system's transfer function.",Artificial neuron,0,1
681,644058,"The Association for Computational Linguistics (ACL) is a scientific and professional organization for people working on natural language processing. Its namesake conference is one of the primary high impact conferences for natural language processing research, along with EMNLP. The conference is held each summer in locations where significant computational linguistics research is carried out.
It was founded in 1962, originally named the Association for Machine Translation and Computational Linguistics (AMTCL). It became the ACL in 1968. The ACL has a European (EACL), a North American (NAACL), and an Asian (AACL) chapter.

",Association for Computational Linguistics,0,1
682,577053,"Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. It is intended to identify strong rules discovered in databases using some measures of interestingness. In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.
Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. For example, the rule 
  
    
      
        {
        
          o
          n
          i
          o
          n
          s
          ,
          p
          o
          t
          a
          t
          o
          e
          s
        
        }
        ⇒
        {
        
          b
          u
          r
          g
          e
          r
        
        }
      
    
    {\displaystyle \{\mathrm {onions,potatoes} \}\Rightarrow \{\mathrm {burger} \}}
   found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
In addition to the above example from market basket analysis association rules are employed today in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
The association rule algorithm itself consists of various parameters that can make it difficult for those without some expertise in data mining to execute, with many rules that are arduous to understand.Despite this, association rule learning is a great system for predicting the behavior in data interconnections. This makes it a noteworthy technique for classification, or discovering patterns in data, when implementing machine learning methods.",Association rule learning,0,1
683,28326718,"Astroinformatics is an interdisciplinary field of study involving the combination of astronomy, data science, machine learning, informatics, and information/communications technologies.",Astroinformatics,0,1
684,66001552,"In neural networks, attention is a technique that mimics cognitive attention. The effect enhances some parts of the input data while diminishing other parts — the motivation being that the network should devote more focus to the small, but important, parts of the data. Learning which part of the data is more important than another depends on the context, and this is trained by gradient descent.
Attention-like mechanisms were introduced in the 1990s under names like multiplicative modules, sigma pi units, and hypernetworks. Its flexibility comes from its role as ""soft weights"" that can change during runtime, in contrast to standard weights that must remain fixed at runtime.  Uses of attention include  memory in neural Turing machines, reasoning tasks in differentiable neural computers, language processing in transformers, and multi-sensory data processing (sound, images, video, and text) in perceivers.",Attention (machine learning),0,1
685,6836612,"An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning). The encoding is validated and refined by attempting to regenerate the input from the encoding. The autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (“noise”). 
Variants exist, aiming to force the learned representations to assume useful properties. Examples are regularized autoencoders (Sparse, Denoising and Contractive), which are effective in learning representations for subsequent classification tasks, and Variational autoencoders, with applications as generative models. Autoencoders are applied to many problems, including facial recognition, feature detection, anomaly detection and acquiring the meaning of words. Autoencoders are also generative models which can randomly generate new data that is similar to the input data (training data).",Autoencoder,0,1
686,103356,"Automata theory is the study of abstract machines and automata, as well as the computational problems that can be solved using them. It is a theory in theoretical computer science. The word automata comes from the Greek word αὐτόματος, which means ""self-acting, self-willed, self-moving"". An automaton (Automata in plural) is an abstract self-propelled computing device which follows a predetermined sequence of operations automatically. An automaton with a finite number of states is called a Finite Automaton (FA) or Finite-State Machine (FSM). The figure at right illustrates a finite-state machine, which belongs to a well-known type of automaton.  This automaton consists of states (represented in the figure by circles) and transitions (represented by arrows).  As the automaton sees a symbol of input, it makes a transition (or jump) to another state, according to its transition function, which takes the previous state and current input symbol as its arguments.
Automata theory is closely related to formal language theory. In this context, automata are used as finite representations of formal languages that may be infinite. Automata are often classified by the class of formal languages they can recognize, as in the Chomsky hierarchy, which describes a nesting relationship between major classes of automata. Automata play a major role in theory of computation, compiler construction, artificial intelligence, parsing and formal verification.",Automata theory,0,1
687,68710620,"Automated decision-making (ADM) involves the use of data, machines and algorithms to make decisions in a range of contexts, including public administration, business, health, education, law, employment, transport, media and entertainment, with varying degrees of human oversight or intervention. ADM involves large-scale data from a range of sources, such as databases, text, social media, sensors, images or speech, that is processed using various technologies including computer software, algorithms, machine learning, natural language processing, artificial intelligence, augmented intelligence and robotics. The increasing use of automated decision-making systems (ADMS) across a range of contexts presents many benefits and challenges to human society requiring consideration of the technical, legal, ethical, societal, educational, economic and health consequences.

",Automated decision-making,0,1
688,55843837,"Automated machine learning (AutoML) is the process of automating the tasks of applying machine learning to real-world problems. AutoML potentially includes every stage from beginning with a raw dataset to building a machine learning model ready for deployment. AutoML was proposed as an artificial intelligence-based solution to the growing challenge of applying machine learning. The high degree of automation in AutoML aims to allow non-experts to make use of machine learning models and techniques without requiring them to become experts in machine learning. Automating the process of applying machine learning end-to-end additionally offers the advantages of producing simpler solutions, faster creation of those solutions, and models that often outperform hand-designed models. Common techniques used in AutoML include hyperparameter optimization, meta-learning and neural architecture search.",Automated machine learning,0,1
689,9034035,"Computer-aided detection (CADe), also called computer-aided diagnosis (CADx), are systems that assist doctors in the interpretation of medical images. Imaging techniques in X-ray, MRI, Endoscopy, and ultrasound diagnostics yield a great deal of information that the radiologist or other medical professional has to analyze and evaluate comprehensively in a short time. CAD systems process digital images or videos for typical appearances and to highlight conspicuous sections, such as possible diseases, in order to offer input to support a decision taken by the professional.
CAD also has potential future applications in digital pathology with the advent of whole-slide imaging and machine learning algorithms. So far its application has been limited to quantifying immunostaining but is also being investigated for the standard H&E stain.CAD is an interdisciplinary technology combining elements of artificial intelligence and computer vision with radiological and pathology image processing. A typical application is the detection of a tumor. For instance, some hospitals use CAD to support preventive medical check-ups in mammography (diagnosis of breast cancer), the detection of polyps in Colonoscopy, and lung cancer.
Computer-aided detection (CADe) systems are usually confined to marking conspicuous structures and sections. Computer-aided diagnosis (CADx) systems evaluate the conspicuous structures. For example, in mammography CAD highlights microcalcification clusters and hyperdense structures in the soft tissue. This allows the radiologist to draw conclusions about the condition of the pathology. Another application is CADq, which quantifies, e.g., the size of a tumor or the tumor's behavior in contrast medium uptake. Computer-aided simple triage (CAST) is another type of CAD, which performs a fully automatic initial interpretation and triage of studies into some meaningful categories (e.g. negative and positive). CAST is particularly applicable in emergency diagnostic imaging, where a prompt diagnosis of critical, life-threatening condition is required.
Although CAD has been used in clinical environments for over 40 years, CAD usually does not substitute the doctor or other professional, but rather plays a supporting role. The professional (generally a radiologist) is generally responsible for the final interpretation of a medical image. However, the goal of some CAD systems is to detect earliest signs of abnormality in patients that human professionals cannot, as in diabetic retinopathy, architectural distortion in mammograms, ground-glass nodules in thoracic CT, and non-polypoid (“flat”) lesions in CT colonography.",Computer-aided diagnosis,0,1
690,1505641,"Automated planning and scheduling, sometimes denoted as simply AI planning, is a branch of artificial intelligence that concerns the realization of strategies or action sequences, typically for execution by intelligent agents, autonomous robots and unmanned vehicles. Unlike classical control and classification problems, the solutions are complex and must be discovered and optimized in multidimensional space. Planning is also related to decision theory.
In known environments with available models, planning can be done offline. Solutions can be found and evaluated prior to execution. In dynamically unknown environments, the strategy often needs to be revised online. Models and policies must be adapted. Solutions usually resort to iterative trial and error processes commonly seen in artificial intelligence. These include dynamic programming, reinforcement learning and combinatorial optimization. Languages used to describe planning and scheduling are often called action languages.",Automated planning and scheduling,0,1
691,2546,"Automated theorem proving (also known as ATP or automated deduction) is a subfield of automated reasoning and mathematical logic dealing with proving mathematical theorems by computer programs.  Automated reasoning over mathematical proof was a major impetus for the development of computer science.

",Automated theorem proving,0,1
692,734787,"In mathematics and computer algebra, automatic differentiation (AD), also called algorithmic differentiation, computational differentiation, auto-differentiation, or simply autodiff, is a set of techniques to evaluate the derivative of a function specified by a computer program. AD exploits the fact that every computer program, no matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). By applying the chain rule repeatedly to these operations, derivatives of arbitrary order can be computed automatically, accurately to working precision, and using at most a small constant factor more arithmetic operations than the original program.

Automatic differentiation is distinct from symbolic differentiation and numerical differentiation. 
Symbolic differentiation faces the difficulty of converting a computer program into a single mathematical expression and can lead to inefficient code. Numerical differentiation (the method of finite differences) can introduce round-off errors in the discretization process and cancellation. Both of these classical methods have problems with calculating higher derivatives, where complexity and errors increase. Finally, both of these classical methods are slow at computing partial derivatives of a function with respect to many inputs, as is needed for gradient-based optimization algorithms. Automatic differentiation solves all of these problems.",Automatic differentiation,0,1
693,245926,"A self-driving car, also known as an autonomous car, driver-less car, or robotic car (robo-car), is a car incorporating vehicular automation, that is, a ground vehicle that is capable of sensing its environment and moving safely with little or no human input.Self-driving cars combine a variety of sensors to perceive their surroundings, such as thermographic cameras, radar, lidar, sonar, GPS, odometry and inertial measurement units. Advanced control systems interpret sensory information to identify appropriate navigation paths, as well as obstacles and relevant signage.Autonomy in vehicles is often categorized in six levels, according to a system developed by SAE International (SAE J3016, revised periodically). The SAE levels can be roughly understood as Level 0 - no automation; Level 1 - hands on/shared control; Level 2 - hands off; Level 3 - eyes off; Level 4 - mind off, and Level 5 - steering wheel optional.
As of March 2022, vehicles operating at Level 3 and above remain a marginal portion of the market. 
In December 2020, Waymo became the first service provider to offer driver-less taxi rides to the general public, in a part of Phoenix, Arizona. In March 2021, Honda became the first manufacturer to provide a legally approved Level 3 car, and Toyota operated a potentially Level 4 service around the Tokyo 2020 Olympic Village. Nuro has been allowed to start autonomous commercial delivery operations in California in 2021. In December 2021, Mercedes-Benz became the second manufacturer to receive legal approval for a Level 3 car complying with legal requirements. In February 2022, Cruise became the second service provider to offer driver-less taxi rides to the general public, in San Francisco in the US.In China, two publicly accessible trials of robotaxis have been launched, in 2020 in Shenzhen's Pingshan District by Chinese firm AutoX and in 2021 at Shougang Park in Beijing by Baidu, a venue for the 2022 Winter Olympics.

",Self-driving car,1,1
694,1434444,"In statistics, econometrics and signal processing, an autoregressive (AR) model is a representation of a type of random process; as such, it is used to describe certain time-varying processes in nature, economics, etc. The autoregressive model specifies that the output variable depends linearly on its own previous values and on a stochastic term (an imperfectly predictable term); thus the model is in the form of a stochastic difference equation (or recurrence relation which should not be confused with differential equation). Together with the moving-average (MA) model, it is a special case and key component of the more general autoregressive–moving-average (ARMA) and autoregressive integrated moving average (ARIMA) models of time series, which have a more complicated stochastic structure; it is also a special case of the vector autoregressive model (VAR), which consists of a system of more than one interlocking stochastic difference equation in more than one evolving random variable.
Contrary to the moving-average (MA) model, the autoregressive model is not always stationary as it may contain a unit root.",Autoregressive model,0,1
695,19961416,"Microsoft Azure, often referred to as Azure ( AZH-ər, AY-zhər, UK also  AZ-ure, AY-zure), is a cloud computing service operated by Microsoft for application management via Microsoft-managed data centers. It provides software as a service (SaaS), platform as a service (PaaS) and infrastructure as a service (IaaS) and supports many different programming languages, tools, and frameworks, including both Microsoft-specific and third-party software and systems.
Azure, announced at Microsoft's Professional Developers Conference (PDC) in October 2008, went by the internal project codename ""Project Red Dog"", and formally released in February 2010, as Windows Azure before being renamed Microsoft Azure on March 25, 2014.",Microsoft Azure,0,1
696,62026514,"Bidirectional Encoder Representations from Transformers (BERT) is a transformer-based machine learning technique for natural language processing (NLP) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. In 2019, Google announced that it had begun leveraging BERT in its search engine, and by late 2020 it was using BERT in almost every English-language query.  A 2020 literature survey concluded that ""in a little over a year, BERT has become a ubiquitous baseline in NLP experiments"", counting over 150 research publications analyzing and improving the model.The original English-language BERT has two models: (1) the BERTBASE: 12 encoders with 12 bidirectional self-attention heads, and (2) the BERTLARGE: 24 encoders with 16 bidirectional self-attention heads. Both models are pre-trained from unlabeled data extracted from the BooksCorpus with 800M words and English Wikipedia with 2,500M words.

",BERT (language model),0,1
697,83443,"Birth is the act or process of bearing or bringing forth offspring, also referred to in technical contexts as parturition. In mammals, the process is initiated by hormones which cause the muscular walls of the uterus to contract, expelling the fetus at a developmental stage when it is ready to feed and breathe.
In some species the offspring is precocial and can move around almost immediately after birth but in others it is altricial and completely dependent on parenting.
In marsupials, the fetus is born at a very immature stage after a short gestational period and develops further in its mother's womb's  pouch.
It is not only mammals that give birth. Some reptiles, amphibians, fish and invertebrates carry their developing young inside them. Some of these are ovoviviparous, with the eggs being hatched inside the mother's body, and others are viviparous, with the embryo developing inside her body, as in the case of mammals.

",Birth,0,1
698,360788,"A backdoor is a typically covert method of bypassing normal authentication or encryption in a computer, product, embedded device (e.g. a home router), or its embodiment (e.g. part of a cryptosystem, algorithm, chipset, or even a ""homunculus computer"" —a tiny computer-within-a-computer such as that found in Intel's AMT technology). Backdoors are most often used for securing remote access to a computer, or obtaining access to plaintext in cryptographic systems. From there it may be used to gain access to privileged information like passwords, corrupt or delete data on hard drives, or transfer information within autoschediastic networks.
A backdoor may take the form of a hidden part of a program, a separate program (e.g. Back Orifice may subvert the system through a rootkit), code in the firmware of the hardware, or parts of an operating system such as Windows. Trojan horses can be used to create vulnerabilities in a device. A Trojan horse may appear to be an entirely legitimate program, but when executed, it triggers an activity that may install a backdoor. Although some are secretly installed, other backdoors are deliberate and widely known. These kinds of backdoors have ""legitimate"" uses such as providing the manufacturer with a way to restore user passwords.
Many systems that store information within the cloud fail to create accurate security measures. If many systems are connected within the cloud, hackers can gain access to all other platforms through the most vulnerable system.Default passwords (or other default credentials) can function as backdoors if they are not changed by the user. Some debugging features can also act as backdoors if they are not removed in the release version.In 1993, the United States government attempted to deploy an encryption system, the Clipper chip, with an explicit backdoor for law enforcement and national security access. The chip was unsuccessful.",Backdoor (computing),0,1
699,1360091,"In machine learning, backpropagation (backprop, BP) is a widely used algorithm for training feedforward neural networks. Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally. These classes of algorithms are all referred to generically as ""backpropagation"". In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. This efficiency makes it feasible to use gradient methods for training multilayer networks, updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.The term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent. Backpropagation generalizes the gradient computation in the delta rule, which is the single-layer version of backpropagation, and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or ""reverse mode""). The term backpropagation and its general use in neural networks was announced in Rumelhart, Hinton & Williams (1986a), then elaborated and popularized in Rumelhart, Hinton & Williams (1986b), but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see § History. A modern overview is given in the deep learning textbook by Goodfellow, Bengio & Courville (2016).",Backpropagation,0,1
700,486777,"Bank fraud is the use of potentially illegal means to obtain money, assets, or other property owned or held by a financial institution, or to obtain money from depositors by fraudulently posing as a bank or other financial institution. In many instances, bank fraud is a criminal offence. While the specific elements of particular banking fraud laws vary depending on jurisdictions, the term bank fraud applies to actions that employ a scheme or artifice, as opposed to bank robbery or theft. For this reason, bank fraud is sometimes considered a white-collar crime.",Bank fraud,0,1
701,1274090,"Basic Books is a book publisher founded in 1950 and located in New York, now an imprint of Hachette Book Group. It publishes books in the fields of psychology, philosophy, economics, science, politics, sociology, current affairs, and history.

",Basic Books,0,1
702,50909,"In mathematics,  a basis function is an element of a particular basis for a function space. Every function in the function space can be represented as a linear combination of basis functions, just as every vector in a vector space can be represented as a linear combination of basis vectors.
In numerical analysis and approximation theory, basis functions are also called blending functions, because of their use in interpolation: In this application, a mixture of the basis functions provides an interpolating function (with the ""blend"" depending on the evaluation of the basis functions at the data points).",Basis function,0,1
703,57222123,"Batch normalization (also known as batch norm) is a method used to make training of artificial neural networks faster and more stable through normalization of the layers' inputs by re-centering and re-scaling. It was proposed by Sergey Ioffe and Christian Szegedy in 2015.While the effect of batch normalization is evident, the reasons behind its effectiveness remain under discussion. It was believed that it can mitigate the problem of internal covariate shift, where parameter initialization and changes in the distribution of the inputs of each layer affect the learning rate of the network. Recently, some scholars have argued that batch normalization does not reduce internal covariate shift, but rather smooths the objective function, which in turn improves the performance. However, at initialization, batch normalization in fact induces severe gradient explosion in deep networks, which is only alleviated by skip connections in residual networks. Others sustain that batch normalization achieves length-direction decoupling, and thereby accelerates neural networks. More recently a normalize gradient clipping technique and smart hyperparameter tuning has been introduced in Normalizer-Free Nets, so called ""NF-Nets"" which mitigates the need for batch normalization.",Batch normalization,0,1
705,540801,"Behaviorism is a systematic approach to understanding the behavior of humans and other animals. It assumes that behavior is either a reflex evoked by the pairing of certain antecedent stimuli in the environment, or a consequence of that individual's history, including especially reinforcement and punishment contingencies, together with the individual's current motivational state and controlling stimuli.  Although behaviorists generally accept the important role of heredity in determining behavior, they focus primarily on environmental events.
Behaviorism emerged in the early 1900s as a reaction to depth psychology and other traditional forms of psychology, which often had difficulty making predictions that could be tested experimentally, but derived from earlier research in the late nineteenth century, such as when Edward Thorndike pioneered the law of effect, a procedure that involved the use of consequences to strengthen or weaken behavior.
With a 1924 publication, John B. Watson devised methodological behaviorism, which rejected introspective methods and sought to understand behavior by only measuring observable behaviors and events. It was not until the 1930s that B. F. Skinner suggested that covert behavior—including cognition and emotions—is subject to the same controlling variables as observable behavior, which became the basis for his philosophy called radical behaviorism. While Watson and Ivan Pavlov investigated how (conditioned) neutral stimuli elicit reflexes in respondent conditioning, Skinner assessed the reinforcement histories of the discriminative (antecedent) stimuli that emits behavior; the technique became known as operant conditioning.
The application of radical behaviorism—known as applied behavior analysis—is used in a variety of contexts, including, for example, applied animal behavior and organizational behavior management to treatment of mental disorders, such as autism and substance abuse. In addition, while behaviorism and cognitive schools of psychological thought do not agree theoretically, they have complemented each other in the cognitive-behavior therapies, which have demonstrated utility in treating certain pathologies, including simple phobias, PTSD, and mood disorders.",Behaviorism,0,1
706,40678189,"In statistics and machine learning, the bias–variance tradeoff is the property of a model that the variance of the parameter estimated across samples can be reduced by increasing the bias in the estimated parameters.
The bias–variance dilemma or bias–variance problem is the conflict in trying to simultaneously minimize these two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:
The bias error is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
The variance is an error from sensitivity to small fluctuations in the training set. High variance may result from an algorithm modeling the random noise in the training data (overfitting).The bias–variance decomposition is a way of analyzing a learning algorithm's expected generalization error with respect to a particular problem as a sum of three terms, the bias, variance, and a quantity called the irreducible error, resulting from noise in the problem itself.

",Bias–variance tradeoff,1,1
710,205393,"Binary classification is the task of classifying the elements of a set into two groups (each called class) on the basis of a classification rule. Typical binary classification problems include:

Medical testing to determine if a patient has certain disease or not;
Quality control in industry, deciding whether a specification has been met;
In information retrieval, deciding whether a page should be in the result set of a search or not.Binary classification is dichotomization applied to a practical situation. In many practical binary classification problems, the two groups are not symmetric, and rather than overall accuracy, the relative proportion of different types of errors is of interest. For example, in medical testing, detecting a disease when it is not present (a false positive) is considered differently from not detecting a disease when it is present (a false negative).

",Binary classification,0,1
712,1729542,"A neural network is a network or circuit of biological neurons, or, in a modern sense, an artificial neural network, composed of artificial neurons or nodes. Thus, a neural network is either a biological neural network, made up of biological neurons, or an artificial neural network, used for solving artificial intelligence (AI) problems. The connections of the biological neuron are modeled in artificial neural networks as weights between nodes. A positive weight reflects an excitatory connection, while negative values mean inhibitory connections. All inputs are modified by a weight and summed. This activity is referred to as a linear combination. Finally, an activation function controls the amplitude of the output. For example, an acceptable range of output is usually between 0 and 1, or it could be −1 and 1.
These artificial networks may be used for predictive modeling, adaptive control and applications where they can be trained via a dataset. Self-learning resulting from experience can occur within networks, which can derive conclusions from a complex and seemingly unrelated set of information.

",Neural network,1,1
714,90500,"In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms that convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): ""Can a set of weak learners create a single strong learner?"" A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
Robert Schapire's affirmative answer in a 1990 paper to the question of Kearns and Valiant has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner. ""Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."" Algorithms that achieve hypothesis boosting quickly became simply known as ""boosting"". Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.

",Boosting (machine learning),0,1
715,1307911,"Bootstrap aggregating, also called bagging (from bootstrap aggregating), is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.

",Bootstrap aggregating,0,1
716,4211,"In general, bootstrapping usually refers to a self-starting process that is supposed to continue or grow without external input.

",Bootstrapping,0,1
717,19009110,"Rain is liquid water in the form of droplets that have condensed from atmospheric water vapor and then become heavy enough to fall under gravity. Rain is a major component of the water cycle and is responsible for depositing most of the fresh water on the Earth. It provides suitable conditions for many types of ecosystems, as well as water for hydroelectric power plants and crop irrigation.
The major cause of rain production is moisture moving along three-dimensional zones of temperature and moisture contrasts known as weather fronts. If enough moisture and upward motion is present, precipitation falls from convective clouds (those with strong upward vertical motion) such as cumulonimbus (thunder clouds) which can organize into narrow rainbands. In mountainous areas, heavy precipitation is possible where upslope flow is maximized within windward sides of the terrain at elevation which forces moist air to condense and fall out as rainfall along the sides of mountains. On the leeward side of mountains, desert climates can exist due to the dry air caused by downslope flow which causes heating and drying of the air mass. The movement of the monsoon trough, or intertropical convergence zone, brings rainy seasons to savannah climes.
The urban heat island effect leads to increased rainfall, both in amounts and intensity, downwind of cities. Global warming is also causing changes in the precipitation pattern globally, including wetter conditions across eastern North America and drier conditions in the tropics. Antarctica is the driest continent. The globally averaged annual precipitation over land is 715 mm (28.1 in), but over the whole Earth it is much higher at 990 mm (39 in). Climate classification systems such as the Köppen classification system use average annual rainfall to help differentiate between differing climate regimes. Rainfall is measured using rain gauges. Rainfall amounts can be estimated by weather radar.
Rain is also known or suspected on other planets, where it may be composed of methane, neon, sulfuric acid, or even iron rather than water.",Rain,0,1
718,623686,"A brain–computer interface (BCI), sometimes called a brain–machine interface (BMI), is a direct communication pathway between the brain's electrical activity and an external device, most commonly a computer or robotic limb. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions. Implementations of BCIs range from non-invasive (EEG, MEG, EOG, MRI) and partially invasive (ECoG and endovascular) to invasive (microelectrode array), based on how close electrodes get to brain tissue.Research on BCIs began in the 1970s by Jacques Vidal at the University of California, Los Angeles (UCLA) under a grant from the National Science Foundation, followed by a contract from DARPA. The Vidal's 1973 paper marks the first appearance of the expression brain–computer interface in scientific literature.
Due to the cortical plasticity of the brain, signals from implanted prostheses can, after adaptation, be handled by the brain like natural sensor or effector channels. Following years of animal experimentation, the first neuroprosthetic devices implanted in humans appeared in the mid-1990s.
Recently, studies in human-computer interaction via the application of machine learning to statistical temporal features extracted from the frontal lobe (EEG brainwave) data has had high levels of success in classifying mental states (Relaxed, Neutral, Concentrating), mental emotional states (Negative, Neutral, Positive) and thalamocortical dysrhythmia.

",Brain–computer interface,0,1
719,22643107,"CURE (Clustering Using REpresentatives) is an efficient data clustering algorithm for large databases. Compared with K-means clustering it is more robust to outliers and able to identify clusters having non-spherical shapes and size variances.

",CURE algorithm,0,1
720,6299994,"Classical cable theory uses mathematical models to calculate the electric current (and accompanying voltage) along passive neurites, particularly the dendrites that receive synaptic inputs at different sites and times. Estimates are made by modeling dendrites and axons as cylinders composed of segments with capacitances 
  
    
      
        
          c
          
            m
          
        
      
    
    {\displaystyle c_{m}}
   and resistances 
  
    
      
        
          r
          
            m
          
        
      
    
    {\displaystyle r_{m}}
   combined in parallel (see Fig. 1). The capacitance of a neuronal fiber comes about because electrostatic forces are acting through the very thin lipid bilayer (see Figure 2). The resistance in series along the fiber 
  
    
      
        
          r
          
            l
          
        
      
    
    {\displaystyle r_{l}}
   is due to the axoplasm's significant resistance to movement of electric charge.",Cable theory,0,1
721,53631046,"Caffe (Convolutional Architecture for Fast Feature Embedding) is a deep learning framework, originally developed at University of California, Berkeley. It is open source, under a BSD license. It is written in C++, with a Python interface.",Caffe (software),0,1
724,27809,"Chemical synapses are biological junctions through which neurons' signals can be sent to each other and to non-neuronal cells such as those in muscles or glands. Chemical synapses allow neurons to form circuits within the central nervous system. They are crucial to the biological computations that underlie perception and thought. They allow the nervous system to connect to and control other systems of the body.
At a chemical synapse, one neuron releases neurotransmitter molecules into a small space (the synaptic cleft) that is adjacent to another neuron. The neurotransmitters are contained within small sacs called synaptic vesicles, and are released into the synaptic cleft by exocytosis. These molecules then bind to neurotransmitter receptors on the postsynaptic cell. Finally, the neurotransmitters are cleared from the synapse through one of several potential mechanisms including enzymatic degradation or re-uptake by specific transporters either on the presynaptic cell or on some other neuroglia to terminate the action of the neurotransmitter.
The adult human brain is estimated to contain from 1014 to 5 × 1014 (100–500 trillion) synapses. Every cubic millimeter of cerebral cortex contains roughly a billion (short scale, i.e. 109) of them. The number of synapses in the  human cerebral cortex has separately been estimated at 0.15 quadrillion (150 trillion)The word ""synapse"" was introduced by Sir Charles Scott Sherrington in 1897. Chemical synapses are not the only type of biological synapse: electrical and immunological synapses also exist. Without a qualifier, however, ""synapse"" commonly refers to chemical synapse.",Chemical synapse,0,1
725,575697,"Cheminformatics (also known as chemoinformatics) refers to use of physical chemistry theory with computer and information science techniques—so called ""in silico"" techniques—in application to a range of descriptive and prescriptive problems in the field of chemistry, including in its applications to biology and related molecular fields. Such in silico techniques are used, for example, by pharmaceutical companies and in academic settings to aid and inform the process of drug discovery, for instance in the design of well-defined combinatorial libraries of synthetic compounds, or to assist in structure-based drug design. The methods can also be used in chemical and allied industries, and such fields as environmental science and pharmacology, where chemical processes are involved or studied.",Cheminformatics,0,1
726,6216,"The Chinese room argument holds that a digital computer executing a program cannot have a ""mind"", ""understanding"" or ""consciousness"", regardless of how intelligently or human-like the program may make the computer behave.  The argument was presented by philosopher John Searle in his paper, ""Minds, Brains, and Programs"", published in Behavioral and Brain Sciences in 1980. Similar arguments were presented by Gottfried Leibniz (1714),  Anatoly Dneprov (1961), Lawrence Davis (1974) and Ned Block (1978). Searle's version has been widely discussed in the years since. The centerpiece of Searle's argument is a thought experiment known as the Chinese room.The argument is directed against the philosophical positions of functionalism and computationalism, which hold that the mind may be viewed as an information-processing system operating on formal symbols, and that simulation of a given mental state is sufficient for its presence. Specifically, the argument is intended to refute a position Searle calls strong AI: ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""Although it was originally presented in reaction to the statements of artificial intelligence (AI) researchers, it is not an argument against the goals of mainstream AI research because it does not show a limit in the amount of ""intelligent"" behavior a machine can display. The argument applies only to digital computers running programs and does not apply to machines in general.",Chinese room,0,1
727,8760516,"Christopher Michael Bishop (born 7 April 1959)   is the Laboratory Director at Microsoft Research Cambridge, Honorary Professor of Computer Science at the University of Edinburgh and a Fellow of Darwin College, Cambridge. Bishop is a member of the UK AI Council. He was also recently appointed to the Prime Minister's Council for Science and Technology.",Christopher Bishop,1,1
729,555480,"In genetic algorithms, a chromosome (also sometimes called a genotype) is a set of parameters which define a proposed solution to the problem that the genetic algorithm is trying to solve. The set of all solutions is known as the population. The chromosome is often represented as a binary string, although a wide variety of other data structures are also used.",Chromosome (genetic algorithm),0,1
730,2155752,"Citizen science (CS; also known as community science, crowd science, crowd-sourced science, civic science, or volunteer monitoring) is scientific research conducted, in whole or in part, by amateur (or nonprofessional) scientists. Citizen science is sometimes described as ""public participation in scientific research"", participatory monitoring, and participatory action research whose outcomes are often advancements in scientific research by improving the scientific community's capacity, as well as increasing the public's understanding of science.Well-known citizen scientists include Isaac Newton, Florence Nightingale, Benjamin Franklin and Charles Darwin. First uses of the term CS can be found in the magazine New Scientist and in campaigns to raise awareness of acid rain. An example of a CS discovery is that of the planet Uranus which was first observed by William Herschel in 1781. The advancement of information technology has had practical benefits for people, such as the Sapelli app that has helped track illegal forestry. Large photographic databases can be examined by groups of participants providing scientifically credible results, as seen with CS project Snapshot Serengetti. As CS grows, subjects such as ethics and economic worth become formally studied while a scientific journal hopes to enhance quality and impact through its articles. The impact of CS internationally can be seen in a series of programs shown by American Public Television and available online. Theorists that have examined CS include The American Philosophical Society, Isabelle Stengers and Paul Feyerabend.",Citizen science,0,1
731,142440,"Climatology (from Greek κλίμα, klima, ""place, zone""; and -λογία, -logia) or climate science is the scientific study of Earth's climate, typically defined as weather conditions averaged over a period of at least 30 years. This modern field of study is regarded as a branch of the atmospheric sciences and a subfield of physical geography, which is one of the Earth sciences. Climatology now includes aspects of oceanography and biogeochemistry.
The main methods employed by climatologists are the analysis of observations and modelling of the physical processes that determine the climate. The main topics of research are the study of climate variability, mechanisms of climate changes and modern climate change. Basic knowledge of climate can be used within shorter term weather forecasting, for instance about climatic cycles such as the El Niño–Southern Oscillation (ENSO), the Madden–Julian oscillation (MJO), the North Atlantic oscillation (NAO), the Arctic oscillation (AO), the Pacific decadal oscillation (PDO), and the Interdecadal Pacific Oscillation (IPO).  
Climate models are used for a variety of purposes from study of the dynamics of the weather and climate system to projections of future climate. Weather is known as the condition of the atmosphere over a period of time, while climate has to do with the atmospheric condition over an extended to indefinite period of time.",Climatology,0,1
733,42581062,"Cognitive computing (CC) refers to technology platforms that, broadly speaking, are based on the scientific disciplines of artificial intelligence and signal processing.  These platforms encompass machine learning, reasoning, natural language processing, speech recognition and vision (object recognition), human–computer interaction, dialog and narrative generation, among other technologies.",Cognitive computing,0,1
734,5739,"In computing, a compiler is a computer program that translates computer code written in one programming language (the source language) into another language (the target language). The name ""compiler"" is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g. assembly language, object code, or machine code) to create an executable program.: p1 There are many different types of compilers which produce output in different useful forms. A cross-compiler  produces code for a different CPU or operating system than the one on which the cross-compiler itself runs.  A bootstrap compiler is often a temporary compiler, used for compiling a more permanent or better optimised compiler for a language.
Related software include, a program that translates from a low-level language to a higher level one is a decompiler ; a program that translates between high-level languages, usually called a source-to-source compiler or transpiler.  A language rewriter is usually a program that translates the form of expressions without a change of language.  A compiler-compiler is a compiler that produces a compiler (or part of one), often in a generic and reusable way so as to be able to produce many differing compilers.
A compiler is likely to perform some or all of the following operations, often called phases: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers generally implement these phases as modular components, promoting efficient design and correctness of transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.Compilers are not the only language processor used to transform source programs. An interpreter is computer software that transforms and then executes the indicated operations.: p2  The translation process influences the design of computer languages, which leads to a preference of compilation or interpretation. In theory, a programming language can have both a compiler and an interpreter. In practice, programming languages tend to be associated with just one (a compiler or an interpreter).",Compiler,0,1
735,155414,"Computability theory, also known as recursion theory, is a branch of mathematical logic, computer science, and the theory of computation that originated in the 1930s with the study of computable functions and Turing degrees. The field has since expanded to include the study of generalized computability and definability. In these areas, computability theory overlaps with proof theory and effective descriptive set theory.
Basic questions addressed by computability theory include:

What does it mean for a function on the natural numbers to be computable?
How can noncomputable functions be classified into a hierarchy based on their level of noncomputability?Although there is considerable overlap in terms of knowledge and methods, mathematical computability theorists study the theory of relative computability, reducibility notions, and degree structures; those in the computer science field focus on the theory of subrecursive hierarchies, formal methods, and formal languages.",Computability theory,0,1
736,48520204,"Computational anatomy is an interdisciplinary field of biology focused on quantitative investigation and modelling of anatomical shapes variability. It involves the development and application of mathematical, statistical and data-analytical methods for modelling and simulation of biological structures.
The field is broadly defined and includes foundations in anatomy, applied mathematics and pure mathematics, machine learning, computational mechanics, computational science, biological imaging, neuroscience, physics, probability, and statistics; it also has strong connections with fluid mechanics and geometric mechanics. Additionally, it complements newer, interdisciplinary fields like bioinformatics and neuroinformatics in the sense that its interpretation uses metadata derived from the original sensor imaging modalities (of which magnetic resonance imaging is one example). It focuses on the anatomical structures being imaged, rather than the medical imaging devices. It is similar in spirit to the history of computational linguistics, a discipline that focuses on the linguistic structures rather than the sensor acting as the transmission and communication media.
In computational anatomy, the diffeomorphism group is used to study different coordinate systems via coordinate transformations as generated via the Lagrangian and Eulerian velocities of flow in 
  
    
      
        
          
            
              R
            
          
          
            3
          
        
      
    
    {\displaystyle {\mathbb {R} }^{3}}
  . The flows between coordinates in computational anatomy are constrained to be geodesic flows satisfying the principle of least action for the Kinetic energy of the flow. The kinetic energy is defined through a Sobolev smoothness norm with strictly more than two generalized, square-integrable derivatives for each component of the flow velocity, which guarantees that the flows in 
  
    
      
        
          
            R
          
          
            3
          
        
      
    
    {\displaystyle \mathbb {R} ^{3}}
   are diffeomorphisms. 
It also implies that the diffeomorphic shape momentum taken pointwise satisfying the Euler-Lagrange equation for geodesics is determined by its neighbors through spatial derivatives on the velocity field.  This separates the discipline from the case of incompressible fluids for which momentum is a pointwise function of velocity. Computational anatomy intersects the study of Riemannian manifolds and nonlinear global analysis, where groups of diffeomorphisms are the central focus. Emerging high-dimensional theories of shape are central to many studies in computational anatomy, as are questions emerging from the fledgling field of shape statistics.
The metric structures in computational anatomy are related in spirit to morphometrics, with the distinction that Computational anatomy focuses on an infinite-dimensional space of coordinate systems transformed by a diffeomorphism, hence the central use of the terminology diffeomorphometry, the metric space study of coordinate systems via diffeomorphisms.",Computational anatomy,0,1
738,6019,"Computational chemistry is a branch of chemistry that uses computer simulation to assist in solving chemical problems. It uses methods of theoretical chemistry, incorporated into computer programs, to calculate the structures and properties of molecules, groups of molecules, and solids. It is essential because, apart from relatively recent results concerning the hydrogen molecular ion (dihydrogen cation, see references therein for more details), the quantum many-body problem cannot be solved analytically, much less in closed form.  While computational results normally complement the information obtained by chemical experiments, it can in some cases predict hitherto unobserved chemical phenomena. It is widely used in the design of new drugs and materials.
Examples of such properties are structure (i.e., the expected positions of the constituent atoms), absolute and relative (interaction) energies, electronic charge density distributions, dipoles and higher multipole moments, vibrational frequencies, reactivity, or other spectroscopic quantities, and cross sections for collision with other particles.
The methods used cover both static and dynamic situations. In all cases, the computer time and other resources (such as memory and disk space) increase quickly with the size of the system being studied.  That system can be a molecule, a group of molecules, or a solid.  Computational chemistry methods range from very approximate to highly accurate; the latter is usually feasible for small systems only. Ab initio methods are based entirely on quantum mechanics and basic physical constants. Other methods are called empirical or semi-empirical because they use additional empirical parameters.
Both ab initio and semi-empirical approaches involve approximations.  These range from simplified forms of the first-principles equations that are easier or faster to solve, to approximations limiting the size of the system (for example, periodic boundary conditions), to fundamental approximations to the underlying equations that are required to achieve any solution to them at all.  For example, most ab initio calculations make the Born–Oppenheimer approximation, which greatly simplifies the underlying Schrödinger equation by assuming that the nuclei remain in place during the calculation.  In principle, ab initio methods eventually converge to the exact solution of the underlying equations as the number of approximations is reduced.  In practice, however, it is impossible to eliminate all approximations, and residual error inevitably remains.  The goal of computational chemistry is to minimize this residual error while keeping the calculations tractable.
In some cases, the details of electronic structure are less important than the long-time phase space behavior of molecules. This is the case in conformational studies of proteins and protein-ligand binding thermodynamics. Classical approximations to the potential energy surface are used, typically with molecular mechanics force fields, as they are computationally less intensive than electronic calculations, to enable longer simulations of molecular dynamics. Furthermore, cheminformatics uses even more empirical (and computationally cheaper) methods like machine learning based on physicochemical properties. One typical problem in cheminformatics is to predict the binding affinity of drug molecules to a given target. Other problems include predicting binding specificity, off-target effects, toxicity, and pharmacokinetic properties.

",Computational chemistry,0,1
740,1420447,"Computational Economics is an interdisciplinary research discipline that involves computer science, economics, and management science.  This subject encompasses computational modeling of economic systems. Some of these areas are unique, while others established areas of economics by allowing robust data analytics and solutions of problems that would be arduous to research without computers and associated numerical methods.Computational methods have been applied in various fields of economics research, including but not limiting to:   
Econometrics: Non-parametric approaches, Semi-parametric approaches, and Machine Learning.
Dynamic Systems Modeling: Optimization, Dynamic stochastic general equilibrium modeling, and Agent-based modeling.

",Computational economics,0,1
741,7948184,"Computational science and engineering (CSE) is a relatively new discipline that deals with the development and application of computational models and simulations, often coupled with high-performance computing, to solve complex physical problems arising in engineering analysis and design (computational engineering) as well as natural phenomena (computational science). CSE has been described as the ""third mode of discovery"" (next to theory and experimentation).In many fields, computer simulation is essential to business and research. Computer simulation provides the capability to enter fields that are either inaccessible to traditional experimentation or where carrying out traditional empirical inquiries is prohibitively expensive. CSE should neither be confused with pure computer science, nor with computer engineering, although a wide domain in the former is used in CSE (e.g., certain algorithms, data structures, parallel programming, high performance computing) and some problems in the latter can be modeled and solved with CSE methods (as an application area).
It is typically offered as a masters or doctorate program.",Computational engineering,0,1
742,176927,"Computational geometry is a branch of computer science devoted to the study of algorithms which can be stated in terms of geometry. Some purely geometrical problems arise out of the study of computational geometric algorithms, and such problems are also considered to be part of computational geometry. While modern computational geometry is a recent development, it is one of the oldest fields of computing with a history stretching back to antiquity.
Computational complexity is central to computational geometry, with great practical significance if algorithms are used on very large datasets containing tens or hundreds of millions of points. For such sets, the difference between O(n2) and O(n log n) may be the difference between days and seconds of computation.
The main impetus for the development of computational geometry as a discipline was progress in computer graphics and computer-aided design and manufacturing (CAD/CAM), but many problems in computational geometry are classical in nature, and may come from mathematical visualization.
Other important applications of computational geometry include robotics (motion planning and visibility problems), geographic information systems (GIS) (geometrical location and search, route planning), integrated circuit design (IC geometry design and verification), computer-aided engineering (CAE) (mesh generation), and computer vision (3D reconstruction).
The main branches of computational geometry are:

Combinatorial computational geometry, also called algorithmic geometry, which deals with geometric objects as discrete entities. A groundlaying book in the subject by Preparata and Shamos dates the first use of the term ""computational geometry"" in this sense by 1975.
Numerical computational geometry, also called machine geometry, computer-aided geometric design (CAGD), or geometric modeling, which deals primarily with representing real-world objects in forms suitable for computer computations in CAD/CAM systems. This branch may be seen as a further development of descriptive geometry and is often considered a branch of computer graphics or CAD. The term ""computational geometry"" in this meaning has been in use since 1971.Although most algorithms of computational geometry have been developed (and are being developed) for electronic computers, some algorithms were developed for unconventional computers (e.g. optical computers )",Computational geometry,0,1
743,387537,"In computer science, computational learning theory (or just learning theory) is a subfield of artificial intelligence devoted to studying the design and analysis of machine learning algorithms.

",Computational learning theory,0,1
744,5561,"Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, as well as the study of appropriate computational approaches to linguistic questions. In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others.",Computational linguistics,0,1
746,106418,"Computational physics is the study and implementation of numerical analysis to solve problems in physics for which a quantitative theory already exists. Historically, computational physics was the first application of modern computers in science, and is now a subset of computational science. It is sometimes regarded as a subdiscipline (or offshoot) of theoretical physics, but others consider it an intermediate branch between theoretical and experimental physics - an area of study which supplements both theory and experiment.",Computational physics,0,1
747,1181008,"Computational science, also known as scientific computing or scientific computation (SC), is a field in mathematics that uses advanced computing capabilities to understand and solve complex problems. It is an area of science that spans many disciplines, but at its core, it involves the development of models and simulations to understand natural systems.

Algorithms (numerical and non-numerical): mathematical models, computational models, and computer simulations developed to solve science (e.g., biological, physical, and social), engineering, and humanities problems
Computer hardware that develops and optimizes the advanced system hardware, firmware, networking, and data management components needed to solve computationally demanding problems
The computing infrastructure that supports both the science and engineering problem solving and the developmental computer and information scienceIn practical use, it is typically the application of computer simulation and other forms of computation from numerical analysis and theoretical computer science to solve problems in various scientific disciplines. The field is different from theory and laboratory experiments, which are the traditional forms of science and engineering. The scientific computing approach is to gain understanding through the analysis of mathematical models implemented on computers. Scientists and engineers develop computer programs and application software that model systems being studied and run these programs with various sets of input parameters. The essence of computational science is the application of numerical algorithms and computational mathematics. In some cases, these models require massive amounts of calculations (usually floating-point) and are often executed on supercomputers or distributed computing platforms.",Computational science,0,1
748,40077102,"Computational social science is the academic sub-discipline concerned with computational approaches to the social sciences. This means that computers are used to model, simulate, and analyze social phenomena. While large scale computational methods have been applied to natural sciences for decades, interest in applying data science to social sciences has grown significantly in the last decade. Fields include computational economics, computational sociology, cliodynamics, culturomics,  and the automated analysis of contents, in social and traditional media. It focuses on investigating social and behavioral relationships and interactions through social simulation, modeling, network analysis, and media analysis.

",Computational social science,0,1
750,411964,"Computer accessibility (also known as accessible computing) refers to the accessibility of a computer system to all people, regardless of disability type or severity of impairment. The term accessibility is most often used in reference to specialized hardware or software, or a combination of both, designed to enable the use of a computer by a person with a disability or impairment. Computer accessibility often has direct positive effects on people with disabilities.
Accessibility features are meant to make the use of technology less challenging for those with disabilities. Common accessibility features include text-to-speech, closed-captioning, and keyboard shortcuts. More specific technologies that need additional hardware may be referred to as assistive technology.There are many disabilities or impairments that can be a barrier to effective computer use. These impairments, which can be acquired from disease, trauma, or maybe congenital, include but are not limited to:

Cognitive impairments (head injury, autism, developmental disabilities) and learning disabilities, (such as dyslexia, dyscalculia, or ADHD).
Visual impairment, such as low-vision, complete or partial blindness, and color blindness.
Hearing-related disabilities (deafness), including deafness, being hard of hearing, or hyperacusis.
Motor or dexterity impairment such as paralysis, cerebral palsy, dyspraxia, carpal tunnel syndrome, and repetitive strain injury.A topic closely linked to computer accessibility is web accessibility. Similar to computer accessibility, web accessibility is the practice of making the use of the World Wide Web easier for individuals with disabilities.Accessibility is often abbreviated as the numeronym a11y, where the number 11 refers to the number of letters omitted.  This parallels the abbreviations of internationalization and localization as i18n and l10n, respectively.  Moreover, a11y is also listed on the USPTO Supplemental Register under Accessibility Now, Inc.",Computer accessibility,0,1
751,6777,"Computer animation is the process used for digitally generating animated images. The more general term computer-generated imagery (CGI) encompasses both static scenes and dynamic images, while computer animation only refers to moving images. Modern computer animation usually uses 3D computer graphics to generate a three-dimensional picture. Sometimes, the target of the animation is the computer itself, but sometimes film as well.
Computer animation is essentially a digital successor to stop motion techniques, but using 3D models, and traditional animation techniques using frame-by-frame animation of 2D illustrations. Computer-generated animations can also allow a single graphic artist to produce such content without the use of actors, expensive set pieces, or props. To create the illusion of movement, an image is displayed on the computer monitor and repeatedly replaced by a new image that is similar to it but advanced slightly in time (usually at a rate of 24, 25, or 30 frames/second). This technique is identical to how the illusion of movement is achieved with television and motion pictures.
For 3D animations, objects (models) are built on the computer monitor (modeled) and 3D figures are rigged with a virtual skeleton. Then the limbs, eyes, mouth, clothes, etc. of the figure are moved by the animator on key frames. The differences in appearance between key frames are automatically calculated by the computer in a process known as tweening or morphing. Finally, the animation is rendered.For 3D animations, all frames must be rendered after the modeling is complete. For pre-recorded presentations, the rendered frames are transferred to a different format or medium, like digital video. The frames may also be rendered in real time as they are presented to the end-user audience. Low bandwidth animations transmitted via the internet (e.g. Adobe Flash, X3D) often use software on the end-user's computer to render in real time as an alternative to streaming or pre-loaded high bandwidth animations.

",Computer animation,0,1
752,25652303,"In computer engineering, computer architecture is a set of rules and methods that describe the functionality, organization, and implementation of computer systems. The architecture of a system refers to its structure in terms of separately specified components of that system and their interrelationships.Some definitions of architecture define it as describing the capabilities and programming model of a computer but not a particular implementation. In other definitions computer architecture involves instruction set architecture design, microarchitecture design, logic design, and implementation.

",Computer architecture,0,1
753,5300,"Computer data storage is a technology consisting of computer components and recording media that are used to retain digital data. It is a core function and fundamental component of computers.: 15–16 The central processing unit (CPU) of a computer is what manipulates data by performing computations. In practice, almost all computers use a storage hierarchy,: 468–473  which puts fast but expensive and small storage options close to the CPU and slower but less expensive and larger options further away. Generally the fast volatile technologies (which lose data when off power) are referred to as ""memory"", while slower persistent technologies are referred to as ""storage"".
Even the first computer designs, Charles Babbage's Analytical Engine and Percy Ludgate's Analytical Machine, clearly distinguished between processing and memory (Babbage stored numbers as rotations of gears, while Ludgate stored numbers as displacements of rods in shuttles). This distinction was extended in the Von Neumann architecture, where the CPU consists of two main parts: The control unit and the arithmetic logic unit (ALU). The former controls the flow of data between the CPU and memory, while the latter performs arithmetic and logical operations on data.",Computer data storage,0,1
754,1336512,"A personal computer game, also known as a PC game or computer game, is a type of video game played on a personal computer (PC) rather than a video game console or arcade machine. Its defining characteristics include: more diverse and user-determined gaming hardware and software; and generally greater capacity in input, processing, video and audio output. The uncoordinated nature of the PC game market, and now its lack of physical media, make precisely assessing its size difficult. In 2018, the global PC games market was valued at about $27.7 billion.Home computer games became popular following the video game crash of 1983, leading to the era of the ""bedroom coder"". In the 1990s, PC games lost mass-market traction to console games, before enjoying a resurgence in the mid-2000s through digital distribution on services such as Steam and GOG.com.Newzoo reports that the PC gaming sector is the third-largest category (and estimated in decline) across all platforms as of 2016, with the console sector second-largest, and mobile / smartphone gaming sector biggest. 2.2 billion video gamers generate US$101.1 billion in revenue, excluding hardware costs. ""Digital game revenues will account for $94.4 billion or 87% of the global market. Mobile is the most lucrative segment, with smartphone and tablet gaming growing 19% year on year to $46.1 billion, claiming 42% of the market. In 2020, mobile gaming will represent just more than half of the total games market. [...] China expected to generate $27.5 billion, or one-quarter of all revenues in 2017.""PC gaming is considered synonymous (by Newzoo and others) with IBM Personal Computer compatible systems; while mobile computers – smartphones and tablets, such as those running Android or iOS –  are also personal computers in the general sense. The APAC region was estimated to generate $46.6 billion in 2016, or 47% of total global video game revenues (note, not only ""PC"" games). China alone accounts for half of APAC's revenues (at $24.4 billion), cementing its place as the largest video game market in the world, ahead of the US's anticipated market size of $23.5 billion. China is expected to have 53% of its video game revenues come from mobile gaming in 2017 (46% in 2016).",PC game,0,1
755,18567210,"Computer graphics deals with generating images with the aid of computers. Today, computer graphics is a core technology in digital photography, film, video games, cell phone and computer displays, and many specialized applications. A great deal of specialized hardware and software has been developed, with the displays of most devices being driven by computer graphics hardware. It is a vast and recently developed area of computer science. The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). The non-artistic aspects of computer graphics are the subject of computer science research.Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surfaces, visualization, scientific computing, image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception.
Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. It is also used for processing image data received from the physical world, such as photo and video content. Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, in general.",Computer graphics,0,1
756,21808348,"Computer hardware includes the physical parts of a computer, such as the case, central processing unit (CPU), random access memory (RAM), monitor, mouse, keyboard, computer data storage, graphics card, sound card, speakers and motherboard.By contrast, software is the set of instructions that can be stored and run by hardware. Hardware is so-termed because it is ""hard"" or rigid with respect to changes, whereas software is ""soft"" because it is easy to change.
Hardware is typically directed by the software to execute any command or instruction. A combination of hardware and software forms a usable computing system, although other systems exist with only hardware.

",Computer hardware,0,1
757,4122592,"A computer network is a set of computers sharing resources located on or provided by network nodes. The computers use common communication protocols over digital interconnections to communicate with each other. These interconnections are made up of telecommunication network technologies, based on physically wired, optical, and wireless radio-frequency methods that may be arranged in a variety of network topologies.
The nodes of a computer network can include personal computers, servers, networking hardware, or other specialised or general-purpose hosts. They are identified by network addresses, and may have hostnames. Hostnames serve as memorable labels for the nodes, rarely changed after initial assignment. Network addresses serve for locating and identifying the nodes by communication protocols such as the Internet Protocol.
Computer networks may be classified by many criteria, including the transmission medium used to carry signals, bandwidth, communications protocols to organize network traffic, the network size, the topology, traffic control mechanism, and organizational intent.
Computer networks support many applications and services, such as access to the World Wide Web, digital video, digital audio, shared use of application and storage servers, printers, and fax machines, and use of email and instant messaging applications.",Computer network,0,1
759,7398,"Computer security, cybersecurity (cyber security), or information technology security (IT security) is the protection of computer systems and networks from information disclosure, theft of, or damage to their hardware, software, or electronic data, as well as from the disruption or misdirection of the services they provide.The field has become of significance due to the expanded reliance on computer systems, the Internet, and wireless network standards such as Bluetooth and Wi-Fi, and due to the growth of ""smart"" devices, including smartphones, televisions, and the various devices that constitute the Internet of things (IoT). Cybersecurity is also one of the significant challenges in the contemporary world, due to the complexity of information systems, both in terms of political usage and technology. Its primary goal is to ensure the system's dependability, integrity, and data privacy.",Computer security,0,1
760,29816614,"Computer security compromised by hardware failure is a branch of computer security applied to hardware.
The objective of computer security includes protection of information and property from theft, corruption, or natural disaster, while allowing the information and property to remain accessible and productive to its intended users. Such secret information could be retrieved by different ways. This article focus on the retrieval of data thanks to misused hardware or hardware failure. Hardware could be misused or exploited to get secret data. This article collects main types of attack that can lead to data theft.
Computer security can be comprised by devices, such as keyboards, monitors or printers (thanks to electromagnetic or acoustic emanation for example) or by components of the computer, such as the memory, the network card or the processor (thanks to time or temperature analysis for example).",Computer security compromised by hardware failure,0,1
761,866349,"Computervision, Inc. (CV) was an early pioneer in Computer Aided Design and Manufacturing (CAD/CAM). Computervision was founded in 1969 by Marty Allen and Philippe Villers, and headquartered in Bedford, Massachusetts, United States.  Its early products were built on a Data General Nova platform. Starting around 1975, Computervision built its own ""CGP"" (Computervision Graphics Processor) Nova-compatible 16-bit computers with added instructions optimized for graphics applications and using its own operating system known as Computervision Graphic Operating System (CGOS). In the 1980s, Computervision rewrote their code to operate on Unix-based platforms.
Computervision was acquired by Prime Computer in 1988 for $434 million. Prime subsequently adopted the Computervision name. In 1998 Parametric Technology Corporation acquired Computervision.

",Computervision,0,1
762,404048,"""Computing Machinery and Intelligence"" is a seminal paper written by Alan Turing on the topic of artificial intelligence. The paper, published in 1950 in Mind, was the first to introduce his concept of what is now known as the Turing test to the general public.
Turing's paper considers the question ""Can machines think?"" Turing says that since the words ""think"" and ""machine"" cannot be clearly defined we should ""replace the question by another, which is closely related to it and is expressed in relatively unambiguous words."" To do this, he must first find a simple and unambiguous idea to replace the word ""think"", second he must explain exactly which ""machines"" he is considering, and finally, armed with these tools, he formulates a new question, related to the first, that he believes he can answer in the affirmative.

",Computing Machinery and Intelligence,0,1
763,81196,"A computing platform or digital platform is an environment in which a piece of software is executed. It may be the hardware or the operating system (OS), even a web browser and associated application programming interfaces, or other underlying software, as long as the program code is executed with it. Computing platforms have different abstraction levels, including a computer architecture, an OS, or runtime libraries. A computing platform is the stage on which computer programs can run.
A platform can be seen both as a constraint on the software development process, in that different platforms provide different functionality and restrictions; and as an assistant to the development process, in that they provide low-level functionality ready-made.  For example, an OS may be a platform that abstracts the underlying differences in hardware and provides a generic command for saving files or accessing the network.",Computing platform,0,1
764,928467,"In computer science, concurrency is the ability of different parts or units of a program, algorithm, or problem to be executed out-of-order or in partial order, without affecting the final outcome.  This allows for parallel execution of the concurrent units, which can significantly improve overall speed of the execution in multi-processor and multi-core systems. In more technical terms, concurrency refers to the decomposability of a program, algorithm, or problem into order-independent or partially-ordered components or units of computation.According to Rob Pike, concurrency is the composition of independently executing computations, and concurrency is not parallelism: concurrency is about dealing with lots of things at once but parallelism is about doing lots of things at once. Concurrency is about structure, parallelism is about execution, concurrency provides a way to structure a solution to solve a problem that may (but not necessarily) be parallelizable.A number of mathematical models have been developed for general concurrent computation including Petri nets, process calculi, the parallel random-access machine model, the actor model and the Reo Coordination Language.",Concurrency (computer science),0,1
765,2581605,"Concurrent computing is a form of computing in which several computations are executed concurrently—during overlapping time periods—instead of sequentially—with one completing before the next starts.
This is a property of a system—whether a program, computer, or a network—where there is a separate execution point or ""thread of control"" for each process. A concurrent system is one where a computation can advance without waiting for all other computations to complete.Concurrent computing is a form of modular programming.  In its paradigm an overall computation is factored into subcomputations that may be executed concurrently. Pioneers in the field of concurrent computing include Edsger Dijkstra, Per Brinch Hansen, and C.A.R. Hoare.

",Concurrent computing,0,1
766,801135,"In probability theory, conditional independence describes situations wherein an observation is irrelevant or redundant when evaluating the certainty of a hypothesis. Conditional independence is usually formulated in terms of conditional probability, as a special case where the probability of the hypothesis given the uninformative observation is equal to the probability without. If 
  
    
      
        A
      
    
    {\displaystyle A}
   is the hypothesis, and 
  
    
      
        B
      
    
    {\displaystyle B}
   and 
  
    
      
        C
      
    
    {\displaystyle C}
   are observations, conditional independence can be stated as an equality:

  
    
      
        P
        (
        A
        ∣
        B
        ,
        C
        )
        =
        P
        (
        A
        ∣
        C
        )
      
    
    {\displaystyle P(A\mid B,C)=P(A\mid C)}
  where 
  
    
      
        P
        (
        A
        ∣
        B
        ,
        C
        )
      
    
    {\displaystyle P(A\mid B,C)}
   is the probability of 
  
    
      
        A
      
    
    {\displaystyle A}
   given both 
  
    
      
        B
      
    
    {\displaystyle B}
   and 
  
    
      
        C
      
    
    {\displaystyle C}
  . Since the probability of 
  
    
      
        A
      
    
    {\displaystyle A}
   given 
  
    
      
        C
      
    
    {\displaystyle C}
   is the same as the probability of 
  
    
      
        A
      
    
    {\displaystyle A}
   given both 
  
    
      
        B
      
    
    {\displaystyle B}
   and 
  
    
      
        C
      
    
    {\displaystyle C}
  , this equality expresses that 
  
    
      
        B
      
    
    {\displaystyle B}
   contributes nothing to the certainty of 
  
    
      
        A
      
    
    {\displaystyle A}
  . In this case, 
  
    
      
        A
      
    
    {\displaystyle A}
   and 
  
    
      
        B
      
    
    {\displaystyle B}
   are said to be conditionally independent given 
  
    
      
        C
      
    
    {\displaystyle C}
  , written symbolically as: 
  
    
      
        (
        A
        ⊥
        
        
        
        ⊥
        B
        ∣
        C
        )
      
    
    {\displaystyle (A\perp \!\!\!\perp B\mid C)}
  .
The concept of conditional independence is essential to graph-based theories of statistical inference, as it establishes a mathematical relation between a collection of conditional statements and a graphoid.

",Conditional independence,0,1
767,4118276,"Conditional random fields (CRFs) are a class of statistical modeling methods often applied in pattern recognition and machine learning and used for structured prediction. Whereas a classifier predicts a label for a single sample without considering ""neighbouring"" samples, a CRF can take context into account. To do so, the predictions are modelled as a graphical model, which represents the presence of dependencies between the predictions. What kind of graph is used depends on the application. For example, in natural language processing, ""linear chain"" CRFs are popular, for which each prediction is dependent only on its immediate neighbours. In image processing, the graph typically connects locations to nearby and/or similar locations to enforce that they receive similar predictions.
Other examples where CRFs are used are: labeling or parsing of sequential data for natural language processing or biological sequences, POS tagging, shallow parsing, named entity recognition, gene finding, peptide critical functional region finding, and object recognition and image segmentation in computer vision.",Conditional random field,0,1
768,5856528,"SIGKDD, representing the Association for Computing Machinery's (ACM) Special Interest Group (SIG) on Knowledge Discovery and Data Mining, hosts an influential annual conference.",Special Interest Group on Knowledge Discovery and Data Mining,0,1
769,1175156,"The Conference and Workshop on Neural Information Processing Systems (abbreviated as NeurIPS and formerly NIPS)  is a machine learning and computational neuroscience conference held every December.  The conference is currently a double-track meeting (single-track until 2015) that includes invited talks as well as oral and poster presentations of refereed papers, followed by parallel-track workshops that up to 2013 were held at ski resorts.

",Conference on Neural Information Processing Systems,0,1
770,263636,"Connectionism refers to both an approach in the field of cognitive science that hopes to explain mental phenomena using artificial neural networks (ANN). and to a wide range of techniques and algorithms using ANNs in the context of artificial intelligence to build more intelligent machines. Connectionism presents a cognitive theory based on simultaneously occurring, distributed signal activity via connections that can be represented numerically, where learning occurs by modifying connection strengths based on experience.Some advantages of the connectionist approach include its applicability to a broad array of functions, structural approximation to biological neurons, low requirements for innate structure, and capacity for graceful degradation. Some disadvantages include the difficulty in deciphering how ANNs process information, or account for the compositionality of mental representations, and a resultant difficulty explaining phenomena at a higher level.The success of deep learning networks in the past decade has greatly increased the popularity of this approach, but the complexity and scale of such networks has brought with them increased interpretability problems. Connectionism is seen by many to offer an alternative to classical theories of mind based on symbolic computation, but the extent to which the two approaches are compatible has been the subject of much debate since their inception.",Connectionism,0,1
771,1642843,"Continuous production is a flow production method used to manufacture, produce, or process materials without interruption.  Continuous production is called a continuous process or a continuous flow process because the materials, either dry bulk or fluids that are being processed are continuously in motion, undergoing chemical reactions or subject to mechanical or heat treatment.  Continuous processing is contrasted with batch production.
Continuous usually means operating 24 hours per day, seven days per week with infrequent maintenance shutdowns, such as semi-annual or annual.  Some chemical plants can operate for more than one to two years without a shutdown.  Blast furnaces can run from four to ten years without stopping.

",Continuous production,0,1
773,47790980,"In computer programming, a control variable is a program variable that is used to regulate the flow of control of the program.
In definite iteration, control variables are variables which are successively assigned (or bound to) values from a predetermined sequence.",Control variable (programming),0,1
774,7519,"In mathematics (in particular, functional analysis), convolution is a mathematical operation on two functions (f and g) that produces a third function (
  
    
      
        f
        ∗
        g
      
    
    {\displaystyle f*g}
  ) that expresses how the shape of one is modified by the other.  The term convolution refers to both the result function and to the process of computing it. It is defined as the integral of the product of the two functions after one is reversed and shifted.  The integral is evaluated for all values of shift, producing the convolution function.
Some features of convolution are similar to cross-correlation: for real-valued functions, of a continuous or discrete variable, it differs from cross-correlation (
  
    
      
        f
        ⋆
        g
      
    
    {\displaystyle f\star g}
  ) only in that either f(x) or g(x) is reflected about the y-axis; thus it is a cross-correlation of g(-x) and f(x), or f(−x) and g(x).  For complex-valued functions, the cross-correlation operator is the adjoint of the convolution operator.
Convolution has applications that include probability, statistics, acoustics, spectroscopy, signal processing and image processing, geophysics, engineering, physics, computer vision and differential equations.The convolution can be defined for functions on Euclidean space and other groups. For example, periodic functions, such as the discrete-time Fourier transform, can be defined on a circle and convolved by periodic convolution. (See row 18 at DTFT § Properties.) A discrete convolution can be defined for functions on the set of integers.
Generalizations of convolution have applications in the field of numerical analysis and numerical linear algebra, and in the design and implementation of finite impulse response filters in signal processing.Computing the inverse of the convolution operation is known as deconvolution.",Convolution,0,1
775,40409788,"In deep learning, a convolutional neural network (CNN, or ConvNet) is a class of artificial neural network (ANN), most commonly applied to analyze visual imagery. CNNs are also known as Shift Invariant or Space Invariant Artificial Neural Networks (SIANN), based on the shared-weight architecture of the convolution kernels or filters that slide along input features and provide translation-equivariant responses known as feature maps. Counter-intuitively, most convolutional neural networks are not invariant to translation, due to the downsampling operation they apply to the input. They have applications in image and video recognition, recommender systems, image classification, image segmentation, medical image analysis, natural language processing, brain–computer interfaces, and financial time series.CNNs are regularized versions of multilayer perceptrons. Multilayer perceptrons usually mean fully connected networks, that is, each neuron in one layer is connected to all neurons in the next layer. The ""full connectivity"" of these networks make them prone to overfitting data. Typical ways of regularization, or preventing overfitting, include: penalizing parameters during training (such as weight decay) or trimming connectivity (skipped connections, dropout, etc.) CNNs take a different approach towards regularization: they take advantage of the hierarchical pattern in data and assemble patterns of increasing complexity using smaller and simpler patterns embossed in their filters. Therefore, on a scale of connectivity and complexity, CNNs are on the lower extreme.
Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.
CNNs use relatively little pre-processing compared to other image classification algorithms. This means that the network learns to optimize the filters (or kernels) through automated learning, whereas in traditional algorithms these filters are hand-engineered. This independence from prior knowledge and human intervention in feature extraction is a major advantage.",Convolutional neural network,0,1
776,33737411,"Corinna Cortes is a Danish computer scientist known for her contributions to machine learning. She is currently the Head of Google Research, New York. Cortes is a recipient of the Paris Kanellakis Theory and Practice Award for her work on theoretical foundations of support vector machines.",Corinna Cortes,0,1
777,15905419,"Credit card fraud is an inclusive term for fraud committed using a payment card, such as a credit card or debit card. The purpose may be to obtain goods or services or to make payment to another account, which is controlled by a criminal. The Payment Card Industry Data Security Standard (PCI DSS) is the data security standard created to help financial institutions process card payments securely and reduce card fraud.Credit card fraud can be authorised, where the genuine customer themselves processes payment to another account which is controlled by a criminal, or unauthorised, where the account holder does not provide authorisation for the payment to proceed and the transaction is carried out by a third party. In 2018, unauthorised financial fraud losses across payment cards and remote banking totalled £844.8 million in the United Kingdom. Whereas banks and card companies prevented £1.66 billion in unauthorised fraud in 2018. That is the equivalent to £2 in every £3 of attempted fraud being stopped.Credit Card Fraud can occur when unauthorized users gain access to an individual's credit card information in order to make purchases, other transactions, or open new accounts. A few examples of credit card fraud include account takeover fraud, new account fraud, cloned cards, and cards-not-present schemes. This unauthorized access occurs through phishing, skimming, and information sharing by a user, oftentimes unknowingly. However, this type of fraud can be detected through means of artificial intelligence and machine learning as well as prevented by issuers, institutions, and individual cardholders. According to a 2021 annual report, about 50% of all Americans have experienced a fraudulent charge on their credit or debit cards, and more than one in three credit or debit card holders have experienced fraud multiple times. This amounts to 127 million people in the US that have been victims of credit card theft at least once.
Credit cards are more secure than ever, with regulators, card providers and banks taking considerable time and effort to collaborate with investigators worldwide to ensure fraudsters are not successful. Cardholders' money is usually protected from scammers with regulations that make the card provider and bank accountable. The technology and security measures behind credit cards are becoming increasingly sophisticated making it harder for fraudsters to steal money.",Credit card fraud,0,1
779,554546,"In genetic algorithms and evolutionary computation, crossover, also called recombination, is a genetic operator used to combine the genetic information of two parents to generate new offspring. It is one way to stochastically generate new solutions from an existing population, and is analogous to the crossover that happens during sexual reproduction in biology. Solutions can also be generated by cloning an existing solution, which is analogous to asexual reproduction. Newly generated solutions are typically mutated before being added to the population.
Different algorithms in evolutionary computation may use different data structures to store genetic information, and each genetic representation can be recombined with different crossover operators. Typical data structures that can be recombined with crossover are bit arrays, vectors of real numbers, or trees.",Crossover (genetic algorithm),0,1
780,5292585,"Crowdsourcing involves a large group of dispersed participants contributing or producing goods or services—including ideas, voting, micro-tasks, and finances—for payment or as volunteers. Contemporary crowdsourcing often involves digital platforms to attract and divide work between participants to achieve a cumulative result; however, it may not always be an online activity, and there are various historical examples of crowdsourcing. The word crowdsourcing is a portmanteau of ""crowd"" and ""outsourcing"". In contrast to outsourcing, crowdsourcing usually involves less-specific, more public groups.Advantages of using crowdsourcing may include improved costs, speed, quality, flexibility, scalability, or diversity. Common crowdsourcing methods include competitions, virtual labour markets, open online collaboration and data donation. Some forms of crowdsourcing, such as in ""idea competitions"" or ""innovation contests"" provide ways for organizations to learn beyond the ""base of minds"" provided by their employees (e.g. LEGO Ideas). Commercial platforms, such as Amazon Mechanical Turk, match microtasks submitted by requesters to workers who perform them. Not-for-profit organizations have used crowdsourcing to develop common goods (e.g. Wikipedia).",Crowdsourcing,0,1
781,18934432,"Cryptography, or cryptology (from Ancient Greek: κρυπτός, romanized: kryptós ""hidden, secret""; and γράφειν graphein, ""to write"", or -λογία -logia, ""study"", respectively), is the practice and study of techniques for secure communication in the presence of adversarial behavior. More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages; various aspects of information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography. Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications.
Cryptography prior to the modern age was effectively synonymous with encryption, converting information from a readable state to unintelligible nonsense. The sender of an encrypted message shares the decoding technique only with intended recipients to preclude access from adversaries. The cryptography literature often uses the names Alice (""A"") for the sender, Bob (""B"") for the intended recipient, and Eve (""eavesdropper"") for the adversary. Since the development of rotor cipher machines in World War I and the advent of computers in World War II, cryptography methods have become increasingly complex and their applications more varied.
Modern cryptography is heavily based on mathematical theory and computer science practice; cryptographic algorithms are designed around computational hardness assumptions, making such algorithms hard to break in actual practice by any adversary. While it is theoretically possible to break into a well-designed system, it is infeasible in actual practice to do so. Such schemes, if well designed, are therefore termed ""computationally secure""; theoretical advances (e.g., improvements in integer factorization algorithms) and faster computing technology require these designs to be continually reevaluated, and if necessary, adapted.  Information-theoretically secure schemes that provably cannot be broken even with unlimited computing power, such as the one-time pad, are much more difficult to use in practice than the best theoretically breakable, but computationally secure, schemes.
The growth of cryptographic technology has raised a number of legal issues in the Information Age. Cryptography's potential for use as a tool for espionage and sedition has led many governments to classify it as a weapon and to limit or even prohibit its use and export. In some jurisdictions where the use of cryptography is legal, laws permit investigators to compel the disclosure of encryption keys for documents relevant to an investigation. Cryptography also plays a major role in digital rights management and copyright infringement disputes in regard to digital media.",Cryptography,0,1
782,3369375,"Cyberwarfare is the use of cyber attacks against an enemy state, causing comparable harm to actual warfare and/or disrupting vital computer systems. Some intended outcomes could be espionage, sabotage, propaganda, manipulation or economic warfare.
There is significant debate among experts regarding the definition of cyberwarfare, and even if such a thing exists. One view is that the term is a misnomer, since no cyber attacks to date could be described as war. An alternative view is that it is a suitable label for cyber attacks which cause physical damage to people and objects in the real world.Many countries including the United States, United Kingdom, Russia, China, Israel, Iran, and North Korea have active cyber capabilities for offensive and defensive operations. As states explore the use of cyber operations and combine capabilities the likelihood of physical confrontation and violence playing out as a result of, or part of, a cyber operation is increased. However, meeting the scale and protracted nature of war is unlikely, thus ambiguity remains.The first instance of kinetic military action used in response to a cyber-attack resulting in the loss of human life was observed on 5 May 2019, when the Israel Defense Forces targeted and destroyed a building associated with an ongoing cyber-attack.",Cyberwarfare,0,1
783,1158125,"DNA sequencing is the process of determining the nucleic acid sequence – the order of nucleotides in DNA. It includes any method or technology that is used to determine the order of the four bases: adenine, guanine, cytosine, and thymine. The advent of rapid DNA sequencing methods has greatly accelerated biological and medical research and discovery.Knowledge of DNA sequences has become indispensable for basic biological research, DNA Genographic Projects and in numerous applied fields such as medical diagnosis, biotechnology, forensic biology, virology and biological systematics. Comparing healthy and mutated DNA sequences can diagnose different diseases including various cancers, characterize antibody repertoire, and can be used to guide patient treatment. Having a quick way to sequence DNA allows for faster and more individualized medical care to be administered, and for more organisms to be identified and cataloged.
The rapid speed of sequencing attained with modern DNA sequencing technology has been instrumental in the sequencing of complete DNA sequences, or genomes, of numerous types and species of life, including the human genome and other complete DNA sequences of many animal, plant, and microbial species.
The first DNA sequences were obtained in the early 1970s by academic researchers using laborious methods based on two-dimensional chromatography. Following the development of fluorescence-based sequencing methods with a DNA sequencer, DNA sequencing has become easier and orders of magnitude faster.",DNA sequencing,0,1
784,1124646,"The Dartmouth Summer Research Project on Artificial Intelligence was a 1956 summer workshop widely considered to be the founding event of artificial intelligence as a field.
The project lasted approximately six to eight weeks and was essentially an extended brainstorming session. Eleven mathematicians and scientists originally planned to attend; not all of them attended, but more than ten others came for short times.

",Dartmouth workshop,0,1
785,3575651,"Data cleansing or data cleaning is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. Data cleansing may be performed interactively with data wrangling tools, or as batch processing through scripting or a data quality firewall.
After cleansing, a data set should be consistent with other similar data sets in the system. The inconsistencies detected or removed may have been originally caused by user entry errors, by corruption in transmission or storage, or by different data dictionary definitions of similar entities in different stores. Data cleaning differs from data validation in that validation almost invariably means data is rejected from the system at entry and is performed at the time of entry, rather than on batches of data.
The actual process of data cleansing may involve removing typographical errors or validating and correcting values against a known list of entities. The validation may be strict (such as rejecting any address that does not have a valid postal code), or with fuzzy or approximate string matching (such as correcting records that partially match existing, known records). Some data cleansing solutions will clean data by cross-checking with a validated data set. A common data cleansing practice is data enhancement, where data is made more complete by adding related information. For example, appending addresses with any phone numbers related to that address. Data cleansing may also involve harmonization (or normalization) of data, which is the process of bringing together data of ""varying file formats, naming conventions, and columns"", and transforming it into one cohesive data set; a simple example is the expansion of abbreviations (""st, rd, etc."" to ""street, road, etcetera"").",Data cleansing,0,1
786,51443362,Data augmentation in data analysis are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data. It acts as a regularizer and helps reduce overfitting when training a machine learning model. It is closely related to oversampling in data analysis.,Data augmentation,0,1
789,1609808,"Data quality refers to the state of qualitative or quantitative pieces of information. There are many definitions of data quality, but data is generally considered high quality if it is ""fit for [its] intended uses in operations, decision making and planning"". Moreover, data is deemed of high quality if it correctly represents the real-world construct to which it refers. Furthermore, apart from these definitions, as the number of data sources increases, the question of internal data consistency becomes significant, regardless of fitness for use for any particular external purpose. People's views on data quality can often be in disagreement, even when discussing the same set of data used for the same purpose. When this is the case, data governance is used to form agreed upon definitions and standards for data quality. In such cases, data cleansing, including standardization, may be required in order to ensure data quality.

",Data quality,0,1
791,8377,"In computing, a database is an organized collection of data stored and accessed electronically. Small databases can be stored on a file system, while large databases are hosted on computer clusters or cloud storage. The design of databases spans formal techniques and practical considerations, including data modeling, efficient data representation and storage, query languages, security and privacy of sensitive data, and distributed computing issues, including supporting concurrent access and fault tolerance.
A database management system (DBMS) is the software that interacts with end users, applications, and the database itself to capture and analyze the data. The DBMS software additionally encompasses the core facilities provided to administer the database. The sum total of the database, the DBMS and the associated applications can be referred to as a database system. Often the term ""database"" is also used loosely to refer to any of the DBMS, the database system or an application associated with the database.
Computer scientists may classify database management systems according to the database models that they support. Relational databases became dominant in the 1980s. These model data as rows and columns in a series of tables, and the vast majority use SQL for writing and querying data. In the 2000s, non-relational databases became popular, collectively referred to as NoSQL, because they use different query languages.",Database,0,1
792,2679315,"Professor Sir David John Cameron MacKay  (22 April 1967 – 14 April 2016) was a British physicist, mathematician, and academic. He was the Regius Professor of Engineering  in the Department of Engineering at the University of Cambridge and from 2009 to 2014 was Chief Scientific Advisor to the UK Department of Energy and Climate Change (DECC). MacKay authored the book Sustainable Energy – Without the Hot Air.",David J. C. MacKay,0,1
793,2823113,"David Everett Rumelhart (June 12, 1942 – March 13, 2011) was an American psychologist who made many contributions to the formal analysis of human cognition, working primarily within the frameworks of mathematical psychology, symbolic artificial intelligence, and parallel distributed processing. He also admired formal linguistic approaches to cognition, and explored the possibility of formulating a formal grammar to capture the structure of stories.",David Rumelhart,0,1
794,50568835,"David Silver  (born 1976) leads the reinforcement learning research group at DeepMind and was lead researcher on AlphaGo, AlphaZero and co-lead on AlphaStar.
He graduated from Cambridge University in 1997 with the Addison-Wesley award, and befriended Demis Hassabis whilst there. Subsequently, Silver co-founded the video games company Elixir Studios, where he was CTO and lead programmer, receiving several awards for technology and innovation.Silver returned to academia in 2004 at the University of Alberta to study for a PhD on reinforcement learning, where he co-introduced the algorithms used in the first master-level 9×9 Go programs and graduated in 2009. His version of program MoGo (co-authored with Sylvain Gelly) was one of the strongest Go programs as of 2009.Silver was awarded a Royal Society University Research Fellowship in 2011, and subsequently became a lecturer at University College London, where he is now a professor. His lectures on Reinforcement Learning are available on YouTube. Silver consulted for DeepMind from its inception, joining full-time in 2013.
His recent work has focused on combining reinforcement learning with deep learning, including a program that learns to play Atari games directly from pixels.  Silver led the AlphaGo project, culminating in the first program to defeat a top professional player in the full-size game of Go. AlphaGo subsequently received an honorary 9 Dan Professional Certification; and won the Cannes Lion award for innovation. He then led development of AlphaZero, which used the same AI to learn to play Go from scratch (learning only by playing itself and not from human games) before learning to play chess and shogi in the same way, to higher levels than any other computer program.
Silver is among the most published members of staff at DeepMind, with over 95,000 citations and has an h-index of 74.He was awarded the 2019 ACM Prize in Computing for breakthrough advances in computer game-playing.In 2021, Silver was elected Fellow of the Royal Society for his contributions to Deep Q-Networks and AlphaGo.",David Silver (computer scientist),0,1
795,265752,"In psychology, decision-making (also spelled decision making and decisionmaking) is regarded as the cognitive process resulting in the selection of a belief or a course of action among several possible alternative options. It could be either rational or irrational. The decision-making process is a reasoning process based on assumptions of values, preferences and beliefs of the decision-maker. Every decision-making process produces a final choice, which may or may not prompt action.
Research about decision-making is also published under the label problem solving, particularly in European psychological research.",Decision-making,0,1
798,577003,"Decision Tree Learning is supervised learning approach used in statistics, data mining and machine learning. In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations. 
Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. 
Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making).",Decision tree learning,0,1
799,47332350,"DeepDream is a computer vision program created by Google engineer Alexander Mordvintsev that uses a convolutional neural network to find and enhance patterns in images via algorithmic pareidolia, thus creating a dream-like appearance reminiscent of a psychedelic experience in the deliberately over-processed images.Google's program popularized the term (deep) ""dreaming"" to refer to the generation of images that produce desired activations in a trained deep network, and the term now refers to a collection of related approaches.

",DeepDream,0,1
800,41755648,"DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research laboratory founded in September 2010. DeepMind was acquired by Google in 2014. The company is based in London, with research centres in Canada, France, and the United States. In 2015, it became a wholly owned subsidiary of Alphabet Inc, Google's parent company.
DeepMind has created a neural network that learns how to play video games in a fashion similar to that of humans, as well as a Neural Turing machine, or a neural network that may be able to access an external memory like a conventional Turing machine, resulting in a computer that mimics the short-term memory of the human brain.DeepMind made headlines in 2016 after its AlphaGo program beat a human professional Go player Lee Sedol, a world champion, in a five-game match, which was the subject of a documentary film. A more general program, AlphaZero, beat the most powerful programs playing go, chess and shogi (Japanese chess) after a few days of play against itself using reinforcement learning. In 2020, DeepMind made significant advances in the problem of protein folding.
DeepMind posted a blog post on April 28, 2022, on a single visual language model (VLM) named Flamingo that can accurately describe a picture of something with just a few training images. In July 2022, DeepMind announced the development of DeepNash, a model-free multi-agent reinforcement learning system capable of playing the board game Stratego at the level of a human expert.

",DeepMind,0,1
801,64396232,"DeepSpeed is an open source deep learning optimization library for PyTorch. The library is designed to reduce computing power and memory use and to train large distributed models with better parallelism on existing computer hardware. DeepSpeed is optimized for low latency, high throughput training. It includes the Zero Redundancy Optimizer (ZeRO) for training models with 1 trillion or more parameters. Features include mixed precision training, single-GPU, multi-GPU, and multi-node training as well as custom model parallelism. The DeepSpeed source code is licensed under MIT License and available on GitHub.The team claimed to achieve up to a 6.2x throughput improvement, 2.8x faster convergence, and 4.6x less communication.",DeepSpeed,0,1
802,32472154,"Deep learning  (also known as deep structured learning) is part of a broader family of machine learning methods based on artificial neural networks with representation learning. Learning can be supervised, semi-supervised or unsupervised.Deep-learning architectures such as deep neural networks, deep belief networks, deep reinforcement learning, recurrent neural networks and convolutional neural networks have been applied to fields including computer vision, speech recognition, natural language processing, machine translation, bioinformatics, drug design, medical image analysis, climate science, material inspection and board game programs, where they have produced results comparable to and in some cases surpassing human expert performance.Artificial neural networks (ANNs) were inspired by information processing and distributed communication nodes in biological systems. ANNs have various differences from biological brains.  Specifically, artificial neural networks tend to be static and symbolic, while the biological brain of most living organisms is dynamic (plastic) and analogue.The adjective ""deep"" in deep learning refers to the use of multiple layers in the network. Early work showed that a linear perceptron cannot be a universal classifier, but that a network with a nonpolynomial activation function with one hidden layer of unbounded width can. Deep learning is a modern variation which is concerned with an unbounded number of layers of bounded size, which permits practical application and optimized implementation, while retaining theoretical universality under mild conditions. In deep learning the layers are also permitted to be heterogeneous and to deviate widely from biologically informed connectionist models, for the sake of efficiency, trainability and understandability, whence the ""structured"" part.",Deep learning,1,1
804,43169442,"Eclipse Deeplearning4j is a programming library written in Java for the Java virtual machine (JVM). It is a framework with wide support for deep learning algorithms. Deeplearning4j includes implementations of the restricted Boltzmann machine, deep belief net, deep autoencoder, stacked denoising autoencoder and recursive neural tensor network, word2vec, doc2vec, and GloVe. These algorithms all include distributed parallel versions that integrate with Apache Hadoop and Spark.Deeplearning4j is open-source software released under Apache License 2.0, developed mainly by a machine learning group headquartered in San Francisco. It is supported commercially by the startup Skymind, which bundles DL4J, TensorFlow, Keras and other deep learning libraries in an enterprise distribution called the Skymind Intelligence Layer. Deeplearning4j was contributed to the Eclipse Foundation in October 2017.",Deeplearning4j,0,1
805,3259263,"Demis Hassabis  (born 27 July 1976) is a British artificial intelligence researcher, neuroscientist, video game designer, entrepreneur, and five times winner of the Pentamind board games championship. He is the chief executive officer and co-founder of DeepMind and Isomorphic Labs, and a UK Government AI Advisor since 2018.",Demis Hassabis,0,1
807,661384,"In systems engineering, dependability is a measure of a system's availability, reliability, maintainability, and in some cases, other characteristics such as durability, safety and security.  In real-time computing, dependability is the ability to provide services that can be trusted within a time-period. The service guarantees must hold even when the system is subject to attacks or natural failures. 
The International Electrotechnical Commission (IEC), via its Technical Committee TC 56 develops and maintains international standards that provide systematic methods and tools for dependability assessment and management of equipment, services, and systems throughout their life cycles. The IFIP Working Group 10.4 on ""Dependable Computing and Fault Tolerance"" plays a role in synthesizing the technical community's progress in the field and organizes two workshops each year to disseminate the results. 
Dependability can be broken down into three elements:

Attributes - a way to assess the dependability of a system
Threats - an understanding of the things that can affect the dependability of a system
Means - ways to increase a system's dependability

",Dependability,0,1
808,48813654,"Sparse coding is a representation learning method which aims at finding a sparse representation of the input data (also known as sparse coding) in the form of a linear combination of basic elements as well as those basic elements themselves. These elements are called atoms and they compose a dictionary. Atoms in the dictionary are not required to be orthogonal, and they may be an over-complete spanning set. This problem setup also allows the dimensionality of the signals being represented to be higher than the one of the signals being observed. The above two properties lead to having seemingly redundant atoms that allow multiple representations of the same signal but also provide an improvement in sparsity and flexibility of the representation.
One of the most important applications of sparse dictionary learning is in the field of compressed sensing or signal recovery. In compressed sensing, a high-dimensional signal can be recovered with only a few linear measurements provided that the signal is sparse or nearly sparse. Since not all signals satisfy this sparsity condition, it is of great importance to find a sparse representation of that signal such as the wavelet transform or the directional gradient of a rasterized matrix. Once a matrix or a high dimensional vector is transferred to a sparse space, different recovery algorithms like basis pursuit, CoSaMP or fast non-iterative algorithms can be used to recover the signal.
One of the key principles of dictionary learning is that the dictionary has to be inferred from the input data. The emergence of sparse dictionary learning methods was stimulated by the fact that in signal processing one typically wants to represent the input data using as few components as possible. Before this approach the general practice was to use predefined dictionaries (such as Fourier or wavelet transforms). However, in certain cases a dictionary that is trained to fit the input data can significantly improve the sparsity, which has applications in data decomposition, compression and analysis and has been used in the fields of image denoising and classification, video and audio processing. Sparsity and overcomplete dictionaries have immense applications in image compression, image fusion and inpainting.  

",Sparse dictionary learning,1,1
810,52036598,"In artificial intelligence, a differentiable neural computer (DNC) is a memory augmented neural network architecture (MANN), which is typically (not by definition) recurrent in its implementation. The model was published in 2016 by Alex Graves et al. of DeepMind.",Differentiable neural computer,0,1
811,59939845,"Differentiable programming is a programming paradigm in which a numeric computer program can be differentiated throughout via automatic differentiation. This allows for gradient-based optimization of parameters in the program, often via gradient descent. Differentiable programming has found use in  a wide variety of areas, particularly scientific computing and artificial intelligence.",Differentiable programming,0,1
812,1667059,"In probability theory and statistics, a diffusion process is a solution to a stochastic differential equation. It is a continuous-time Markov process with almost surely continuous sample paths. Brownian motion, reflected Brownian motion and Ornstein–Uhlenbeck processes are examples of diffusion processes.
A sample path of a diffusion process models the trajectory of a particle embedded in a flowing fluid and subjected to random displacements due to collisions with other particles, which is called Brownian motion.  The position of the particle is then random; its probability density function as a function of space and time is  governed by an advection–diffusion equation.",Diffusion process,0,1
813,148113,"Digital art can either be understood as any artistic work or practice that uses digital technology as part of the creative or presentation process, or more specifically as computational art that uses and engages with digital media.Since the 1960s, various names have been used to describe digital art, including computer art, multimedia art and new media art. Christiane Paul 

",Digital art,0,1
814,18934863,"A digital library, also called an online library, an internet library, a digital repository, or a digital collection is an online database of digital objects that can include text, still images, audio, video, digital documents, or other digital media formats or a library accessible through the internet. Objects can consist of digitized content like print or photographs, as well as originally produced digital content like word processor files or social media posts. In addition to storing content, digital libraries provide means for organizing, searching, and retrieving the content contained in the collection. Digital libraries can vary immensely in size and scope, and can be maintained by individuals or organizations. The digital content may be stored locally, or accessed remotely via computer networks. These information retrieval systems are able to exchange information with each other through interoperability and sustainability.",Digital library,0,1
815,9933471,"Digital marketing is the component of marketing that uses the Internet and online based digital technologies such as desktop computers, mobile phones and other digital media and platforms to promote products and services. Its development during the 1990s and 2000s changed the way brands and businesses use technology for marketing. As digital platforms became increasingly incorporated into marketing plans and everyday life, and as people increasingly use digital devices instead of visiting physical shops, digital marketing campaigns have become prevalent, employing combinations of search engine optimization (SEO), search engine marketing (SEM), content marketing, influencer marketing, content automation, campaign marketing, data-driven marketing, e-commerce marketing, social media marketing, social media optimization, e-mail direct marketing, display advertising, e–books, and optical disks and games have become commonplace. Digital marketing extends to non-Internet channels that provide digital media, such as television, mobile phones (SMS and MMS), callback, and on-hold mobile ring tones. The extension to non-Internet channels differentiates digital marketing from online advertising.",Digital marketing,0,1
816,579867,"Dimensionality reduction, or dimension reduction, is the transformation of data from a high-dimensional space into a low-dimensional space so that the low-dimensional representation retains some meaningful properties of the original data, ideally close to its intrinsic dimension. Working in high-dimensional spaces can be undesirable for many reasons; raw data are often sparse as a consequence of the curse of dimensionality, and analyzing the data is usually computationally intractable (hard to control or deal with). Dimensionality reduction is common in fields that deal with large numbers of observations and/or large numbers of variables, such as signal processing, speech recognition, neuroinformatics, and bioinformatics.Methods are commonly divided into linear and nonlinear approaches. Approaches can also be divided into feature selection and feature extraction. Dimensionality reduction can be used for noise reduction, data visualization, cluster analysis, or as an intermediate step to facilitate other analyses.

",Dimensionality reduction,0,1
817,204002,"In mathematics, particularly graph theory, and computer science, a directed acyclic graph (DAG  (listen)) is a directed graph with no directed cycles. That is, it consists of vertices and edges (also called arcs), with each edge directed from one vertex to another, such that following those directions will never form a closed loop. A directed graph is a DAG if and only if it can be topologically ordered, by arranging the vertices as a linear ordering that is consistent with all edge directions. DAGs have numerous scientific and computational applications, ranging from biology (evolution, family trees, epidemiology) to information science (citation networks) to computation (scheduling).

",Directed acyclic graph,0,1
819,53231,"Discovery is the act of detecting something new, or something previously unrecognized as meaningful. With reference to sciences and academic disciplines, discovery is the observation of new phenomena, new actions, or new events and providing new reasoning to explain the knowledge gathered through such observations with previously acquired knowledge from abstract thought and everyday experiences. A discovery may sometimes be based on earlier discoveries, collaborations, or ideas. Some discoveries represent a radical breakthrough in knowledge or technology.
New discoveries are acquired through various senses and are usually assimilated, merging with pre-existing knowledge and actions. Questioning is a major form of human thought and interpersonal communication, and plays a key role in discovery. Discoveries are often made due to questions. Some discoveries lead to the invention of objects, processes, or techniques. A discovery may sometimes be based on earlier discoveries, collaborations or ideas, and the process of discovery requires at least the awareness that an existing concept or method can be modified or transformed. However, some discoveries also represent a radical breakthrough in knowledge.",Discovery (observation),0,1
821,237629,Distributed Artificial Intelligence (DAI) also called Decentralized Artificial Intelligence is a subfield of artificial intelligence research dedicated to the development of distributed solutions for problems. DAI is closely related to and a predecessor of the field of multi-agent systems.,Distributed artificial intelligence,0,1
822,8501,"Distributed computing is a field of computer science that studies distributed systems. A distributed system is a system whose components are located on different networked computers, which communicate and coordinate their actions by passing messages to one another from any system. The components interact with one another in order to achieve a common goal. Three significant challenges of distributed systems are: maintaining concurrency of components, overcoming the lack of a global clock, and managing the independent failure of components. When a component of one system fails, the entire system does not fail. Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
A computer program that runs within a distributed system is called  a distributed program, and distributed programming is the process of writing such programs. There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.Distributed computing also refers to the use of distributed systems to solve computational problems. In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, which communicate with each other via message passing.",Distributed computing,0,1
823,71608,"A document management system (DMS) is a system used to receive, track, manage and store documents and reduce paper. Most are capable of keeping a record of the various versions created and modified by different users (history tracking). In the case of the management of digital documents such systems are based on computer programs. The term has some overlap with the concepts of content management systems. It is often viewed as a component of enterprise content management (ECM) systems and related to digital asset management, document imaging, workflow systems and records management systems.

",Document management system,0,1
825,519239,"A domain-specific language (DSL) is a computer language specialized to a particular application domain. This is in contrast to a general-purpose language (GPL), which is broadly applicable across domains. There are a wide variety of DSLs, ranging from widely used languages for common domains, such as HTML for web pages, down to languages used by only one or a few pieces of software, such as MUSH soft code. DSLs can be further subdivided by the kind of language, and include domain-specific markup languages, domain-specific modeling languages (more generally, specification languages), and domain-specific programming languages. Special-purpose computer languages have always existed in the computer age, but the term ""domain-specific language"" has become more popular due to the rise of domain-specific modeling. Simpler DSLs, particularly ones used by a single application, are sometimes informally called mini-languages.
The line between general-purpose languages and domain-specific languages is not always sharp, as a language may have specialized features for a particular domain but be applicable more broadly, or conversely may in principle be capable of broad application but in practice used primarily for a specific domain. For example, Perl was originally developed as a text-processing and glue language, for the same domain as AWK and shell scripts, but was mostly used as a general-purpose programming language later on. By contrast, PostScript is a Turing-complete language, and in principle can be used for any task, but in practice is narrowly used as a page description language.

",Domain-specific language,0,1
826,1242713,"A Dynamic Bayesian Network (DBN) is a Bayesian network (BN) which relates variables to each other over adjacent time steps. This is often called a Two-Timeslice BN (2TBN) because it says that at any point in time T, the value of a variable can be calculated from the internal regressors and the immediate prior value (time T-1).  DBNs were developed by Paul Dagum in the early 1990s at Stanford University's Section on Medical Informatics. Dagum developed DBNs to unify and extend traditional linear state-space models such as Kalman filters, linear and normal forecasting models such as ARMA and simple dependency models such as hidden Markov models into a general probabilistic representation and inference mechanism for arbitrary nonlinear and non-normal time-dependent domains.Today, DBNs are common in robotics, and have shown potential for a wide range of data mining applications.  For example, they have been used in speech recognition, digital forensics, protein sequencing, and bioinformatics.  DBN is a generalization of hidden Markov models and Kalman filters.DBNs are conceptually related to Probabilistic Boolean Networks  and can, similarly, be used to model dynamical systems at steady-state.",Dynamic Bayesian network,0,1
827,125297,"Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.
In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems. In the optimization literature this relationship is called the Bellman equation.",Dynamic programming,0,1
828,39208,"Commerce is the exchange of goods and services, especially on a large scale.",Commerce,0,1
829,19834151,"ECML PKDD, the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, is one of the leading academic conferences on machine learning and knowledge discovery, held in Europe every year.",ECML PKDD,1,1
830,27193284,"ELKI (for Environment for DeveLoping KDD-Applications Supported by Index-Structures) is a data mining (KDD, knowledge discovery in databases) software framework developed for use in research and teaching. It was originally at the database systems research unit of Professor Hans-Peter Kriegel at the Ludwig Maximilian University of Munich, Germany, and now continued at the Technical University of Dortmund, Germany. It aims at allowing the development and evaluation of advanced data mining algorithms and their interaction with database index structures.",ELKI,0,1
831,8887731,"An echo state network (ESN) is a type of reservoir computer that uses a recurrent neural network with a sparsely connected hidden layer (with typically 1% connectivity). The connectivity and weights of hidden neurons are fixed and randomly assigned. The weights of output neurons can be learned so that the network can produce or reproduce specific temporal patterns. The main interest of this network is that although its behaviour is non-linear, the only weights that are modified during training are for the synapses that connect the hidden neurons to output neurons. Thus, the error function is quadratic with respect to the parameter vector and can be differentiated easily to a linear system.
Alternatively, one may consider a nonparametric Bayesian formulation of the output layer, under which: (i) a prior distribution is imposed over the output weights; and (ii) the output weights are marginalized out in the context of prediction generation, given the training data. This idea has been demonstrated in by using Gaussian priors, whereby a Gaussian process model with ESN-driven kernel function is obtained. Such a solution was shown to outperform ESNs with trainable (finite) sets of weights in several benchmarks.
Some publicly available implementations of ESNs are: (i) aureservoir: an efficient C++ library for various kinds of echo state networks with python/numpy bindings; and (ii) Matlab code: an efficient matlab for an echo state network, (iii) ReservoirComputing.jl: an efficient Julia-based implementation of various types of echo state networks, and (iv) pyESN: simple echo state networks in Python.",Echo state network,0,1
832,3670866,"An edge device is a device that provides an entry point into enterprise or service provider core networks. Examples include routers, routing switches, integrated access devices (IADs), multiplexers, and a variety of metropolitan area network (MAN) and wide area network (WAN) access devices.  Edge devices also provide connections into carrier and service provider networks. An edge device that connects a local area network to a high speed switch or backbone (such as an ATM switch) may be called an edge concentrator.",Edge device,0,1
833,1944675,"Educational technology (commonly abbreviated as edutech, or edtech) is the combined use of computer hardware, software, and educational theory and practice to facilitate learning. When referred to with its abbreviation, edtech, it is often referring to the industry of companies that create educational technology.In addition to practical educational experience, educational technology is based on theoretical knowledge from various disciplines such as communication, education, psychology, sociology, artificial intelligence, and computer science. It encompasses several domains including learning theory, computer-based training, online learning, and m-learning, where mobile technologies are used.",Educational technology,0,1
834,891719,"Ehud Shapiro (Hebrew: אהוד שפירא; born 1955) is a multi-disciplinary scientist, artist, entrepreneur and Professor of Computer Science and Biology at the Weizmann Institute of Science. With international reputation, he made fundamental contributions to many scientific disciplines. Ehud was also an Internet pioneer, a successful Internet entrepreneur, and a pioneer and proponent of E-democracy. Ehud is the founder of the Ba Rock Band and conceived its original artistic program. He is a winner of two ERC (European Research Council) Advanced Grants.",Ehud Shapiro,0,1
835,64305316,"Electrochemical Random-Access Memory (ECRAM) is a type of non-volatile memory (NVM) with multiple levels per cell (MLC) designed for deep learning analog acceleration. An ECRAM cell is a three-terminal device composed of a conductive channel, an insulating electrolyte, an ionic reservoir, and metal contacts. The resistance of the channel is modulated by ionic exchange at the interface between the channel and the electrolyte upon application of an electric field. The charge-transfer process allows both for state retention in the absence of applied power, and for programming of multiple distinct levels, both differentiating ECRAM operation from the one of a field-effect transistor (FET). The write operation is deterministic and can result in symmetrical potentiation and depression, making ECRAM arrays attractive for acting as artificial synaptic weights in physical implementations of artificial neural networks (ANN). The technology challenges include open circuit potential (OCP) and semiconductor foundry compatibility associated with energy materials. Universities, government laboratories, and corporate research teams have contributed to the development of ECRAM for analog computing. Notably, Sandia National Laboratories designed a lithium-based cell inspired by solid-state battery materials, Stanford University built an organic proton-based cell, and International Business Machines (IBM) demonstrated in-memory selector-free parallel programming for a logistic regression task in an array of metal-oxide ECRAM designed for insertion in the back end of line (BEOL).",Electrochemical RAM,0,1
836,216881,"Electronic design automation (EDA), also referred to as electronic computer-aided design (ECAD), is a category of software tools for designing electronic systems such as integrated circuits and printed circuit boards. The tools work together in a design flow that chip designers use to design and analyze entire semiconductor chips. Since a modern semiconductor chip can have billions of components, EDA tools are essential for their design; this article in particular describes EDA specifically with respect to integrated circuits (ICs).",Electronic design automation,0,1
837,190831,"Electronic publishing (also referred to as publishing, digital publishing, or online publishing) includes the digital publication of e-books, digital magazines, and the development of digital libraries and catalogues. It also includes the editing of books, journals and magazines to be posted on a screen (computer, e-reader, tablet, or smartphone).

",Electronic publishing,0,1
838,371301,"Electronic voting (also known as e-voting) is voting that uses electronic means to either aid or take care of casting and counting ballots.
Depending on the particular implementation, e-voting may use standalone electronic voting machines (also called EVM) or computers connected to the Internet.  It may encompass a range of Internet services, from basic transmission of tabulated results to full-function online voting through common connectable household devices. The degree of automation may be limited to marking a paper ballot, or may be a comprehensive system of vote input, vote recording, data encryption and transmission to servers, and consolidation and tabulation of election results.
A worthy e-voting system must perform most of these tasks while complying with a set of standards established by regulatory bodies, and must also be capable to deal successfully with strong requirements associated with security, accuracy, integrity, swiftness, privacy, auditability, accessibility, cost-effectiveness, scalability and ecological sustainability.
Electronic voting technology can include punched cards, optical scan voting systems and specialized voting kiosks (including self-contained direct-recording electronic voting systems, or DRE). It can also involve transmission of ballots and votes via telephones, private computer networks, or the Internet.
In general, two main types of e-voting can be identified:

e-voting which is physically supervised by representatives of governmental or independent electoral authorities (e.g. electronic voting machines located at polling stations);
remote e-voting via the Internet (also called i-voting) where the voter submits his or her vote electronically to the election authorities, from any location.",Electronic voting,0,1
839,1336001,"Email filtering is the processing of email to organize it according to specified criteria. The term can apply to the intervention of human intelligence, but most often refers to the automatic processing of messages at an SMTP server, possibly applying anti-spam techniques. Filtering can be applied to incoming emails as well as to outgoing ones.
Depending on the calling environment, email filtering software can reject an item at the initial SMTP connection stage or pass it through unchanged for delivery to the user's mailbox.  It is also possible to redirect the message for delivery elsewhere, quarantine it for further checking, modify it or 'tag' it in any other way.",Email filtering,0,1
840,46630,"An embedded system is a computer system—a combination of a computer processor, computer memory, and input/output peripheral devices—that has a dedicated function within a larger mechanical or electronic system. It is embedded as part of a complete device often including electrical or electronic hardware and mechanical parts. 
Because an embedded system typically controls physical operations of the machine that it is embedded within, it often has real-time computing constraints.  Embedded systems control many devices in common use today. In 2009, it was estimated that ninety-eight percent of all microprocessors manufactured were used in embedded systems.Modern embedded systems are often based on microcontrollers (i.e. microprocessors with integrated memory and peripheral interfaces), but ordinary microprocessors (using external chips for memory and peripheral interface circuits) are also common, especially in more complex systems. In either case, the processor(s) used may be types ranging from general purpose to those specialized in a certain class of computations, or even custom designed for the application at hand. A common standard class of dedicated processors is the digital signal processor (DSP).
Since the embedded system is dedicated to specific tasks, design engineers can optimize it to reduce the size and cost of the product and increase its reliability and performance. Some embedded systems are mass-produced, benefiting from economies of scale.
Embedded systems range in size from portable personal devices such as digital watches and MP3 players to bigger machines like home appliances, industrial assembly lines, robots, transport vehicles, traffic light controllers, and medical imaging systems. Often they constitute subsystems of other machines like avionics in aircraft and astrionics in spacecraft. Large installations like factories, pipelines and electrical grids rely on multiple embedded systems networked together. Generalized through software customization, embedded systems such as programmable logic controllers frequently comprise their functional units.
Embedded systems range from those low in complexity, with a single microcontroller chip, to very high with multiple units, peripherals and networks, which may reside in equipment racks or across large geographical areas connected via long-distance communications lines.",Embedded system,1,1
842,1455062,"Empirical risk minimization (ERM) is a principle in statistical learning theory which defines a family of learning algorithms and is used to give theoretical bounds on their performance. The core idea is that we cannot know exactly how well an algorithm will work in practice (the true ""risk"") because we don't know the true distribution of data that the algorithm will work on, but we can instead measure its performance on a known set of training data (the ""empirical"" risk).",Empirical risk minimization,0,1
843,22212276,"In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.
Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.

",Ensemble learning,1,1
845,37531624,"Logical consequence (also entailment) is a fundamental concept in logic, which describes the relationship between statements that hold true when one statement logically follows from one or more statements. A valid logical argument is one in which the conclusion is entailed by the premises, because the conclusion is the consequence of the premises. The philosophical analysis of logical consequence involves the questions: In what sense does a conclusion follow from its premises?  and What does it mean for a conclusion to be a consequence of premises?  All of philosophical logic is meant to provide accounts of the nature of logical consequence and the nature of logical truth.Logical consequence is necessary and formal, by way of examples that explain with formal proof and models of interpretation. A sentence is said to be a logical consequence of a set of sentences, for a given language, if and only if, using only logic (i.e., without regard to any personal interpretations of the sentences) the sentence must be true if every sentence in the set is true.Logicians make precise accounts of logical consequence regarding a given language 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
  , either by constructing a deductive system for 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
   or by formal intended semantics for language 
  
    
      
        
          
            L
          
        
      
    
    {\displaystyle {\mathcal {L}}}
  .  The Polish logician Alfred Tarski identified three features of an adequate characterization of entailment: (1) The logical consequence relation relies on the logical form of the sentences: (2) The relation is a priori, i.e., it can be determined with or without regard to empirical evidence (sense experience); and (3) The logical consequence relation has a modal component.

",Logical consequence,0,1
846,1010494,"An Enterprise Information System (EIS) is any kind of information system which improves the functions of enterprise business processes by integration. This means typically offering high quality of service, dealing with large volumes of data and capable of supporting some large and possibly complex organization or enterprise. An EIS must be able to be used by all parts and all levels of an enterprise.The word enterprise can have various connotations. Frequently the term is used only to refer to very large organizations such as multi-national companies or public-sector organizations. However, the term may be used to mean virtually anything, by virtue of it having become a corporate-speak buzzword.",Enterprise information system,0,1
847,9310,"Enterprise resource planning (ERP) is the integrated management of main business processes, often in real time and mediated by software and technology.  ERP is usually referred to as a category of business management software—typically a suite of integrated applications—that an organization can use to collect, store, manage, and interpret data from many business activities. ERP Systems can be local based or Cloud-based. Cloud-based applications have grown in recent years due to information being readily available from any location with internet access.
ERP provides an integrated and continuously updated view of core business processes using common databases maintained by a database management system. ERP systems track business resources—cash, raw materials, production capacity—and the status of business commitments: orders, purchase orders, and payroll. The applications that make up the system share data across various departments (manufacturing, purchasing, sales, accounting, etc.) that provide the data. ERP facilitates information flow between all business functions and manages connections to outside stakeholders.Enterprise system software is a multibillion-dollar industry that produces components supporting a variety of business functions. Though early ERP systems focused on large enterprises, smaller enterprises increasingly use ERP systems.The ERP system integrates varied organizational systems and facilitates error-free transactions and production, thereby enhancing the organization's efficiency. However, developing an ERP system differs from traditional system development.
ERP systems run on a variety of computer hardware and network configurations, typically using a database as an information repository.",Enterprise resource planning,0,1
848,2302514,"Enterprise software, also known as enterprise application software (EAS), is computer software used to satisfy the needs of an organization rather than individual users. Such organizations include businesses, schools, interest-based user groups, clubs, charities, and governments. Enterprise software is an integral part of a (computer-based) information system; a collection of such software is called an enterprise system. These systems handle a chunk of operations in an organization to enhance the business and management reporting tasks. The systems must process the information at a relatively high speed and can be deployed across a variety of networks.Services provided by enterprise software are typically business-oriented tools, such as online shopping, and online payment processing, interactive product catalogue, automated billing systems, security, Business Process Management,  enterprise content management, IT Service Management, customer relationship management, enterprise resource planning, business intelligence, project management, collaboration, human resource management, manufacturing, occupational health and safety, enterprise application integration, and enterprise forms automation.
As enterprises have similar departments and systems in common, enterprise software is often available as a suite of customizable programs. Generally, the complexity of these tools requires specialist capabilities and specific knowledge. Enterprise computing is the information technology (IT) tool that businesses use for efficient production operations and back-office support. These IT tools cover database management, customer relationship management, supply chain management, business process management and so on.",Enterprise software,0,1
850,13659583,"The ethics of artificial intelligence is the branch of the ethics of technology specific to artificially intelligent systems. It is sometimes divided into a concern with the moral behavior of humans as they design, make, use and treat artificially intelligent systems, and a concern with the behavior of machines, in machine ethics. It also includes the issue of a possible singularity due to superintelligent AI.",Ethics of artificial intelligence,0,1
852,190837,"In computational intelligence (CI), an evolutionary algorithm (EA) is a subset of evolutionary computation, a generic population-based metaheuristic optimization algorithm. An EA uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection. Candidate solutions to the optimization problem play the role of individuals in a population, and the fitness function determines the quality of the solutions (see also loss function). Evolution of the population then takes place after the repeated application of the above operators.
Evolutionary algorithms often perform well approximating solutions to all types of problems because they ideally do not make any assumption about the underlying fitness landscape. Techniques from evolutionary algorithms applied to the modeling of biological evolution are generally limited to explorations of microevolutionary processes and planning models based upon cellular processes. In most real applications of EAs, computational complexity is a prohibiting factor. In fact, this computational complexity is due to fitness function evaluation. Fitness approximation is one of the solutions to overcome this difficulty. However, seemingly simple EA can solve often complex problems; therefore, there may be no direct link between algorithm complexity and problem complexity.",Evolutionary algorithm,0,1
853,46583121,"Existential risk from artificial general intelligence is the hypothesis that substantial progress in artificial general intelligence (AGI) could result in human extinction or some other unrecoverable global catastrophe. It is argued that the human species currently dominates other species because the human brain has some distinctive capabilities that other animals lack. If AI surpasses humanity in general intelligence and becomes ""superintelligent"", then it could become difficult or impossible for humans to control. Just as the fate of the mountain gorilla depends on human goodwill, so might the fate of humanity depend on the actions of a future machine superintelligence.The likelihood of this type of scenario is widely debated, and hinges in part on differing scenarios for future progress in computer science. Once the exclusive domain of science fiction, concerns about superintelligence started to become mainstream in the 2010s, and were popularized by public figures such as Stephen Hawking, Bill Gates, and Elon Musk.One source of concern is that controlling a superintelligent machine, or instilling it with human-compatible values, may be a harder problem than naïvely supposed. Many researchers believe that a superintelligence would naturally resist attempts to shut it off or change its goals—a principle called instrumental convergence—and that preprogramming a superintelligence with a full set of human values will prove to be an extremely difficult technical task. In contrast, skeptics such as computer scientist Yann LeCun argue that superintelligent machines will have no desire for self-preservation.A second source of concern is that a sudden and unexpected ""intelligence explosion"" might take an unprepared human race by surprise. To illustrate, if the first generation of a computer program able to broadly match the effectiveness of an AI researcher is able to rewrite its algorithms and double its speed or capabilities in six months, then the second-generation program is expected to take three calendar months to perform a similar chunk of work. In this scenario the time for each generation continues to shrink, and the system undergoes an unprecedentedly large number of generations of improvement in a short time interval, jumping from subhuman performance in many areas to superhuman performance in all relevant areas. Empirically, examples like AlphaZero in the domain of Go show that AI systems can sometimes progress from narrow human-level ability to narrow superhuman ability extremely rapidly.",Existential risk from artificial general intelligence,0,1
854,470752,"In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, where the model depends on unobserved latent variables. The EM iteration alternates between performing an expectation (E) step, which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. These parameter-estimates are then used to determine the distribution of the latent variables in the next E step.",Expectation–maximization algorithm,0,1
855,10136,"In artificial intelligence, an expert system is a computer system emulating the decision-making ability of a human expert.
Expert systems are designed to solve complex problems by reasoning through bodies of knowledge, represented mainly as if–then rules rather than through conventional procedural code. The first expert systems were created in the 1970s and then proliferated in the 1980s. Expert systems were among the first truly successful forms of artificial intelligence (AI) software. 
An expert system is divided into two subsystems: the inference engine and the knowledge base. The knowledge base represents facts and rules. The inference engine applies the rules to the known facts to deduce new facts. Inference engines can also include explanation and debugging abilities.

",Expert system,0,1
857,602401,"A facial recognition system is a technology capable of matching a human face from a digital image or a video frame against a database of faces, typically employed to authenticate users through ID verification services, works by pinpointing and measuring facial features from a given image.Development began on similar systems in the 1960s, beginning as a form of computer application. Since their inception, facial recognition systems have seen wider uses in recent times on smartphones and in other forms of technology, such as robotics. Because computerized facial recognition involves the measurement of a human's physiological characteristics, facial recognition systems are categorized as biometrics. Although the accuracy of facial recognition systems as a biometric technology is lower than iris recognition and fingerprint recognition, it is widely adopted due to its contactless process. Facial recognition systems have been deployed in advanced human–computer interaction, video surveillance and automatic indexing of images.Facial recognition systems are employed throughout the world today by governments and private companies. Their effectiveness varies, and some systems have previously been scrapped because of their ineffectiveness. The use of facial recognition systems has also raised controversy, with claims that the systems violate citizens' privacy, commonly make incorrect identifications, encourage gender norms and racial profiling, and do not protect important biometric data. The appearance of synthetic media such as deepfakes has also raised concerns about its security. These claims have led to the ban of facial recognition systems in several cities in the United States. As a result of growing societal concerns, Meta announced that it plans to shut down Facebook facial recognition system, deleting the face scan data of more than one billion users. This change will represent one of the largest shifts in facial recognition usage in the technology's history.",Facial recognition system,0,1
859,62683332,"Fairness in machine learning refers to the various attempts at correcting algorithmic bias in automated decision processes based on machine learning models. Decisions made by computers after a machine-learning process may be considered unfair if they were based on variables considered sensitive. Examples of these kinds of variable include gender, ethnicity, sexual orientation, disability and more. As it is the case with many ethical concepts, definitions of fairness and bias are always controversial. In general, fairness and bias are considered relevant when the decision process impacts people's lives. In machine learning, the problem of algorithmic bias is well known and well studied. Outcomes may be skewed by a range of factors and thus might be considered unfair with respect to certain groups or individuals. An example would be the way social media sites deliver personalized news to consumers.",Fairness (machine learning),0,1
860,43286898,"A false positive is an error in binary classification in which a test result incorrectly indicates the presence of a condition (such as a disease when the disease is not present), while a false negative is the opposite error, where the test result incorrectly indicates the absence of a condition when it is actually present. These are the two kinds of errors in a binary test, in contrast to the two kinds of correct result (a true positive and a true negative). They are also known in medicine as a false positive (or false negative) diagnosis, and in statistical classification as a false positive (or false negative) error.In statistical hypothesis testing the analogous concepts are known as type I and type II errors, where a positive result corresponds to rejecting the null hypothesis, and a negative result corresponds to not rejecting the null hypothesis. The terms are often used interchangeably, but there are differences in detail and interpretation due to the differences between medical testing and statistical hypothesis testing.",False positives and false negatives,0,1
861,56276,"Family (Latin: familia, plural familiae) is one of the eight major hierarchical taxonomic ranks in Linnaean taxonomy. It is classified between order and genus. A family may be divided into subfamilies, which are intermediate ranks between the ranks of family and genus. The official family names are Latin in origin; however, popular names are often used: for example, walnut trees and hickory trees belong to the family Juglandaceae, but that family is commonly referred to as being the ""walnut family"".
What belongs to a family—or if a described family should be recognized at all—are proposed and determined by practicing taxonomists. There are no hard rules for describing or recognizing a family, but in plants, they can be characterized on the basis of both vegetative and reproductive feature of plant species. Taxonomists often take different positions about descriptions, and there may be no broad consensus across the scientific community for some time. The publishing of new data and opinions often enables adjustments and consensus.

",Family (biology),0,1
862,46207323,"Feature engineering or feature extraction  or feature discovery is the process of using domain knowledge to extract features (characteristics, properties, attributes) from raw data.  The motivation is to use these extra features to improve the quality of results from a machine learning process, compared with supplying only the raw data to the machine learning process. 

",Feature engineering,0,1
863,38870173,"In machine learning, feature learning or representation learning is a set of techniques that allows a system to automatically discover the representations needed for feature detection or classification from raw data. This replaces manual feature engineering and allows a machine to both learn the features  and use them to perform  a specific task.
Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensor data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.
Feature learning can be either supervised or unsupervised.

In supervised feature learning, features are learned using labeled input data. Examples include supervised neural networks, multilayer perceptron and (supervised) dictionary learning.
In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, independent component analysis, autoencoders, matrix factorization and various forms of clustering.

",Feature learning,0,1
864,60992857,"Federated learning (also known as collaborative learning) is a machine learning technique that trains an algorithm across multiple decentralized edge devices or servers holding local data samples, without exchanging them. This approach stands in contrast to traditional centralized machine learning techniques where all the local datasets are uploaded to one server, as well as to more classical decentralized approaches which often assume that local data samples are identically distributed.
Federated learning enables multiple actors to build a common, robust machine learning model without sharing data, thus allowing to address critical issues such as data privacy, data security, data access rights and access to heterogeneous data. Its applications are spread over a number of industries including defense, telecommunications, IoT, and pharmaceutics.

",Federated learning,0,1
865,44359594,"Fei-Fei Li (simplified Chinese: 李飞飞; traditional Chinese: 李飛飛; born 1976) is an American computer scientist. She is the Sequoia Capital Professor of Computer Science at Stanford University. Li is a Co-Director of the Stanford Institute for Human-Centered Artificial Intelligence, and a Co-Director of the Stanford Vision and Learning Lab. She served as the director of the Stanford Artificial Intelligence Laboratory (SAIL) from 2013 to 2018.
In 2017, she co-founded AI4ALL, a nonprofit organization working to increase diversity and inclusion in the field of artificial intelligence. Her research expertise includes artificial intelligence (AI), machine learning, deep learning, computer vision and cognitive neuroscience. She was the leading scientist and principal investigator of ImageNet.Li was elected a member of the National Academy of Engineering (NAE) in 2020 for contributions in building large knowledge bases for machine learning and visual understanding. She is also a member of the National Academy of Medicine (NAM), and American Academy of Arts and Sciences (AAAS).",Fei-Fei Li,0,1
866,41916,"A financial market is a market in which people trade financial securities and derivatives at low transaction costs. Some of the securities include stocks and bonds, raw materials and precious metals, which are known in the financial markets as commodities.
The term ""market"" is sometimes used for what are more strictly exchanges, organizations that facilitate the trade in financial securities, e.g., a stock exchange or commodity exchange. This may be a physical location (such as the New York Stock Exchange (NYSE), London Stock Exchange (LSE), JSE Limited (JSE), Bombay Stock Exchange (BSE) or an electronic system such as NASDAQ. Much trading of stocks takes place on an exchange; still, corporate actions (merger, spinoff) are outside an exchange, while any two companies or people, for whatever reason, may agree to sell the stock from the one to the other without using an exchange.
Trading of currencies and bonds is largely on a bilateral basis, although some bonds trade on a stock exchange, and people are building electronic systems for these as well,  to stock exchanges. There are also global initiatives such as the United Nations Sustainable Development Goal 10 which has a target to improve regulation and monitoring of global financial markets.",Financial market,0,1
867,10939,"In logic, mathematics, computer science, and linguistics, a formal language consists of words whose letters are taken from an alphabet and are well-formed according to a specific set of rules.
The alphabet of a formal language consists of symbols, letters, or tokens that concatenate into strings of the language. Each string concatenated from symbols of this alphabet is called a word, and the words that belong to a particular formal language are sometimes called well-formed words or well-formed formulas. A formal language is often defined by means of a formal grammar such as a regular grammar or context-free grammar, which consists of its formation rules.
The field of formal language theory studies primarily the purely syntactical aspects of such languages—that is, their internal structural patterns. Formal language theory sprang out of linguistics, as a way of understanding the syntactic regularities of natural languages.
In computer science, formal languages are used among others as the basis for defining the grammar of programming languages and formalized versions of subsets of natural languages in which the words of the language represent concepts that are associated with particular meanings or semantics. In computational complexity theory, decision problems are typically defined as formal languages, and complexity classes are defined as the sets of the formal languages that can be parsed by machines with limited computational power. In logic and the foundations of mathematics, formal languages are used to represent the syntax of axiomatic systems, and mathematical formalism is the philosophy that all of mathematics can be reduced to the syntactic manipulation of formal languages in this way.",Formal language,0,1
868,161883,"In computer science, formal methods are mathematically rigorous techniques for the specification, development, and verification of software and hardware systems. The use of formal methods for software and hardware design is motivated by the expectation that, as in other engineering disciplines, performing appropriate mathematical analysis can contribute to the reliability and robustness of a design.Formal methods employ a variety of theoretical computer science fundamentals, including logic calculi, formal languages, automata theory, control theory, program semantics, type systems, and type theory.",Formal methods,0,1
869,351887,"Friendly artificial intelligence (also friendly AI or FAI) refers to hypothetical artificial general intelligence (AGI) that would have a positive (benign) effect on humanity or at least align with human interests or contribute to foster the improvement of the human species. It is a part of the ethics of artificial intelligence and is closely related to machine ethics. While machine ethics is concerned with how an artificially intelligent agent should behave, friendly artificial intelligence research is focused on how to practically bring about this behaviour and ensuring it is adequately constrained.

",Friendly artificial intelligence,0,1
870,10933,"In computer science, functional programming is a programming paradigm where programs are constructed by applying and composing functions. It is a declarative programming paradigm in which function definitions are trees of expressions that map values to other values, rather than a sequence of imperative statements which update the running state of the program.
In functional programming, functions are treated as first-class citizens, meaning that they can be bound to names (including local identifiers), passed as arguments, and returned from other functions, just as any other data type can. This allows programs to be written in a declarative and composable style, where small functions are combined in a modular manner.
Functional programming is sometimes treated as synonymous with purely functional programming, a subset of functional programming which treats all functions as deterministic mathematical functions, or pure functions. When a pure function is called with some given arguments, it will always return the same result, and cannot be affected by any mutable state or other side effects. This is in contrast with impure procedures, common in imperative programming, which can have side effects (such as modifying the program's state or taking input from a user). Proponents of purely functional programming claim that by restricting side effects, programs can have fewer bugs, be easier to debug and test, and be more suited to formal verification.Functional programming has its roots in academia, evolving from the lambda calculus, a formal system of computation based only on functions. Functional programming has historically been less popular than imperative programming, but many functional languages are seeing use today in industry and education, including Common Lisp, Scheme, Clojure, Wolfram Language, Racket, Erlang, Elixir, OCaml, Haskell, and F#. Functional programming is also key to some languages that have found success in specific domains, like JavaScript in the Web, R in statistics, J, K and Q in financial analysis, and XQuery/XSLT for XML. Domain-specific declarative languages like SQL and Lex/Yacc use some elements of functional programming, such as not allowing mutable values. In addition, many other programming languages support programming in a functional style or have implemented features from functional programming, such as C++11, C#, Kotlin, Perl, PHP, Python, Go, Rust, Raku, Scala, and Java (since Java 8).",Functional programming,0,1
871,49180,"Fuzzy logic is a form of many-valued logic in which the truth value of variables may be any real number between 0 and 1. It is employed to handle the concept of partial truth, where the truth value may range between completely true and completely false. By contrast, in Boolean logic, the truth values of variables may only be the integer values 0 or 1.
The term fuzzy logic was introduced with the 1965 proposal of fuzzy set theory by Iranian Azerbaijani Turk mathematician Lotfi Zadeh.  Fuzzy logic had, however,  been studied since the 1920s, as infinite-valued logic—notably by Łukasiewicz and Tarski.Fuzzy logic is based on the observation that people make decisions based on imprecise and non-numerical information. Fuzzy models or sets are mathematical means of representing vagueness and imprecise information (hence the term fuzzy). These models have the capability of recognising, representing, manipulating, interpreting, and using data and information that are vague and lack certainty.Fuzzy logic has been applied to many fields, from control theory to artificial intelligence.

",Fuzzy logic,0,1
872,147954,"Goiás (Brazilian Portuguese: [ɡojˈjas] (listen)) is a Brazilian state located in the Center-West region. Goiás borders the Federal District and the states of (from north clockwise) Tocantins, Bahia, Minas Gerais, Mato Grosso do Sul and Mato Grosso. The state capital is Goiânia. With 7.2 million inhabitants, Goiás is the most populous state in the Center-West and the 11th most populous in the country. It has the ninth largest economy among Brazilian federative units. In Brazil's geoeconomic division, Goiás belongs to the Centro-Sul (Center-South), being the northernmost state of the southern portion of Brazil. The state has 3.3% of the Brazilian population and is responsible for 2.7% of the Brazilian GDP.The history of Goiás dates back to the beginning of the 18th century, with the arrival of pioneers from São Paulo. The Rio Vermelho region was the first to be occupied, where Vila Boa (later renamed Goiás) was founded. The development and settlement of the state took place, in a more intensified way, starting in the 1930s with the founding of the new capital, Goiânia, and especially with the construction of Brasília in the 1950s.Goiás is characterized by a landscape of chapadões (plateaus). In the height of the drought season, from June to September, the lack of rain makes the level of the Araguaia River go down and exposes almost 2 kilometres (1.2 mi) of beaches. At the Emas National Park in the municipality of Mineiros, it is possible to observe the typical fauna and flora from the region. At the Chapada dos Veadeiros the main attractions are the canyons, valleys, rapids and waterfalls. Other attractions include the historical city of Goiás and the hot springs of Caldas Novas.",Goiás,0,1
873,5218,"A central processing unit (CPU), also called a central processor, main processor or just processor, is the electronic circuitry that executes instructions comprising a computer program. The CPU performs basic arithmetic, logic, controlling, and input/output (I/O) operations specified by the instructions in the program. This contrasts with external components such as main memory and I/O circuitry, and specialized processors such as graphics processing units (GPUs).
The form, design, and implementation of CPUs have changed over time, but their fundamental operation remains almost unchanged. Principal components of a CPU include the arithmetic–logic unit (ALU) that performs arithmetic and logic operations, processor registers that supply operands to the ALU and store the results of ALU operations, and a control unit that orchestrates the fetching (from memory), decoding and execution (of instructions) by directing the coordinated operations of the ALU, registers and other components.
Most modern CPUs are implemented on integrated circuit (IC) microprocessors, with one or more CPUs on a single IC chip. Microprocessor chips with multiple CPUs are multi-core processors. The individual physical CPUs, processor cores, can also be multithreaded to create additional virtual or logical CPUs.An IC that contains a CPU may also contain memory, peripheral interfaces, and other components of a computer; such integrated devices are variously called microcontrollers or systems on a chip (SoC).
Array processors or vector processors have multiple processors that operate in parallel, with no unit considered central. Virtual CPUs are an abstraction of dynamical aggregated computational resources.

",Central processing unit,0,1
875,50569499,"Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. 
GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.

",Gated recurrent unit,0,1
876,50523514,"Gboard is a virtual keyboard app developed by Google for Android and iOS devices. It was first released on iOS in May 2016, followed by a release on Android in December 2016, debuting as a major update to the already-established Google Keyboard app on Android.
Gboard features Google Search, including web results (removed since April 2020) and predictive answers, easy searching and sharing of GIF and emoji content, a predictive typing engine suggesting the next word depending on context, and multilingual language support. Updates to the keyboard have enabled additional functionality, including GIF suggestions, options for a dark color theme or adding a personal image as the keyboard background, support for voice dictation, next-phrase prediction, and hand-drawn emoji recognition. At the time of its launch on iOS, the keyboard only offered support for the English language, with more languages being gradually added in the following months, whereas on Android, the keyboard supported more than 100 languages at the time of release.
In August 2018, Gboard passed 1 billion installs on the Google Play Store, making it one of the most popular Android apps. This is measured by the Google Play Store and includes downloads by users as well as pre-installed instances of the app.",Gboard,0,1
877,7408685,"General game playing (GGP) is the design of artificial intelligence programs to be able to play more than one game successfully. For many games like chess, computers are programmed to play these games using a specially designed algorithm, which cannot be transferred to another context. For instance, a chess-playing computer program cannot play checkers. General game playing is considered as a necessary milestone on the way to artificial general intelligence.General video game playing  (GVGP) is the concept of GGP adjusted to the purpose of playing video games. For video games, game rules have to be either learnt over multiple iterations by artificial players like TD-Gammon, or are predefined manually in a domain-specific language and sent in advance to artificial players like in traditional GGP. Starting in 2013, significant progress was made following the deep reinforcement learning approach, including the development of programs that can learn to play Atari 2600 games as well as a program that can learn to play Nintendo Entertainment System games.The first commercial usage of general game playing technology was Zillions of Games in 1998. General game playing was also proposed for trading agents in supply chain management thereunder price negotiation in online auctions from 2003 on.",General game playing,0,1
879,50073184,"A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014. Two neural networks contest with each other in a game (in the form of a zero-sum game, where one agent's gain is another agent's loss).
Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning, fully supervised learning, and reinforcement learning.The core idea of a GAN is based on the ""indirect"" training through the discriminator, another neural network that can to tell how much input is ""realistic"", which itself is also being updated dynamically. This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.
GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.

",Generative adversarial network,0,1
880,40254,"In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). Genetic algorithms are commonly used to generate high-quality solutions to optimization and search problems by relying on biologically inspired operators such as mutation, crossover and selection. Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles, hyperparameter optimization, etc.",Genetic algorithm,0,1
881,507174,"Geoffrey Everest Hinton  (born 6 December 1947) is a British-Canadian cognitive psychologist and computer scientist, most noted for his work on artificial neural networks. Since 2013, he has divided his time working for Google (Google Brain) and the University of Toronto. In 2017, he co-founded and became the Chief Scientific Advisor of the Vector Institute in Toronto.With David Rumelhart and Ronald J. Williams, Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks, although they were not the first to propose the approach. Hinton is viewed as a leading figure in the deep learning community. The dramatic image-recognition milestone of the AlexNet designed in collaboration with his students Alex Krizhevsky and Ilya Sutskever for the ImageNet challenge 2012 was a breakthrough in the field of computer vision.Hinton received the 2018 Turing Award, together with Yoshua Bengio and Yann LeCun, for their work on deep learning. They are sometimes referred to as the ""Godfathers of AI"" and ""Godfathers of Deep Learning"", and have continued to give public talks together.",Geoffrey Hinton,1,1
884,72798,"Goofy is a cartoon character created by The Walt Disney Company. He is a tall, anthropomorphic dog who typically wears a turtle neck and vest, with pants, shoes, white gloves, and a tall hat originally designed as a rumpled fedora. Goofy is a close friend of Mickey Mouse and Donald Duck. He is normally characterized as hopelessly clumsy and dim-witted, yet this interpretation is not always definitive; occasionally, Goofy is shown as intuitive and clever, albeit in his own unique, eccentric way.
Goofy debuted in animated cartoons, starting in 1932 with Mickey's Revue as Dippy Dawg, who is older than Goofy would come to be. Later the same year, he was re-imagined as a younger character, now called Goofy, in the short The Whoopee Party. During the 1930s, he was used extensively as part of a comedy trio with Mickey and Donald. Starting in 1939, Goofy was given his own series of shorts that were popular in the 1940s and early 1950s. Two Goofy shorts were nominated for an Oscar: How to Play Football (1944) and Aquamania (1961). He also co-starred in a short series with Donald, including Polar Trappers (1938), where they first appeared without Mickey Mouse. Three more Goofy shorts were produced in the 1960s after which Goofy was only seen in television and Disney comics. He returned to theatrical animation in 1983 with Mickey's Christmas Carol. His most recent theatrical appearance was How to Hook Up Your Home Theater in 2007. Goofy has also been featured in television, most extensively in Goof Troop (1992), House of Mouse (2001–2003), Mickey Mouse Clubhouse (2006–2016), Mickey Mouse (2013–2019), and Mickey and the Roadster Racers / Mickey Mouse Mixed-Up Adventures (2017–2021).
Originally known as Dippy Dawg, the character is more commonly known simply as ""Goofy"", a name used in his short film series. In his 1950s cartoons, he usually played a character called George G. Geef. Sources from the Goof Troop continuity give the character's full name as G. G. ""Goofy"" Goof, likely in reference to the 1950s name. In many other sources, both animated and comics, the surname Goof continues to be used. In other 2000s-era comics, the character's full name has occasionally been given as Goofus D. Dawg.",Goofy,0,1
885,38651188,"Google APIs are application programming interfaces (APIs) developed by Google which allow communication with Google Services and their integration to other services. Examples of these include Search, Gmail, Translate or Google Maps. Third-party apps can use these APIs to take advantage of or extend the functionality of the existing services.
The APIs provide functionality like analytics, machine learning as a service (the Prediction API) or access to user data (when permission to read the data is given). Another important example is an embedded Google map on a website, which can be achieved using the Static Maps API, Places API or Google Earth API.",Google APIs,0,1
886,41184517,"Google Brain is a deep learning artificial intelligence research team under the umbrella of Google AI, a research division at Google dedicated to artificial intelligence. Formed in 2011, Google Brain combines open-ended machine learning research with information systems and large-scale computing resources. The team has created tools such as TensorFlow, which allow for neural networks to be used by the public, with multiple internal AI research projects. The team aims to create research opportunities in machine learning and natural language processing.",Google Brain,0,1
887,2671720,"Gordon David Plotkin,  (born 9 September 1946) is a theoretical computer scientist in the School of Informatics at the University of Edinburgh. Plotkin is probably best known for his introduction of structural operational semantics (SOS) and his work on denotational semantics. In particular, his notes on A Structural Approach to Operational Semantics were very influential. He has contributed to many other areas of computer science.",Gordon Plotkin,0,1
888,201489,"In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm for finding a local minimum of a differentiable function. The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) of the function at the current point, because this is the direction of steepest descent. Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent.
Gradient descent is generally attributed to Augustin-Louis Cauchy, who first suggested it in 1847. Jacques Hadamard independently proposed a similar method in 1907. Its convergence properties for non-linear optimization problems were first studied by Haskell Curry in 1944, with the method becoming increasingly well-studied and used in the following decades.",Gradient descent,0,1
889,4375576,"Grammar induction (or grammatical inference) is the process in machine learning of learning a formal grammar (usually as a collection of re-write rules or productions or alternatively as a finite state machine or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.",Grammar induction,0,1
891,390214,"A graphics processing unit (GPU) is a specialized electronic circuit designed to manipulate and alter memory to accelerate the creation of images in a frame buffer intended for output to a display device. GPUs are used in embedded systems, mobile phones, personal computers, workstations, and game consoles.
Modern GPUs are efficient at manipulating computer graphics and image processing. Their parallel structure makes them more efficient than general-purpose central processing units (CPUs) for algorithms that process large blocks of data in parallel. In a personal computer, a GPU can be present on a video card or embedded on the motherboard. In some CPUs, they are embedded on the CPU die.In the 1970s, the term ""GPU"" originally stood for graphics processor unit and described a programmable processing unit independently working from the CPU and responsible for graphics manipulation and output. Later, in 1994, Sony used the term (now standing for graphics processing unit) in reference to the PlayStation console's Toshiba-designed Sony GPU in 1994. The term was popularized by Nvidia in 1999, who marketed the GeForce 256 as ""the world's first GPU"". It was presented as a ""single-chip processor with integrated transform, lighting, triangle setup/clipping, and rendering engines"". Rival ATI Technologies coined the term ""visual processing unit"" or VPU with the release of the Radeon 9700 in 2002.",Graphics processing unit,0,1
892,1661475,"Green computing, green IT, or ICT sustainability, is the study and practice of environmentally sustainable computing or IT.
The goals of green computing are similar to green chemistry: reduce the use of hazardous materials, maximize energy efficiency during the product's lifetime,  the recyclability or biodegradability of defunct products and factory waste. Green computing is important for all classes of systems, ranging from handheld systems to large-scale data centers.
Many corporate IT departments have green computing initiatives to reduce the environmental effect of their IT operations.",Green computing,0,1
893,203619,"Handwriting recognition (HWR), also known as handwritten text recognition (HTR), is the ability of a computer to receive and interpret intelligible handwritten input from sources such as paper documents, photographs, touch-screens and other devices. The image of the written text may be sensed ""off line"" from a piece of paper by optical scanning (optical character recognition) or intelligent word recognition. Alternatively, the movements of the pen tip may be sensed ""on line"", for example by a pen-based computer screen surface, a generally easier task as there are more clues available. A handwriting recognition system handles formatting, performs correct segmentation into characters, and finds the most plausible words.",Handwriting recognition,0,1
894,2031045,"Hardware acceleration is the use of computer hardware designed to perform specific functions more efficiently when compared to software running on a general-purpose central processing unit (CPU). Any transformation of data that can be calculated in software running on a generic CPU can also be calculated in custom-made hardware, or in some mix of both.
To perform computing tasks more quickly (or better in some other way), generally one can invest time and money in improving the software, improving the hardware, or both. There are various approaches with advantages and disadvantages in terms of decreased latency, increased throughput and reduced energy consumption. Typical advantages of focusing on software may include more rapid development, lower non-recurring engineering costs, heightened portability, and ease of updating features or patching bugs, at the cost of overhead to compute general operations. Advantages of focusing on hardware may include speedup, reduced power consumption, lower latency, increased parallelism and bandwidth, and better utilization of area and functional components available on an integrated circuit; at the cost of lower ability to update designs once etched onto silicon and higher costs of functional verification, and times to market. In the hierarchy of digital computing systems ranging from general-purpose processors to fully customized hardware, there is a tradeoff between flexibility and efficiency, with efficiency increasing by orders of magnitude when any given application is implemented higher up that hierarchy. This hierarchy includes general-purpose processors such as CPUs, more specialized processors such as GPUs, fixed-function implemented on field-programmable gate arrays (FPGAs), and fixed-function implemented on application-specific integrated circuits (ASICs).Hardware acceleration is advantageous for performance, and practical when the functions are fixed so updates are not as needed as in software solutions. With the advent of reprogrammable logic devices such as FPGAs, the restriction of hardware acceleration to fully fixed algorithms has eased since 2010, allowing hardware acceleration to be applied to problem domains requiring modification to algorithms and processing control flow. The disadvantage however, is that in many open source projects, it requires proprietary libraries that not all vendors are keen to distribute or expose, making it difficult to integrate in such projects.

",Hardware acceleration,0,1
895,18426501,"Harvard University is a private Ivy League research university in Cambridge, Massachusetts. Founded in 1636 as Harvard College and named for its first benefactor, the Puritan clergyman John Harvard, it is the oldest institution of higher learning in the United States and among the most prestigious in the world.The Massachusetts colonial legislature authorized Harvard's founding, ""dreading to leave an illiterate ministry to the churches, when our present ministers shall lie in the dust""; though never formally affiliated with any denomination, in its early years Harvard College primarily trained Congregational clergy. Its curriculum and student body were gradually secularized during the 18th century, and by the 19th century, it had emerged as the central cultural establishment among the Boston elite.
Following the American Civil War, President Charles William Eliot's long tenure (1869–1909) transformed the college and affiliated professional schools into a modern research university; Harvard became a founding member of the Association of American Universities in 1900. James B. Conant led the university through the Great Depression and World War II, and liberalized admissions after the war.
The university is composed of ten academic faculties plus the Harvard Radcliffe Institute. Arts and Sciences offers study in a wide range of academic disciplines for undergraduates and for graduates, while the other faculties offer only graduate degrees, mostly professional. Harvard has three main campuses:
the 209-acre (85 ha) Cambridge campus centered on Harvard Yard; an adjoining campus immediately across the Charles River in the Allston neighborhood of Boston; and the medical campus in Boston's Longwood Medical Area. Harvard's endowment is valued at $53.2 billion, making it the largest of any academic institution. Endowment income helps enable the undergraduate college to admit students regardless of financial need and provide generous financial aid with no loans. The Harvard Library is the world's largest academic library system, comprising 79 individual libraries holding about 20.4 million items.Harvard alumni, faculty, and researchers have included numerous Nobel laureates, Fields Medalists, members of the U.S. Congress, MacArthur Fellows, Rhodes Scholars, Marshall Scholars, and Fulbright Scholars, all of which are arguably the most among all higher education institutions over the globe, depending upon the metrics a list adopts. Its alumni include eight U.S. presidents and 188 living billionaires, the most of any university. Fourteen Turing Award laureates have been Harvard affiliates. Students and alumni have won 10 Academy Awards, 48 Pulitzer Prizes, and 110 Olympic medals (46 gold), and they have founded many notable companies.",Harvard University,0,1
896,22048289,"The Handle System is the Corporation for National Research Initiatives's proprietary registry assigning persistent identifiers, or handles, to information resources, and for resolving ""those handles into the information necessary to locate, access, and otherwise make use of the resources"".As with handles used elsewhere in computing, Handle System handles are opaque, and encode no information about the underlying resource, being bound only to metadata regarding the resource. Consequently, the handles are not rendered invalid by changes to the metadata.
The system was developed by Bob Kahn at the Corporation for National Research Initiatives (CNRI). The original work was funded by the Defense Advanced Research Projects Agency (DARPA) between 1992 and 1996, as part of a wider framework for distributed digital object services, and was thus contemporaneous with the early deployment of the World Wide Web, with similar goals.
The Handle System was first implemented in autumn 1994, and was administered and operated by CNRI until December 2015, when a new ""multi-primary administrator"" (MPA) mode of operation was introduced. The DONA Foundation now administers the system's Global Handle Registry and accredits MPAs, including CNRI and the International DOI Foundation.
The system currently provides the underlying infrastructure for such handle-based systems as Digital Object Identifiers and DSpace, which are mainly used to provide access to scholarly, professional and government documents and other information resources.
CNRI provides specifications and the source code for reference implementations for the servers and protocols used in the system under a royalty-free ""Public License"", similar to an open source license.Thousands of handle services are currently running. Over 1000 of these are at universities and libraries, but they are also in operation at national laboratories, research groups, government agencies, and commercial enterprises, receiving over 200 million resolution requests per month.

",Handle System,0,1
897,351581,"Health informatics is the field of science and engineering that aims at developing methods and technologies for the acquisition, processing, and study of patient data, which can come from different sources and modalities, such as electronic health records, diagnostic test results, medical scans. The health domain provides an extremely wide variety of problems that can be tackled using computational techniques.Health informatics is a spectrum of multidisciplinary fields that includes study of the design, development and application of computational innovations to improve health care. The disciplines involved combines medicine fields with computing fields, in particular computer engineering, software engineering, information engineering, bioinformatics, bio-inspired computing, theoretical computer science, information systems, data science, information technology, autonomic computing, and behavior informatics. In academic institutions, medical informatics research focus on applications of artificial intelligence in healthcare and designing medical devices based on embedded systems. In some countries term informatics is also used in the context of applying library science to data management in hospitals.",Health informatics,0,1
898,63452,"A heuristic (; from Ancient Greek  εὑρίσκω (heurískō) 'I find, discover'), or heuristic technique, is any approach to problem solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate, short-term goal or approximation. Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution. Heuristics can be mental shortcuts that ease the cognitive load of making a decision.Examples that employ heuristics include using trial and error, a rule of thumb or an educated guess.
Heuristics are the strategies derived from previous experiences with similar problems. These strategies depend on using readily accessible, though loosely applicable, information to control problem solving in human beings, machines and abstract issues. When an individual applies a heuristic in practice, it generally performs as expected. However it can alternatively create systematic errors.
The most fundamental heuristic is trial and error, which can be used in everything from matching nuts and bolts to finding the values of variables in algebra problems. In mathematics, some common heuristics involve the use of visual representations, additional assumptions, forward/backward reasoning and simplification. Here are a few commonly used heuristics from George Pólya's 1945 book, How to Solve It:
If you are having difficulty understanding a problem, try drawing a picture.
If you can't find a solution, try assuming that you have a solution and seeing what you can derive from that (""working backward"").
If the problem is abstract, try examining a concrete example.
Try solving a more general problem first (the ""inventor's paradox"": the more ambitious plan may have more chances of success).
In psychology, heuristics are simple, efficient rules, either learned or inculcated by evolutionary processes. These psychological heuristics have been proposed to explain how people make decisions, come to judgements, and solve problems. These rules typically come into play when people face complex problems or incomplete information. Researchers employ various methods to test whether people use these rules. The rules have been shown to work well under most circumstances, but in certain cases can lead to systematic errors or cognitive biases.

",Heuristic,0,1
899,14220429,"In mathematical optimization and computer science, heuristic (from Greek εὑρίσκω ""I find, discover"") is a technique designed for solving a problem more quickly when classic methods are too slow or for finding an approximate solution when classic methods fail to find any exact solution.  This is achieved by trading optimality, completeness, accuracy, or precision for speed.  In a way, it can be considered a shortcut.
A heuristic function, also simply called a heuristic, is a function that ranks alternatives in search algorithms at each branching step based on available information to decide which branch to follow. For example, it may approximate the exact solution.

",Heuristic (computer science),0,1
900,98770,"A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled is assumed to be a Markov process — call it 
  
    
      
        X
      
    
    {\displaystyle X}
   — with unobservable (""hidden"") states. As part of the definition, HMM requires that there be an observable process 
  
    
      
        Y
      
    
    {\displaystyle Y}
   whose outcomes are ""influenced"" by the outcomes of 
  
    
      
        X
      
    
    {\displaystyle X}
   in a known way. Since 
  
    
      
        X
      
    
    {\displaystyle X}
   cannot be observed directly, the goal is to learn about 
  
    
      
        X
      
    
    {\displaystyle X}
   by observing 
  
    
      
        Y
        .
      
    
    {\displaystyle Y.}
   HMM has an additional requirement that the outcome of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   at time 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
   must be ""influenced"" exclusively by the outcome of 
  
    
      
        X
      
    
    {\displaystyle X}
   at 
  
    
      
        t
        =
        
          t
          
            0
          
        
      
    
    {\displaystyle t=t_{0}}
   and that the outcomes of 
  
    
      
        X
      
    
    {\displaystyle X}
   and 
  
    
      
        Y
      
    
    {\displaystyle Y}
   at 
  
    
      
        t
        <
        
          t
          
            0
          
        
      
    
    {\displaystyle t<t_{0}}
   must not affect the outcome of 
  
    
      
        Y
      
    
    {\displaystyle Y}
   at 
  
    
      
        t
        =
        
          t
          
            0
          
        
        .
      
    
    {\displaystyle t=t_{0}.}
  
Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, physics, chemistry, economics, finance, signal processing, information theory, pattern recognition - such as speech, handwriting, gesture recognition, part-of-speech tagging, musical score following, partial discharges and bioinformatics.",Hidden Markov model,0,1
901,477573,"In data mining and statistics, hierarchical clustering (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a hierarchy of clusters. Strategies for hierarchical clustering generally fall into two types:

Agglomerative: This is a ""bottom-up"" approach: each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.
Divisive: This is a ""top-down"" approach: all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.In general, the merges and splits are determined in a greedy manner. The results of hierarchical clustering are usually presented in a dendrogram.
The standard algorithm for hierarchical agglomerative clustering (HAC) has a time complexity of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
   and requires 
  
    
      
        Ω
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle \Omega (n^{2})}
   memory, which makes it too slow for even medium data sets. However, for some special cases, optimal efficient agglomerative methods (of complexity 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2})}
  ) are known: SLINK for single-linkage and CLINK for complete-linkage clustering. With a heap, the runtime of the general case can be reduced to 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            2
          
        
        log
        ⁡
        n
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{2}\log n)}
  , an improvement on the aforementioned bound of 
  
    
      
        
          
            O
          
        
        (
        
          n
          
            3
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(n^{3})}
  , at the cost of further increasing the memory requirements. In many cases, the memory overheads of this approach are too large to make it practically usable.
Except for the special case of single-linkage, none of the algorithms (except exhaustive search in 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  ) can be guaranteed to find the optimum solution.
Divisive clustering with an exhaustive search is 
  
    
      
        
          
            O
          
        
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle {\mathcal {O}}(2^{n})}
  , but it is common to use faster heuristics to choose splits, such as k-means.",Hierarchical clustering,0,1
902,2894560,"The history of artificial intelligence (AI) began in antiquity, with myths, stories and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The seeds of modern AI were planted by philosophers who attempted to describe the process of human thinking as the mechanical manipulation of symbols. This work culminated in the invention of the programmable digital computer in the 1940s, a machine based on the abstract essence of mathematical reasoning. This device and the ideas behind it inspired a handful of scientists to begin seriously discussing the possibility of building an electronic brain.
The field of AI research was founded at a workshop held on the campus of Dartmouth College, USA during the summer of 1956. Those who attended would become the leaders of AI research for decades. Many of them predicted that a machine as intelligent as a human being would exist in no more than a generation, and they were given millions of dollars to make this vision come true.Eventually, it became obvious that commercial developers and researchers had grossly underestimated the difficulty of the project. In 1974, in response to the criticism from James Lighthill and ongoing pressure from congress, the U.S. and British Governments stopped funding undirected research into artificial intelligence, and the difficult years that followed would later be known as an ""AI winter"". Seven years later, a visionary initiative by the Japanese Government inspired governments and industry to provide AI with billions of dollars, but by the late 80s the investors became disillusioned and withdrew funding again.
Investment and interest in AI boomed in the first decades of the 21st century when machine learning was successfully applied to many problems in academia and industry due to new methods, the application of powerful computer hardware, and the collection of immense data sets.",History of artificial intelligence,0,1
903,22904524,"Human-in-the-loop  or HITL is defined as a model that requires human interaction. HITL is associated with modeling and simulation (M&S) in the live, virtual, and constructive taxonomy. HITL models may conform to human factors requirements as in the case of a mockup.  In this type of simulation a human is always part of the simulation and consequently influences the outcome in such a way that is difficult if not impossible to reproduce exactly.  HITL also readily allows for the identification of problems and requirements that may not be easily identified by other means of simulation.
HITL is often referred to as interactive simulation, which is a special kind of physical simulation in which physical simulations include human operators, such as in a flight or a driving simulator.",Human-in-the-loop,0,1
904,490620,"The human brain is the central organ of the human nervous system, and with the spinal cord makes up the central nervous system. The brain consists of the cerebrum, the brainstem and the cerebellum. It controls most of the activities of the body, processing, integrating, and coordinating the information it receives from the sense organs, and making decisions as to the instructions sent to the rest of the body. The brain is contained in, and protected by, the skull bones of the head.
The cerebrum, the largest part of the human brain, consists of two cerebral hemispheres. Each hemisphere has an inner core composed of white matter, and an outer surface – the cerebral cortex – composed of grey matter. The cortex has an outer layer, the neocortex, and an inner allocortex. The neocortex is made up of six neuronal layers, while the allocortex has three or four. Each hemisphere is conventionally divided into four lobes – the frontal, temporal, parietal, and occipital lobes. The frontal lobe is associated with executive functions including self-control, planning, reasoning, and abstract thought, while the occipital lobe is dedicated to vision. Within each lobe, cortical areas are associated with specific functions, such as the sensory, motor and association regions. Although the left and right hemispheres are broadly similar in shape and function, some functions are associated with one side, such as language in the left and visual-spatial ability in the right. The hemispheres are connected by commissural nerve tracts, the largest being the corpus callosum.
The cerebrum is connected by the brainstem to the spinal cord. The brainstem consists of the midbrain, the pons, and the medulla oblongata. The cerebellum is connected to the brainstem by three pairs of nerve tracts called cerebellar peduncles. Within the cerebrum is the ventricular system, consisting of four interconnected ventricles in which cerebrospinal fluid is produced and circulated. Underneath the cerebral cortex are several important structures, including the thalamus, the epithalamus, the pineal gland, the hypothalamus, the pituitary gland, and the subthalamus; the limbic structures, including the amygdala and the hippocampus; the claustrum, the various nuclei of the basal ganglia; the basal forebrain structures, and the three circumventricular organs. The cells of the brain include neurons and supportive glial cells. There are more than 86 billion neurons in the brain, and a more or less equal number of other cells. Brain activity is made possible by the interconnections of neurons and their release of neurotransmitters in response to nerve impulses. Neurons connect to form neural pathways, neural circuits, and elaborate network systems. The whole circuitry is driven by the process of neurotransmission.
The brain is protected by the skull, suspended in cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier. However, the brain is still susceptible to damage, disease, and infection. Damage can be caused by trauma, or a loss of blood supply known as a stroke. The brain is susceptible to degenerative disorders, such as Parkinson's disease, dementias including Alzheimer's disease, and multiple sclerosis. Psychiatric conditions, including schizophrenia and clinical depression, are thought to be associated with brain dysfunctions. The brain can also be the site of tumours, both benign and malignant; these mostly originate from other sites in the body.
The study of the anatomy of the brain is neuroanatomy, while the study of its function is neuroscience. Numerous techniques are used to study the brain. Specimens from other animals, which may be examined microscopically, have traditionally provided much information. Medical imaging technologies such as functional neuroimaging, and electroencephalography (EEG) recordings are important in studying the brain. The medical history of people with brain injury has provided insight into the function of each part of the brain. Brain research has evolved over time, with philosophical, experimental, and theoretical phases. An emerging phase may be to simulate brain activity.In culture, the philosophy of mind has for centuries attempted to address the question of the nature of consciousness and the mind–body problem. The pseudoscience of phrenology attempted to localise personality attributes to regions of the cortex in the 19th century. In science fiction, brain transplants are imagined in tales such as the 1942 Donovan's Brain.",Human brain,0,1
905,4287389,"Human image synthesis is technology that can be applied to make believable and even photorealistic renditions of human-likenesses, moving or still. It has effectively existed since the early 2000s. Many films using computer generated imagery have featured synthetic images of human-like characters digitally composited onto the real or other simulated film material. Towards the end of the 2010s deep learning artificial intelligence has been applied to synthesize images and video that look like humans, without need for human assistance, once the training phase has been completed, whereas the old school 7D-route required massive amounts of human work.",Human image synthesis,0,1
906,23534602,"Human–computer interaction (HCI) is research in the design and the use of computer technology, which focuses on the interfaces between people (users) and computers. HCI researchers observe the ways humans interact with computers and design technologies that allow humans to interact with computers in novel ways.
As a field of research, human–computer interaction is situated at the intersection of computer science, behavioral sciences, design, media studies, and several other fields of study. The term was popularized by Stuart K. Card, Allen Newell, and Thomas P. Moran in their 1983 book, The Psychology of Human–Computer Interaction. The first known use was in 1975 by Carlisle. The term is intended to convey that, unlike other tools with specific and limited uses, computers have many uses which often involve an open-ended dialogue between the user and the computer. The notion of dialogue likens human–computer interaction to human-to-human interaction: an analogy that is crucial to theoretical considerations in the field.",Human–computer interaction,0,1
907,22584291,"IBM Watson is a question-answering computer system capable of answering questions posed in natural language, developed in IBM's DeepQA project by a research team led by principal investigator David Ferrucci. Watson was named after IBM's founder and first CEO, industrialist Thomas J. Watson.The computer system was initially developed to answer questions on the quiz show Jeopardy! and, in 2011, the Watson computer system competed on Jeopardy! against champions Brad Rutter and Ken Jennings, winning the first place prize of $1 million.In February 2013, IBM announced that Watson software system's first commercial application would be for utilization management decisions in lung cancer treatment at Memorial Sloan Kettering Cancer Center, New York City, in conjunction with WellPoint (now Anthem).",IBM Watson,1,1
908,55207134,"Watson Studio, formerly Data Science Experience or DSX, is IBM’s software platform for data science. The platform consists of a workspace that includes multiple collaboration and open-source tools for use in data science.In Watson Studio, a data scientist can create a project with a group of collaborators, all having access to various analytics models and using various languages (R/Python/Scala). Watson Studio brings together staple open source tools including RStudio, Spark and Python in an integrated environment, along with additional tools such as a managed Spark service and data shaping facilities, in a secure and governed environment.Watson Studio provides access to data sets that are available through Watson Data Platform, on-premises or on the cloud. The platform also has a large community and embedded resources such as articles on the latest developments from the data science world and public data sets. The platform is available in on-premises, cloud, and desktop forms.

",IBM Watson Studio,0,1
909,1951226,"IEEE Spectrum is a magazine edited by the Institute of Electrical and Electronics Engineers. IEEE Spectrum publishes 12 issues annually.
The first issue of IEEE Spectrum published in January 1964 as a successor to Electrical Engineering. The magazine contains peer-reviewed articles about technology and science trends affecting business and society, focused on electrical and electronics engineering, mechanical and civil engineering, computer science, biology, physics, and mathematics.
In 2010, IEEE Spectrum was the recipient of Utne Reader magazines Utne Independent Press Award for Science/Technology Coverage. In 2012 IEEE Spectrum was selected as the winner of the National Magazine Awards ""General Excellence Among Thought Leader Magazines"" category.

",IEEE Spectrum,0,1
910,29261811,"The i is a British national morning paper published in London by Daily Mail and General Trust and distributed across the United Kingdom. It is aimed at ""readers and lapsed readers"" of all ages and commuters with limited time, and was originally launched in 2010 as a sister paper to The Independent. It was later acquired by Johnston Press in 2016 after The Independent shifted to a digital-only model. The i came under the control of JPIMedia a day after Johnston Press filed for administration on 16 November 2018. The paper and its website were bought by the Daily Mail and General Trust (DMGT) on 29 November 2019, for £49.6 million. On 6 December 2019 the Competition and Markets Authority served an initial enforcement order on DMGT and DMG Media Limited requiring the paper to be run separately pending investigation.The i was named British National Newspaper of the Year in 2015.
Since its inception, the i has expanded its layout and coverage, adding special sections for notable events and revamping its weekend edition. The paper had an average daily circulation of 302,757 in March 2013, significantly more than The Independent, though that figure has since continued to decline, and had dropped to 233,869 by February 2019. The paper is classified as a 'quality' in the UK market but is published in the standard compact tabloid-size format.

",I (newspaper),0,1
911,51213339,"Ian J. Goodfellow (born 1985 or 1986) is a computer scientist, engineer, and executive, most noted for his work on artificial neural networks and deep learning. He was previously employed as a research scientist at Google Brain and director of machine learning at Apple and has made several important contributions to the field of deep learning including the invention of the generative adversarial network (GAN). Goodfellow wrote the chapter on deep learning in the most popular textbook in the field of artificial intelligence, Artificial Intelligence: A Modern Approach (used in more than 1,500 universities in 135 countries), as well as the textbook Deep Learning.",Ian Goodfellow,0,1
912,46469,"Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. Algorithms may take advantage of visual perception and the statistical properties of image data to provide superior results compared with generic data compression methods which are used for other digital data.",Image compression,0,1
913,476836,"Noise reduction is the process of removing noise from a signal. Noise reduction techniques exist for audio and images. Noise reduction algorithms may distort the signal to some degree.
All signal processing devices, both analog and digital, have traits that make them susceptible to noise. Noise can be random with an even frequency distribution (white noise), or frequency-dependent noise introduced by a device's mechanism or signal processing algorithms.
In electronic recording devices, a major type of noise is hiss created by random electron motion due to thermal agitation that occurs due to temperature. These agitated electrons rapidly add and subtract from the voltage of the output signal and thus create detectable noise.
In the case of photographic film and magnetic tape, noise (both visible and audible) is introduced due to the grain structure of the medium. In photographic film, the size of the grains in the film determines the film's sensitivity, more sensitive film having larger-sized grains. In magnetic tape, the larger the grains of the magnetic particles (usually ferric oxide or magnetite), the more prone the medium is to noise. To compensate for this, larger areas of film or magnetic tape may be used to lower the noise to an acceptable level.

",Noise reduction,0,1
914,598031,"In signal processing, independent component analysis (ICA) is a computational method for separating a multivariate signal into additive subcomponents. This is done by assuming that at most one subcomponent is a non-Gaussian signals and that the subcomponents are statistically independent from each other. ICA is a special case of blind source separation. A common example application is the ""cocktail party problem"" of listening in on one person's speech in a noisy room.",Independent component analysis,0,1
915,54069,"Inductive logic programming (ILP) is a subfield of symbolic artificial intelligence  which uses logic programming as a uniform representation for examples, background knowledge and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesised logic program which entails all the positive and none of the negative examples.

Schema: positive examples + negative examples + background knowledge ⇒ hypothesis.Inductive logic programming is particularly useful in bioinformatics and natural language processing. Gordon Plotkin and Ehud Shapiro laid the initial theoretical foundation for inductive machine learning in a logical setting. Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples. The term Inductive Logic Programming was first introduced in a paper by Stephen Muggleton in 1991. Muggleton also founded the annual international conference on Inductive Logic Programming, introduced the theoretical ideas of Predicate Invention, Inverse resolution, and Inverse entailment. Muggleton implemented Inverse entailment first in the PROGOL system. The term ""inductive"" here refers to philosophical (i.e. suggesting a theory to explain observed facts) rather than mathematical (i.e. proving a property for all members of a well-ordered set) induction.",Inductive logic programming,0,1
916,41644056,"Inductive programming (IP) is a special area of automatic programming, covering research from artificial intelligence and programming, which addresses learning of typically declarative (logic or functional) and often recursive programs from incomplete specifications, such as input/output examples or constraints.
Depending on the programming language used, there are several kinds of inductive programming. Inductive functional programming, which uses functional programming languages such as Lisp or Haskell, and most especially inductive logic programming, which uses logic programming languages such as Prolog and other logical representations  such as description logics, have been more prominent, but other (programming) language paradigms have also been used, such as constraint programming or probabilistic programming.

",Inductive programming,0,1
918,14539,"The Internet (or internet) is the global system of interconnected computer networks that uses the Internet protocol suite (TCP/IP) to communicate between networks and devices. It is a network of networks that consists of private, public, academic, business, and government networks of local to global scope, linked by a broad array of electronic, wireless, and optical networking technologies. The Internet carries a vast range of information resources and services, such as the inter-linked hypertext documents and applications of the World Wide Web (WWW), electronic mail, telephony, and file sharing.
The origins of the Internet date back to the development of packet switching and research commissioned by the United States Department of Defense in the 1960s to enable time-sharing of computers. The primary precursor network, the ARPANET, initially served as a backbone for interconnection of regional academic and military networks in the 1970s. The funding of the National Science Foundation Network as a new backbone in the 1980s, as well as private funding for other commercial extensions, led to worldwide participation in the development of new networking technologies, and the merger of many networks. The linking of commercial networks and enterprises by the early 1990s marked the beginning of the transition to the modern Internet, and generated a sustained exponential growth as generations of institutional, personal, and mobile computers were connected to the network. Although the Internet was widely used by academia in the 1980s, commercialization incorporated its services and technologies into virtually every aspect of modern life.
Most traditional communication media, including telephone, radio, television, paper mail and newspapers are reshaped, redefined, or even bypassed by the Internet, giving birth to new services such as email, Internet telephone, Internet television, online music, digital newspapers, and video streaming websites. Newspaper, book, and other print publishing are adapting to website technology, or are reshaped into blogging, web feeds and online news aggregators. The Internet has enabled and accelerated new forms of personal interactions through instant messaging, Internet forums, and social networking services. Online shopping has grown exponentially for major retailers, small businesses, and entrepreneurs, as it enables firms to extend their ""brick and mortar"" presence to serve a larger market or even sell goods and services entirely online. Business-to-business and financial services on the Internet affect supply chains across entire industries.
The Internet has no single centralized governance in either technological implementation or policies for access and usage; each constituent network sets its own policies. The overreaching definitions of the two principal name spaces in the Internet, the Internet Protocol address (IP address) space and the Domain Name System (DNS), are directed by a maintainer organization, the Internet Corporation for Assigned Names and Numbers (ICANN). The technical underpinning and standardization of the core protocols is an activity of the Internet Engineering Task Force (IETF), a non-profit organization of loosely affiliated international participants that anyone may associate with by contributing technical expertise. In November 2006, the Internet was included on USA Today's list of New Seven Wonders.",Internet,0,1
919,20110874,"Reference is a relationship between objects in which one object designates, or acts as a means by which to connect to or link to, another object. The first object in this relation is said to refer to the second object. It is called a name for the second object. The second object, the one to which the first object refers, is called the referent of the first object. A name is usually a phrase or expression, or some other symbolic representation. Its referent may be anything – a material object, a person, an event, an activity, or an abstract concept.
References can take on many forms, including: a thought, a sensory perception that is audible (onomatopoeia), visual (text), olfactory, or tactile, emotional state, relationship with other, spacetime coordinate, symbolic or alpha-numeric, a physical object or an energy projection. In some cases, methods are used that intentionally hide the reference from some observers, as in cryptography.References feature in many spheres of human activity and knowledge, and the term adopts shades of meaning particular to the contexts in which it is used. Some of them are described in the sections below.

",Reference,0,1
920,1194259,"An influence diagram (ID) (also called a relevance diagram, decision diagram or a decision network) is a compact graphical and mathematical representation of a decision situation.  It is a generalization of a Bayesian network, in which not only probabilistic inference problems but also decision making problems (following the maximum expected utility criterion) can be modeled and solved.
ID was first developed in the mid-1970s by decision analysts with an intuitive semantic that is easy to understand. It is now adopted widely and becoming an alternative to the decision tree which typically suffers from exponential growth in number of branches with each variable modeled.  ID is directly applicable in team decision analysis, since it allows incomplete sharing of information among team members to be modeled and solved explicitly.  Extensions of ID also find their use in game theory as an alternative representation of the game tree.

",Influence diagram,0,1
921,487312,"Information geometry is an interdisciplinary field that applies the techniques of differential geometry to study probability theory and statistics.   It studies statistical manifolds, which are Riemannian manifolds whose points correspond to probability distributions.",Information geometry,0,1
922,15271,"Information retrieval (IR) in computing and information science is the process of obtaining information system resources that are relevant to an information need from a collection of those resources.  Searches can be based on full-text or other content-based indexing. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.
Automated information retrieval systems are used to reduce what has been called information overload. An IR system is a software system that provides access to books, journals and other documents; stores and manages those documents. Web search engines are the most visible IR applications.",Information retrieval,0,1
923,15036,"Information security, sometimes shortened to InfoSec, is the practice of protecting information by mitigating information risks. It is part of information risk management. It typically involves preventing or reducing the probability of unauthorized/inappropriate access to data, or the unlawful use, disclosure, disruption, deletion, corruption, modification, inspection, recording, or devaluation of information. It also involves actions intended to reduce the adverse impacts of such incidents. Protected information may take any form, e.g. electronic or physical, tangible (e.g. paperwork) or intangible (e.g. knowledge). Information security's primary focus is the balanced protection of the confidentiality, integrity, and availability of data (also known as the CIA triad) while maintaining a focus on efficient policy implementation, all without hampering organization productivity. This is largely achieved through a structured risk management process that involves: 

identifying information and related assets, plus potential threats, vulnerabilities, and impacts;
evaluating the risks
deciding how to address or treat the risks i.e. to avoid, mitigate, share or accept them
where risk mitigation is required, selecting or designing appropriate security controls and implementing them
monitoring the activities, making adjustments as necessary to address any issues, changes and improvement opportunitiesTo standardize this discipline, academics and professionals collaborate to offer guidance, policies, and industry standards on password, antivirus software, firewall, encryption software, legal liability, security awareness and training, and so forth. This standardization may be further driven by a wide variety of laws and regulations that affect how data is accessed, processed, stored, transferred and destroyed. However, the implementation of any standards and guidance within an entity may have limited effect if a culture of continual improvement isn't adopted.",Information security,0,1
924,237495,"An information system (IS) is a formal, sociotechnical, organizational system designed to collect, process, store, and distribute information. From a sociotechnical perspective, information systems are composed by four components: task, people, structure (or roles), and technology. Information systems can be defined as an integration of components for collection, storage and processing of data of which the data is used to provide information, contribute to knowledge as well as digital products that facilitate decision making.A computer information system is a system that is composed of people and computers that processes or interprets information. The term is also sometimes used to simply refer to a computer system with software installed.
""Information systems"" is also an academic field study about systems with a specific reference to information and the complementary networks of computer hardware and software that people and organizations use to collect, filter, process, create and also distribute data. An emphasis is placed on an information system having a definitive boundary, users, processors, storage, inputs, outputs and the aforementioned communication networks.In many organizations, the department or unit responsible for information systems and data processing is known as ""information services"".Any specific information system aims to support operations, management and decision-making. An information system is the information and communication technology (ICT) that an organization uses, and also the way in which people interact with this technology in support of business processes.Some authors make a clear distinction between information systems, computer systems, and business processes. Information systems typically include an ICT component but are not purely concerned with ICT, focusing instead on the end-use of information technology. Information systems are also different from business processes. Information systems help to control the performance of business processes.Alter argues for advantages of viewing an information system as a special type of work system. A work system is a system in which humans or machines perform processes and activities using resources to produce specific products or services for customers. An information system is a work system whose activities are devoted to capturing, transmitting, storing, retrieving, manipulating and displaying information.As such, information systems inter-relate with data systems on the one hand and activity systems on the other. An information system is a form of communication system in which data represent and are processed as a form of social memory. An information system can also be considered a semi-formal language which supports human decision making and action.
Information systems are the primary focus of study for organizational informatics.",Information system,0,1
926,30876141,"Securitization is the financial practice of pooling various types of contractual debt such as residential mortgages, commercial mortgages, auto loans or credit card debt obligations (or other non-debt assets which generate receivables) and selling their related cash flows to third party investors as securities, which may be described as bonds, pass-through securities, or collateralized debt obligations (CDOs). Investors are repaid from the principal and interest cash flows collected from the underlying debt and redistributed through the capital structure of the new financing. Securities backed by mortgage receivables are called mortgage-backed securities (MBS), while those backed by other types of receivables are asset-backed securities (ABS).
The granularity of pools of securitized assets can mitigate the credit risk of individual borrowers. Unlike general corporate debt, the credit quality of securitized debt is non-stationary due to changes in volatility that are time- and structure-dependent. If the transaction is properly structured and the pool performs as expected, the credit risk of all tranches of structured debt improves; if improperly structured, the affected tranches may experience dramatic credit deterioration and loss.Securitization has evolved from its beginnings in the late 18th century to an estimated outstanding of $10.24 trillion in the United States and $2.25 trillion in Europe as of the 2nd quarter of 2008. In 2007, ABS issuance amounted to $3.455 trillion in the US and $652 billion in Europe. WBS (Whole Business Securitization) arrangements first appeared in the United Kingdom in the 1990s, and became common in various Commonwealth legal systems where senior creditors of an insolvent business effectively gain the right to control the company.",Securitization,0,1
927,15150,"An integrated circuit or monolithic integrated circuit (also referred to as an IC, a chip, or a microchip) is a set of electronic circuits on one small flat piece (or ""chip"") of semiconductor material, usually silicon. Large numbers of tiny MOSFETs (metal–oxide–semiconductor field-effect transistors) integrate into a small chip. This results in circuits that are orders of magnitude smaller, faster, and less expensive than those constructed of discrete electronic components. The IC's mass production capability, reliability, and building-block approach to integrated circuit design has ensured the rapid adoption of standardized ICs in place of designs using discrete transistors. ICs are now used in virtually all electronic equipment and have revolutionized the world of electronics. Computers, mobile phones and other home appliances are now inextricable parts of the structure of modern societies, made possible by the small size and low cost of ICs such as modern computer processors and microcontrollers.
Very-large-scale integration was made practical by technological advancements in metal–oxide–silicon (MOS) semiconductor device fabrication. Since their origins in the 1960s, the size, speed, and capacity of chips have progressed enormously, driven by technical advances that fit more and more MOS transistors on chips of the same size – a modern chip may have many billions of MOS transistors in an area the size of a human fingernail. These advances, roughly following Moore's law, make the computer chips of today possess millions of times the capacity and thousands of times the speed of the computer chips of the early 1970s.
ICs have two main advantages over discrete circuits: cost and performance. The cost is low because the chips, with all their components, are printed as a unit by photolithography rather than being constructed one transistor at a time. Furthermore, packaged ICs use much less material than discrete circuits. Performance is high because the IC's components switch quickly and consume comparatively little power because of their small size and proximity. The main disadvantage of ICs is the high cost of designing them and fabricating the required photomasks. This high initial cost means ICs are only commercially viable when high production volumes are anticipated.",Integrated circuit,0,1
928,15305,"An integrated development environment (IDE) is a software application that provides comprehensive facilities to computer programmers for software development. An IDE normally consists of at least a source code editor, build automation tools and a debugger. Some IDEs, such as NetBeans and Eclipse, contain the necessary compiler, interpreter, or both; others, such as SharpDevelop and Lazarus, do not.
The boundary between an IDE and other parts of the broader software development environment is not well-defined; sometimes a version control system or various tools to simplify the construction of a graphical user interface (GUI) are integrated. Many modern IDEs also have a class browser, an object browser, and a class hierarchy diagram for use in object-oriented software development.",Integrated development environment,0,1
929,527943,"Interaction design, often abbreviated as IxD, is ""the practice of designing interactive digital products, environments, systems, and services."": xxvii, 30  Beyond the digital aspect, interaction design is also useful when creating physical (non-digital) products, exploring how a user might interact with it. Common topics of interaction design include design, human–computer interaction, and software development. While interaction design has an interest in form (similar to other design fields), its main area of focus rests on behavior.: xxvii, 30  Rather than analyzing how things are, interaction design synthesizes and imagines things as they could be. This element of interaction design is what characterizes IxD as a design field as opposed to a science or engineering field.While disciplines such as software engineering have a heavy focus on designing for technical stakeholders, interaction design is focused on meeting the needs and optimizing the experience of users, within relevant technical or business constraints.: xviii ",Interaction design,0,1
930,39758073,The International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics (CIBB) is a yearly scientific conference focused on machine learning and computational intelligence applied to bioinformatics and biostatistics.,International Conference on Computational Intelligence Methods for Bioinformatics and Biostatistics,0,1
931,27855504,"IROS, the IEEE/RSJ International Conference on Intelligent Robots and Systems, is an annual academic conference covering advances in robotics. It is one of the premier conferences of its field (alongside ICRA, International Conference on Robotics and Automation) with an 'A' rating from the Australian Ranking of ICT Conferences obtained in 2010 and an 'A1' rating from the Brazilian ministry of education in 2012.The acceptance rate can vary substantially (for example, it has been 32% in 2011 and 39% in 2012) and IROS typically receives more than 2000 paper submissions (for example, 790 out of 2459 submitted papers have been accepted for IROS 2011).",International Conference on Intelligent Robots and Systems,0,1
932,59466481,"The International Conference on Learning Representations (ICLR)  is a machine learning conference held every spring. The conference includes invited talks as well as oral and poster presentations of refereed papers. Since its inception in 2013, ICLR has employed an open peer review process to referee paper submissions (based on models proposed by Yann LeCun). In 2019, there were 1591 paper submissions, of which 500 accepted with poster presentations (31%) and 24 with oral presentations (1.5%).. In 2021, there were 2997 paper submissions, of which 860 were accepted (29%)..
Along with ICML and NeurIPS, ICLR is one of the three major machine learning and artificial intelligence conferences, and has the highest impact of the three.

",International Conference on Learning Representations,0,1
933,18586449,"The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning. Along with NeurIPS and ICLR, it is one of the three primary conferences of high impact in machine learning and artificial intelligence research. It is supported by the International Machine Learning Society (IMLS). Precise dates vary from year to year, but paper submissions are generally due at the end of January, and the conference is generally held during the following July. The first ICML was held 1980 in Pittsburgh.

",International Conference on Machine Learning,0,1
934,805626,"Internet fraud is a type of cybercrime fraud or deception which makes use of the Internet and could involve hiding of information or providing incorrect information for the purpose of tricking victims out of money, property, and inheritance. Internet fraud is not considered a single, distinctive crime but covers a range of illegal and illicit actions that are committed in cyberspace. It is, however, differentiated from theft since, in this case, the victim voluntarily and knowingly provides the information, money or property to the perpetrator. It is also distinguished by the way it involves temporally and spatially separated offenders.According to the FBI's 2017 Internet Crime Report, the Internet Crime Complaint Center (IC3) received about 300,000 complaints. Victims lost over $1.4 billion in online fraud in 2017. According to a study conducted by the Center for Strategic and International Studies (CSIS) and McAfee, cybercrime costs the global economy as much as $600 billion, which translates into 0.8% of total global GDP. Online fraud appears in many forms. It ranges from email spam to online scams. Internet fraud can occur even if partly based on the use of Internet services and is mostly or completely based on the use of the Internet.",Internet fraud,0,1
935,59868,"In computer science, an interpreter is a computer program that directly executes instructions written in a programming or scripting language, without requiring them previously to have been compiled into a machine language program. An interpreter generally uses one of the following strategies for program execution:

Parse the source code and perform its behavior directly;
Translate source code into some efficient intermediate representation or object code and immediately execute that;
Explicitly execute stored precompiled bytecode made by a compiler and matched with the interpreter Virtual Machine.Early versions of Lisp programming language and minicomputer and microcomputer BASIC dialects would be examples of the first type. Perl, Raku, Python, MATLAB, and Ruby are examples of the second, while UCSD Pascal is an example of the third type. Source programs are compiled ahead of time and stored as machine independent code, which is then linked at run-time and executed by an interpreter and/or compiler (for JIT systems). Some systems, such as Smalltalk and contemporary versions of BASIC and Java may also combine two and three. Interpreters of various types have also been constructed for many languages traditionally associated with compilation, such as Algol, Fortran, Cobol, C and C++.
While interpretation and compilation are the two main means by which programming languages are implemented, they are not mutually exclusive, as most interpreting systems also perform some translation work, just like compilers. The terms ""interpreted language"" or ""compiled language"" signify that the canonical implementation of that language is an interpreter or a compiler, respectively. A high-level language is ideally an abstraction independent of particular implementations.

",Interpreter (computing),0,1
936,113021,"An intrusion detection system (IDS; also intrusion prevention system or IPS) is a device or software application that monitors a network or systems for malicious activity or policy violations. Any intrusion activity or violation is typically reported either to an administrator or collected centrally using a security information and event management (SIEM) system. A SIEM system combines outputs from multiple sources and uses alarm filtering techniques to distinguish malicious activity from false alarms.IDS types range in scope from single computers to large networks. The most common classifications are network intrusion detection systems (NIDS) and host-based intrusion detection systems (HIDS). A system that monitors important operating system files is an example of an HIDS, while a system that analyzes incoming network traffic is an example of an NIDS. It is also possible to classify IDS by detection approach. The most well-known variants are signature-based detection (recognizing bad patterns, such as malware) and anomaly-based detection (detecting deviations from a model of ""good"" traffic, which often relies on machine learning). Another common variant is reputation-based detection (recognizing the potential threat according to the reputation scores). Some IDS products have the ability to respond to detected intrusions. Systems with response capabilities are typically referred to as an intrusion prevention system. Intrusion detection systems can also serve specific purposes by augmenting them with custom tools, such as using a honeypot to attract and characterize malicious traffic.

",Intrusion detection system,1,1
938,40671069,"Jerome Harold Friedman (born December 29, 1939) is an American statistician, consultant and Professor of Statistics at Stanford University, known for his contributions in the field of statistics and data mining.",Jerome H. Friedman,0,1
939,649572,"John Joseph Hopfield (born July 15, 1933) is an American scientist most widely known for his invention of an associative neural network in 1982. It is now more commonly known as the Hopfield network.",John Hopfield,0,1
940,5721283,"The Journal of Machine Learning Research is a peer-reviewed open access scientific journal covering machine learning. It was established in 2000 and the first editor-in-chief was Leslie Kaelbling. The current editors-in-chief are Francis Bach (Inria), David Blei (Columbia University) and Bernhard Schölkopf (Max Planck Institute for Intelligent Systems).",Journal of Machine Learning Research,0,1
941,38455554,"Julia is a high-level, high-performance, dynamic programming language. While it is a general-purpose language and can be used to write any application, many of its features are well suited for numerical analysis and computational science.Distinctive aspects of Julia's design include a type system with parametric polymorphism in a dynamic programming language; with multiple dispatch as its core programming paradigm. Julia supports concurrent, (composable) parallel and distributed computing (with or without using MPI or the built-in corresponding to ""OpenMP-style"" threads), and direct calling of C and Fortran libraries without glue code. Julia uses a just-in-time (JIT) compiler that is referred to as ""just-ahead-of-time"" (JAOT) in the Julia community, as Julia compiles all code (by default) to machine code before running it.Julia is garbage-collected, uses eager evaluation, and includes efficient libraries for floating-point calculations, linear algebra, random number generation, and regular expression matching. Many libraries are available, including some (e.g., for fast Fourier transforms) that were previously bundled with Julia and are now separate.Several development tools support coding in Julia, such as integrated development environments (e.g. for Microsoft's Visual Studio Code, an extension is available providing debugging and linting support); with integrated tools, e.g. a profiler (and flame graph support available for the built-in one), debugger, and the Rebugger.jl package ""supports repeated-execution debugging"" and more.

",Julia (programming language),0,1
942,58568429,"Julia Angwin is a Pulitzer Prize-winning American investigative journalist, New York Times bestselling author, and entrepreneur. She is co-founder and editor-in-chief of The Markup, a nonprofit newsroom that investigates the impact of technology on society. She was a senior reporter at ProPublica from 2014 to April 2018 and staff reporter at the New York bureau of The Wall Street Journal from 2000 to 2013. Angwin is author of non-fiction books, Stealing MySpace: The Battle to Control the Most Popular Website in America (2009) and Dragnet Nation (2014). She is a winner and two-time finalist for the Pulitzer Prize in journalism.",Julia Angwin,0,1
943,39799215,"In applied mathematics, K-SVD is a dictionary learning algorithm for creating a dictionary for sparse representations, via a singular value decomposition approach. K-SVD is a generalization of the k-means clustering method, and it works by iteratively alternating between sparse coding the input data based on the current dictionary, and updating the atoms in the dictionary to better fit the data. It is structurally related to the expectation maximization (EM) algorithm. K-SVD can be found widely in use in applications such as image processing, audio processing, biology, and document analysis.",K-SVD,0,1
944,1860407,"k-means clustering is a method of vector quantization, originally from signal processing, that aims to partition n observations into k clusters in which each observation belongs to the cluster with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster. This results in a partitioning of the data space into Voronoi cells. k-means clustering minimizes within-cluster variances (squared Euclidean distances), but not regular Euclidean distances, which would be the more difficult Weber problem: the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. For instance, better Euclidean solutions can be found using k-medians and k-medoids.
The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. They both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the Gaussian mixture model allows clusters to have different shapes.
The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, a popular supervised machine learning technique for classification that is often confused with k-means due to the name. Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. This is known as nearest centroid classifier or Rocchio algorithm.",K-means clustering,0,1
945,1775388,"In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. The output depends on whether k-NN is used for classification or regression:

In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. Since this algorithm relies on distance for classification, if the features represent different physical units or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, so that the nearer neighbors contribute more to the average than the more distant ones. For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. This can be thought of as the training set for the algorithm, though no explicit training step is required.
A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data.

",K-nearest neighbors algorithm,0,1
946,24960689,"KNIME (), the Konstanz Information Miner, is a free and open-source data analytics, reporting and integration platform. KNIME integrates various components for machine learning and data mining through its modular data pipelining ""Building Blocks of Analytics"" concept. A graphical user interface and use of JDBC allows assembly of nodes blending different data sources, including preprocessing (ETL: Extraction, Transformation, Loading), for modeling, data analysis and visualization without, or with only minimal, programming.
Since 2006, KNIME has been used in pharmaceutical research, it also used in other areas such as CRM customer data analysis, business intelligence, text mining and financial data analysis. Recently attempts were made to use KNIME as robotic process automation (RPA) tool.KNIME's headquarters are based in Zurich, with additional offices in Konstanz, Berlin, and Austin (USA).

",KNIME,0,1
947,12185719,"KXEN was an American software company which existed from 1998 to 2013 when it was acquired by SAP AG.

",KXEN Inc.,0,1
948,4349459,"Kerala (English:  KERR-ə-lə; Malayalam: [ke:ɾɐɭɐm] (listen)) is a state on the Malabar Coast of India. It was formed on 1 November 1956, following the passage of the States Reorganisation Act, by combining Malayalam-speaking regions of the erstwhile regions of Cochin, Malabar, South Canara, and Travancore. Spread over 38,863 km2 (15,005 sq mi), Kerala is the 21st largest Indian state by area. It is bordered by Karnataka to the north and northeast, Tamil Nadu to the east and south, and the Lakshadweep Sea to the west. With 33 million inhabitants as per the 2011 census, Kerala is the 13th-largest Indian state by population. It is divided into 14 districts with the capital being Thiruvananthapuram. Malayalam is the most widely spoken language and is also the official language of the state.The Chera Dynasty was the first prominent kingdom based in Kerala. The Ay kingdom in the deep south and the Ezhimala kingdom in the north formed the other kingdoms in the early years of the Common Era (CE). The region had been a prominent spice exporter since 3000 BCE. The region's prominence in trade was noted in the works of Pliny as well as the Periplus around 100 CE. In the 15th century, the spice trade attracted Portuguese traders to Kerala, and paved the way for European colonisation of India. At the time of Indian independence movement in the early 20th century, there were two major princely states in Kerala: Travancore and Cochin. They united to form the state of Thiru-Kochi in 1949. The Malabar region, in the northern part of Kerala, had been a part of the Madras province of British India, which later became a part of the Madras State post-independence. After the States Reorganisation Act, 1956, the modern-day state of Kerala was formed by merging the Malabar district of Madras State (excluding Gudalur taluk of Nilgiris district, Lakshadweep Islands, Topslip, the Attappadi Forest east of Anakatti), the taluk of Kasaragod (now Kasaragod District) in South Canara, and the erstwhile state of Thiru-Kochi (excluding four southern taluks of Kanyakumari district, and Shenkottai taluks).Kerala has the lowest positive population growth rate in India, 3.44%; the highest Human Development Index (HDI), 0.784 in 2018 (0.712 in 2015); the highest literacy rate, 96.2% in the 2018 literacy survey conducted by the National Statistical Office, India; the highest life expectancy, 77 years; and the highest sex ratio, 1,084 women per 1,000 men. Kerala is the second-least impoverished state in India according to the Annual Report of Reserve Bank of India published in 2013. Kerala is the second-most urbanised major state in the country with 47.7% urban population according to the 2011 Census of India. The state topped in the country to achieve the Sustainable Development Goals according to the annual report of NITI Aayog published in 2019. The state has the highest media exposure in India with newspapers publishing in nine languages, mainly English and Malayalam. Hinduism is practised by more than half of the population, followed by Islam and Christianity. The culture is a synthesis of Aryan, Dravidian, Arab, and European cultures, developed over millennia, under influences from other parts of India and abroad.
The economy of Kerala is the 8th-largest in India with ₹8.55 trillion (US$110 billion) in gross state domestic product (GSDP) and a per capita net state domestic product of ₹222,000 (US$2,900). The tertiary sector contributes around 65% to state's GSVA, while the primary sector contributes only 8%. The state has witnessed significant emigration, especially to the Arab states of the Persian Gulf during the Gulf Boom of the 1970s and early 1980s, and its economy depends significantly on remittances from a large Malayali expatriate community. The production of pepper and natural rubber contributes significantly to the total national output. In the agricultural sector, coconut, tea, coffee, cashew and spices are important. The state is situated between Arabian Sea to the west and Western Ghats mountain ranges to the east. The state's coastline extends for 595 kilometres (370 mi), and around 1.1 million people in the state are dependent on the fishery industry which contributes 3% to the state's income. Kerala is one of the prominent tourist destinations of India, with coconut-lined sandy beaches, backwaters, hill stations, Ayurvedic tourism and tropical greenery as its major attractions.",Kerala,0,1
949,3424576,"In machine learning, kernel machines are a class of algorithms for pattern analysis, whose best known member is the support-vector machine (SVM). The general task of  pattern analysis is to find and study general types of relations (for example clusters, rankings, principal components, correlations, classifications) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed into feature vector representations via a user-specified feature map: in contrast, kernel methods require only a user-specified kernel, i.e., a similarity function over pairs of data points in raw representation.
Kernel methods  owe their name to the use of kernel functions, which enable them to operate in a high-dimensional, implicit feature space without ever computing the coordinates of the data in that space, but rather by simply computing the inner products between the images of all pairs of data in the feature space.  This operation is often computationally cheaper than the explicit computation of the coordinates.  This approach is called the ""kernel trick"". Kernel functions have been introduced for sequence data, graphs, text, images, as well as vectors.
Algorithms capable of operating with kernels include the kernel perceptron, support-vector machines (SVM), Gaussian processes, principal components analysis (PCA), canonical correlation analysis, ridge regression, spectral clustering, linear adaptive filters and many others.
Most kernel algorithms are based on convex optimization or eigenproblems and are statistically well-founded. Typically, their statistical properties are analyzed using statistical learning theory (for example, using Rademacher complexity).

",Kernel method,1,1
950,8771825,"In statistics, Kernel regression is a non-parametric technique to estimate the conditional expectation of a random variable. The objective is to find a non-linear relation between a pair of random variables X and Y.
In any nonparametric regression, the conditional expectation of a variable 
  
    
      
        Y
      
    
    {\displaystyle Y}
   relative to a variable 
  
    
      
        X
      
    
    {\displaystyle X}
   may be written:

  
    
      
        E
        ⁡
        (
        Y
        
          |
        
        X
        )
        =
        m
        (
        X
        )
      
    
    {\displaystyle \operatorname {E} (Y|X)=m(X)}
  
where 
  
    
      
        m
      
    
    {\displaystyle m}
   is an unknown function.

",Kernel regression,0,1
952,31002435,"Knowledge extraction is the creation of knowledge from structured (relational databases, XML) and unstructured (text, documents, images) sources. The resulting knowledge needs to be in a machine-readable and machine-interpretable format and must represent knowledge in a manner that facilitates inferencing. Although it is methodically similar to information extraction (NLP) and ETL (data warehouse), the main criterion is that the extraction result goes beyond the creation of structured information or the transformation into a relational schema. It requires either the reuse of existing formal knowledge (reusing identifiers or ontologies) or the generation of a schema based on the source data.
The RDB2RDF W3C group  is currently standardizing a language for extraction of resource description frameworks (RDF) from relational databases. Another popular example for knowledge extraction is the transformation of Wikipedia into structured data and also the mapping to existing knowledge (see DBpedia and Freebase).

",Knowledge extraction,0,1
953,67944487,"In representation learning, knowledge graph embedding (KGE), also referred to as knowledge representation learning (KRL), or multi-relation learning, is a machine learning task of learning a low-dimensional representation of a knowledge graph's entities and relations while preserving their semantic meaning.  Leveraging their embedded representation, knowledge graphs (KGs) can be used for various applications such as link prediction, triple classification, entity recognition, clustering, and relation extraction.",Knowledge graph embedding,0,1
954,16920,"Knowledge representation and reasoning (KRR, KR&R, KR²) is the field of artificial intelligence (AI) dedicated to representing information about the world in a form that a computer system can use to solve complex tasks such as diagnosing a medical condition or having a dialog in a natural language. Knowledge representation incorporates findings from psychology about how humans solve problems and represent knowledge in order to design formalisms that will make complex systems easier to design and build.  Knowledge representation and reasoning also incorporates findings from logic to automate various kinds of reasoning, such as the application of rules or the relations of sets and subsets.
Examples of knowledge representation formalisms include semantic nets, systems architecture, frames, rules, and ontologies. Examples of automated reasoning engines include inference engines, theorem provers, and classifiers.

",Knowledge representation and reasoning,0,1
955,34913689,"LIONsolver is an integrated software for data mining, business intelligence, analytics, and modeling and reactive business intelligence approach. A non-profit version is also available as LIONoso.
LIONsolver is used to build models, visualize them, and improve business and engineering processes. 
It is a tool for decision making based on data and quantitative model and it can be connected to most databases and external programs. 
The software is fully integrated with the Grapheur business intelligence  and intended for more advanced users.",LIONsolver,0,1
956,17851,"Lambda (; uppercase Λ, lowercase λ; Greek: λάμ(β)δα, lám(b)da) is the 11th letter of the Greek alphabet, representing the sound /l/. In the system of Greek numerals, lambda has a value of 30. Lambda is derived from the Phoenician Lamed . Lambda gave rise to the Latin L and the Cyrillic El (Л). The ancient grammarians and dramatists give evidence to the pronunciation as [laːbdaː] (λάβδα) in Classical Greek times. In Modern Greek, the name of the letter, Λάμδα, is pronounced [ˈlam.ða].
In early Greek alphabets, the shape and orientation of lambda varied. Most variants consisted of two straight strokes, one longer than the other, connected at their ends. The angle might be in the upper-left, lower-left (""Western"" alphabets) or top (""Eastern"" alphabets). Other variants had a vertical line with a horizontal or sloped stroke running to the right. With the general adoption of the Ionic alphabet, Greek settled on an angle at the top; the Romans put the angle at the lower-left.
The HTML 4 character entity references for the Greek capital and small letter lambda are &#923; and &#955; respectively. The Unicode code points for lambda are U+039B and U+03BB.",Lambda,0,1
957,30806,"In computer science, a tree is a widely used abstract data type that represents a hierarchical tree structure with a set of connected nodes. Each node in the tree can be connected to many children (depending on the type of tree), but must be connected to exactly one parent, except for the root node, which has no parent. These constraints mean there are no cycles or ""loops"" (no node can be its own ancestor), and also that each child can be treated like the root node of its own subtree, making recursion a useful technique for tree traversal. In contrast to linear data structures, many trees cannot be represented by relationships between neighboring nodes in a single straight line.
Binary trees are a commonly used type, which constrain the number of children for each parent to exactly two. When the order of the children is specified, this data structure corresponds to an  ordered tree in graph theory. A value or pointer to other data may be associated with every node in the tree, or sometimes only with the leaf nodes, which have no children.
The abstract data type can be represented in a number of ways, including a list of parents with pointers to children, a list of children with pointers to parents, or a list of nodes and a separate list of parent-child relations (a specific type of adjacency list). Representations might also be more complicated, for example using indexes or ancestor lists for performance.
Trees as used in computing are similar to but can be different from mathematical constructs of trees in graph theory, trees in set theory, and trees in descriptive set theory.

",Tree (data structure),0,1
958,854461,"Learning classifier systems, or LCS, are a paradigm of rule-based machine learning methods that combine a discovery component (e.g. typically a genetic algorithm) with a learning component (performing either supervised learning, reinforcement learning, or unsupervised learning).  Learning classifier systems seek to identify a set of context-dependent rules that collectively store and apply knowledge in a piecewise manner in order to make predictions (e.g. behavior modeling, classification, data mining, regression, function approximation, or game strategy).  This approach allows complex solution spaces to be broken up into smaller, simpler parts.
The founding concepts behind learning classifier systems came from attempts to model complex adaptive systems, using rule-based agents to form an artificial cognitive system (i.e. artificial intelligence).",Learning classifier system,0,1
959,59968610,"In machine learning, a learning curve (or training curve) plots the optimal value of a model's loss function for a training set against this loss function evaluated on a validation data set with same parameters as produced the optimal function. It is a tool to find out how much a machine model benefits from adding more training data and whether the estimator suffers more from a variance error or a bias error. If both the validation score and the training score converge to a value that is too low with increasing size of the training set, it will not benefit much from more training data.The machine learning curve is useful for many purposes including comparing different algorithms,  choosing model parameters during design, adjusting optimization to improve convergence, and determining the amount of data used for training.In the machine learning domain, there are two implications of learning curves differing in the x-axis of the curves, with experience of the model graphed either as the number of training examples used for learning or the number of iterations used in training the model.",Learning curve (machine learning),0,1
960,25050663,"Learning to rank or machine-learned ranking (MLR) is the application of machine learning, typically supervised, semi-supervised or reinforcement learning, in the construction of ranking models for information retrieval systems. Training data consists of lists of items with some partial order specified between items in each list. This order is typically induced by giving a numerical or ordinal score or a binary judgment (e.g. ""relevant"" or ""not relevant"") for each item. The goal of constructing the ranking model is to rank new, unseen lists in a similar way to rankings in the training data.

",Learning to rank,0,1
961,4909283,"Leo Breiman (January 27, 1928 – July 5, 2005) was a distinguished statistician at the University of California, Berkeley. He was the recipient of numerous honors and awards, and was a member of the United States National Academy of Sciences.
Breiman's work helped to bridge the gap between statistics and computer science, particularly in the field of machine learning. His most important contributions were his work on classification and regression trees and ensembles of trees fit to bootstrap samples. Bootstrap aggregation was given the name bagging by Breiman. Another of Breiman's ensemble approaches is the random forest.",Leo Breiman,0,1
962,106421,"In computer science, a library is a collection of non-volatile resources used by computer programs, often for software development. These may include configuration data, documentation, help data, message templates, pre-written code and subroutines, classes, values or type specifications. In IBM's OS/360 and its successors they are referred to as partitioned data sets.A library is also a collection of implementations of behavior, written in terms of a language, that has a well-defined interface by which the behavior is invoked. For instance, people who want to write a higher-level program can use a library to make system calls instead of implementing those system calls over and over again. In addition, the behavior is provided for reuse by multiple independent programs. A program invokes the library-provided behavior via a mechanism of the language. For example, in a simple imperative language such as C, the behavior in a library is invoked by using C's normal function-call. What distinguishes the call as being to a library function, versus being to another function in the same program, is the way that the code is organized in the system.Library code is organized in such a way that it can be used by multiple programs that have no connection to each other, while code that is part of a program is organized to be used only within that one program. This distinction can gain a hierarchical notion when a program grows large, such as a multi-million-line program. In that case, there may be internal libraries that are reused by independent sub-portions of the large program. The distinguishing feature is that a library is organized for the purposes of being reused by independent programs or sub-programs, and the user only needs to know the interface and not the internal details of the library.
The value of a library lies in the reuse of standardized program elements. When a program invokes a library, it gains the behavior implemented inside that library without having to implement that behavior itself. Libraries encourage the sharing of code in a modular fashion and ease the distribution of the code.
The behavior implemented by a library can be connected to the invoking program at different program lifecycle phases. If the code of the library is accessed during the build of the invoking program, then the library is called a static library. An alternative is to build the executable of the invoking program and distribute that, independently of the library implementation. The library behavior is connected after the executable has been invoked to be executed, either as part of the process of starting the execution, or in the middle of execution. In this case the library is called a dynamic library (loaded at runtime). A dynamic library can be loaded and linked when preparing a program for execution, by the linker. Alternatively, in the middle of execution, an application may explicitly request that a module be loaded.
Most compiled languages have a standard library, although programmers can also create their own custom libraries. Most modern software systems provide libraries that implement the majority of the system services. Such libraries have organized the services which a modern application requires. As such, most code used by modern applications is provided in these system libraries.

",Library (computing),0,1
963,64785522,"LightGBM, short for Light Gradient Boosting Machine, is a free and open source distributed gradient boosting framework for machine learning originally developed by Microsoft. It is based on decision tree algorithms and used for ranking, classification and other machine learning tasks. The development focus is on performance and scalability.",LightGBM,0,1
964,98974,"In the field of machine learning, the goal of statistical classification is to use an object's characteristics to identify which class (or group) it belongs to.  A linear classifier achieves this by making a classification decision based on the value of a linear combination of the characteristics.  An object's characteristics are also known as feature values and are typically presented to the machine in a vector called a feature vector. Such classifiers work well for practical problems such as document classification, and more generally for problems with many variables (features), reaching accuracy levels comparable to non-linear classifiers while taking less time to train and use.

",Linear classifier,0,1
967,2142,"The following is a list of current and past, non-classified notable artificial intelligence projects.

",List of artificial intelligence projects,0,1
968,49082762,"These datasets are applied for machine learning research and have been cited in peer-reviewed academic journals. Datasets are an integral part of the field of machine learning. Major advances in this field can result from advances in learning algorithms (such as deep learning), computer hardware, and, less-intuitively, the availability of high-quality training datasets. High-quality labeled training datasets for supervised and semi-supervised machine learning algorithms are usually difficult and expensive to produce because of the large amount of time needed to label the data. Although they do not need to be labeled, high-quality datasets for unsupervised learning can also be difficult and costly to produce.",List of datasets for machine-learning research,0,1
969,454351,"This is a list of important publications in computer science, organized by field. Some reasons why a particular publication might be regarded as important:

Topic creator – A publication that created a new topic
Breakthrough – A publication that changed scientific knowledge significantly
Influence – A publication which has significantly influenced the world or has had a massive impact on the teaching of computer science.",List of important publications in computer science,0,1
970,23886619,"Artificial intelligence researchers have developed several specialized programming languages for artificial intelligence:

",List of programming languages for artificial intelligence,0,1
971,27321681,"In anomaly detection, the local outlier factor (LOF) is an algorithm proposed by Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng and Jörg Sander in 2000 for finding anomalous data points by measuring the local deviation of a given data point with respect to its neighbours.LOF shares some concepts with DBSCAN and OPTICS such as the concepts of ""core distance"" and ""reachability distance"", which are used for local density estimation.",Local outlier factor,0,1
972,1151991,"Logic in computer science covers the overlap between the field of logic and that of computer science. The topic can essentially be divided into three main areas:

Theoretical foundations and analysis
Use of computer technology to aid logicians
Use of concepts from logic for computer applications

",Logic in computer science,0,1
973,17927,"Logic programming is a programming paradigm which is largely based on formal logic. Any program written in a logic programming language is a set of sentences in logical form, expressing facts and rules about some problem domain.  Major logic programming language families include Prolog, answer set programming (ASP) and Datalog. In all of these languages, rules are written in the form of clauses:

H :- B1, …, Bn.and are read declaratively as logical implications:

H if B1 and … and Bn.H is called the head of the rule and B1, ..., Bn is called the body. Facts are rules that have no body, and are written in the simplified form:

H.In the simplest case in which H, B1, ..., Bn are all atomic formulae, these clauses are called definite clauses or Horn clauses. However, there are many extensions of this simple case, the most important one being the case in which conditions in the body of a clause can also be negations of atomic formulas. Logic programming languages that include this extension have the knowledge representation capabilities of a non-monotonic logic.
In ASP and Datalog, logic programs have only a declarative reading, and their execution is performed by means of a proof procedure or model generator whose behaviour is not meant to be controlled by the programmer. However, in the Prolog family of languages, logic programs also have a procedural interpretation as goal-reduction procedures:

to solve H, solve B1, and ... and solve Bn.Consider the following clause as an example:

fallible(X) :- human(X).based on an example used by Terry Winograd to illustrate the programming language Planner. As a clause in a logic program, it can be used both as a procedure to test whether X is fallible by testing whether X is human, and as a procedure to find an X which is fallible by finding an X which is human. Even facts have a procedural interpretation. For example, the clause:

human(socrates).can be used both as a procedure to show that socrates is human, and as a procedure to find an X that is human by ""assigning"" socrates to X.
The declarative reading of logic programs can be used by a programmer to verify their correctness. Moreover, logic-based program transformation techniques can also be used to transform logic programs into logically equivalent programs that are more efficient. In the Prolog family of logic programming languages, the programmer can also use the known problem-solving behaviour of the execution mechanism to improve the efficiency of programs.",Logic programming,0,1
974,18152,"In logic, mathematics and linguistics, And (
  
    
      
        ∧
      
    
    {\displaystyle \wedge }
  ) is the truth-functional operator of logical conjunction; the and of a set of operands is true if and only if all of its operands are true. The logical connective that represents this operator is typically written as 
  
    
      
        ∧
      
    
    {\displaystyle \wedge }
   or  ⋅ .
  
    
      
        A
        ∧
        B
      
    
    {\displaystyle A\land B}
   is true if and only if 
  
    
      
        A
      
    
    {\displaystyle A}
   is true and 
  
    
      
        B
      
    
    {\displaystyle B}
   is true.
An operand of a conjunction is a conjunct.
Beyond logic, the term ""conjunction"" also refers to similar concepts in other fields:

In natural language, the denotation of expressions such as English ""and"".
In programming languages, the short-circuit and control structure.
In set theory, intersection.
In lattice theory, logical conjunction (greatest lower bound).
In predicate logic, universal quantification.",Logical conjunction,0,1
976,10711453,"Long short-term memory (LSTM) is an artificial neural network  used in the fields of artificial intelligence and deep learning. Unlike standard feedforward neural networks, LSTM has feedback connections. Such a recurrent neural network can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected handwriting recognition, speech recognition,  machine translation, robot control, video games, and healthcare. LSTM has become the most cited neural network of the 20th century.A common LSTM unit is composed of a cell, an input gate, an output gate and a forget gate. The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell.
LSTM networks are well-suited to classifying, processing and making predictions based on time series data, since there can be lags of unknown duration between important events in a time series. LSTMs were developed to deal with the vanishing gradient problem that can be encountered when training traditional RNNs. Relative insensitivity to gap length is an advantage of LSTM over RNNs, hidden Markov models and other sequence learning methods in numerous applications.

",Long short-term memory,0,1
978,44578554,"In machine learning and mathematical optimization, loss functions for classification are computationally feasible loss functions representing the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to).  Given 
  
    
      
        
          
            X
          
        
      
    
    {\displaystyle {\mathcal {X}}}
   as the space of all possible inputs (usually 
  
    
      
        
          
            X
          
        
        ⊂
        
          
            R
          
          
            d
          
        
      
    
    {\displaystyle {\mathcal {X}}\subset \mathbb {R} ^{d}}
  ), and 
  
    
      
        
          
            Y
          
        
        =
        {
        −
        1
        ,
        1
        }
      
    
    {\displaystyle {\mathcal {Y}}=\{-1,1\}}
   as the set of labels (possible outputs), a typical goal of classification algorithms is to find a function 
  
    
      
        f
        :
        
          
            X
          
        
        →
        
          R
        
      
    
    {\displaystyle f:{\mathcal {X}}\to \mathbb {R} }
   which best predicts a label 
  
    
      
        y
      
    
    {\displaystyle y}
   for a given input 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
  .  However, because of incomplete information, noise in the measurement, or probabilistic components in the underlying process, it is possible for the same 
  
    
      
        
          
            
              x
              →
            
          
        
      
    
    {\displaystyle {\vec {x}}}
   to generate different 
  
    
      
        y
      
    
    {\displaystyle y}
  .  As a result, the goal of the learning problem is to minimize expected loss (also known as the risk), defined as

  
    
      
        I
        [
        f
        ]
        =
        
          
            ∫
            
              
                
                  X
                
              
              ×
              
                
                  Y
                
              
            
          
          V
          (
          f
          (
          
            
              
                x
                →
              
            
          
          )
          ,
          y
          )
          p
          (
          
            
              
                x
                →
              
            
          
          ,
          y
          )
          
          d
          
            
              
                x
                →
              
            
          
          
          d
          y
        
      
    
    {\displaystyle I[f]=\displaystyle \int _{{\mathcal {X}}\times {\mathcal {Y}}}V(f({\vec {x}}),y)p({\vec {x}},y)\,d{\vec {x}}\,dy}
  where 
  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
      
    
    {\displaystyle V(f({\vec {x}}),y)}
   is a given loss function, and  
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        ,
        y
        )
      
    
    {\displaystyle p({\vec {x}},y)}
   is the probability density function of the process that generated the data, which can equivalently be written as

  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        ,
        y
        )
        =
        p
        (
        y
        ∣
        
          
            
              x
              →
            
          
        
        )
        p
        (
        
          
            
              x
              →
            
          
        
        )
        .
      
    
    {\displaystyle p({\vec {x}},y)=p(y\mid {\vec {x}})p({\vec {x}}).}
  Within classification, several commonly used loss functions are written solely in terms of the product of the true label 
  
    
      
        y
      
    
    {\displaystyle y}
   and the predicted label 
  
    
      
        f
        (
        
          
            
              x
              →
            
          
        
        )
      
    
    {\displaystyle f({\vec {x}})}
  . Therefore, they can be defined as functions of only one variable 
  
    
      
        υ
        =
        y
        f
        (
        
          
            
              x
              →
            
          
        
        )
      
    
    {\displaystyle \upsilon =yf({\vec {x}})}
  , so that 
  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
        =
        ϕ
        (
        y
        f
        (
        
          
            
              x
              →
            
          
        
        )
        )
        =
        ϕ
        (
        υ
        )
      
    
    {\displaystyle V(f({\vec {x}}),y)=\phi (yf({\vec {x}}))=\phi (\upsilon )}
   with a suitably chosen function 
  
    
      
        ϕ
        :
        
          R
        
        →
        
          R
        
      
    
    {\displaystyle \phi :\mathbb {R} \to \mathbb {R} }
  . These are called margin-based loss functions. Choosing a margin-based loss function amounts to choosing 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  . Selection of a loss function within this framework impacts the optimal 
  
    
      
        
          f
          
            ϕ
          
          
            ∗
          
        
      
    
    {\displaystyle f_{\phi }^{*}}
   which minimizes the expected risk. 
In the case of binary classification, it is possible to simplify the calculation of expected risk from the integral specified above.  Specifically,

  
    
      
        
          
            
              
                I
                [
                f
                ]
              
              
                
                =
                
                  ∫
                  
                    
                      
                        X
                      
                    
                    ×
                    
                      
                        Y
                      
                    
                  
                
                V
                (
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                ,
                y
                )
                p
                (
                
                  
                    
                      x
                      →
                    
                  
                
                ,
                y
                )
                
                d
                
                  
                    
                      x
                      →
                    
                  
                
                
                d
                y
              
            
            
              
              
                
                =
                
                  ∫
                  
                    
                      X
                    
                  
                
                
                  ∫
                  
                    
                      Y
                    
                  
                
                ϕ
                (
                y
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                p
                (
                y
                ∣
                
                  
                    
                      x
                      →
                    
                  
                
                )
                p
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                
                d
                y
                
                d
                
                  
                    
                      x
                      →
                    
                  
                
              
            
            
              
              
                
                =
                
                  ∫
                  
                    
                      X
                    
                  
                
                [
                ϕ
                (
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                p
                (
                1
                ∣
                
                  
                    
                      x
                      →
                    
                  
                
                )
                +
                ϕ
                (
                −
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                p
                (
                −
                1
                ∣
                
                  
                    
                      x
                      →
                    
                  
                
                )
                ]
                p
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                
                d
                
                  
                    
                      x
                      →
                    
                  
                
              
            
            
              
              
                
                =
                
                  ∫
                  
                    
                      X
                    
                  
                
                [
                ϕ
                (
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                p
                (
                1
                ∣
                
                  
                    
                      x
                      →
                    
                  
                
                )
                +
                ϕ
                (
                −
                f
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                (
                1
                −
                p
                (
                1
                ∣
                
                  
                    
                      x
                      →
                    
                  
                
                )
                )
                ]
                p
                (
                
                  
                    
                      x
                      →
                    
                  
                
                )
                
                d
                
                  
                    
                      x
                      →
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}I[f]&=\int _{{\mathcal {X}}\times {\mathcal {Y}}}V(f({\vec {x}}),y)p({\vec {x}},y)\,d{\vec {x}}\,dy\\[6pt]&=\int _{\mathcal {X}}\int _{\mathcal {Y}}\phi (yf({\vec {x}}))p(y\mid {\vec {x}})p({\vec {x}})\,dy\,d{\vec {x}}\\[6pt]&=\int _{\mathcal {X}}[\phi (f({\vec {x}}))p(1\mid {\vec {x}})+\phi (-f({\vec {x}}))p(-1\mid {\vec {x}})]p({\vec {x}})\,d{\vec {x}}\\[6pt]&=\int _{\mathcal {X}}[\phi (f({\vec {x}}))p(1\mid {\vec {x}})+\phi (-f({\vec {x}}))(1-p(1\mid {\vec {x}}))]p({\vec {x}})\,d{\vec {x}}\end{aligned}}}
  The second equality follows from the properties described above.  The third equality follows from the fact that 1 and −1 are the only possible values for 
  
    
      
        y
      
    
    {\displaystyle y}
  , and the fourth because 
  
    
      
        p
        (
        −
        1
        ∣
        x
        )
        =
        1
        −
        p
        (
        1
        ∣
        x
        )
      
    
    {\displaystyle p(-1\mid x)=1-p(1\mid x)}
  . The term within brackets 
  
    
      
        [
        ϕ
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        )
        p
        (
        1
        ∣
        
          
            
              x
              →
            
          
        
        )
        +
        ϕ
        (
        −
        f
        (
        
          
            
              x
              →
            
          
        
        )
        )
        (
        1
        −
        p
        (
        1
        ∣
        
          
            
              x
              →
            
          
        
        )
        )
        ]
      
    
    {\displaystyle [\phi (f({\vec {x}}))p(1\mid {\vec {x}})+\phi (-f({\vec {x}}))(1-p(1\mid {\vec {x}}))]}
   is known as the conditional risk.
One can solve for the minimizer of 
  
    
      
        I
        [
        f
        ]
      
    
    {\displaystyle I[f]}
   by taking the functional derivative of the last equality with respect to 
  
    
      
        f
      
    
    {\displaystyle f}
   and setting the derivative equal to 0.  This will result in the following equation

  
    
      
        
          
            
              ∂
              ϕ
              (
              f
              )
            
            
              ∂
              f
            
          
        
        η
        +
        
          
            
              ∂
              ϕ
              (
              −
              f
              )
            
            
              ∂
              f
            
          
        
        (
        1
        −
        η
        )
        =
        0
        
        
        
        
        
        (
        1
        )
      
    
    {\displaystyle {\frac {\partial \phi (f)}{\partial f}}\eta +{\frac {\partial \phi (-f)}{\partial f}}(1-\eta )=0\;\;\;\;\;(1)}
  which is also equivalent to setting the derivative of the conditional risk equal to zero.
Given the binary nature of classification, a natural selection for a loss function (assuming equal cost for false positives and false negatives) would be the 0-1 loss function (0–1 indicator function), which takes the value of 0 if the predicted classification equals that of the true class or a 1 if the predicted classification does not match the true class. This selection is modeled by

  
    
      
        V
        (
        f
        (
        
          
            
              x
              →
            
          
        
        )
        ,
        y
        )
        =
        H
        (
        −
        y
        f
        (
        
          
            
              x
              →
            
          
        
        )
        )
      
    
    {\displaystyle V(f({\vec {x}}),y)=H(-yf({\vec {x}}))}
  where 
  
    
      
        H
      
    
    {\displaystyle H}
   indicates the Heaviside step function.
However, this loss function is non-convex and non-smooth, and solving for the optimal solution is an NP-hard combinatorial optimization problem.  As a result, it is better to substitute loss function surrogates which are tractable for commonly used learning algorithms, as they have convenient properties such as being convex and smooth.  In addition to  their computational tractability,  one can show that the solutions to the learning problem using these loss surrogates allow for the recovery of the actual solution to the original classification problem.  Some of these surrogates are described below.
In practice, the probability distribution 
  
    
      
        p
        (
        
          
            
              x
              →
            
          
        
        ,
        y
        )
      
    
    {\displaystyle p({\vec {x}},y)}
   is unknown.  Consequently, utilizing a training set of 
  
    
      
        n
      
    
    {\displaystyle n}
   independently and identically distributed sample points

  
    
      
        S
        =
        {
        (
        
          
            
              
                x
                →
              
            
          
          
            1
          
        
        ,
        
          y
          
            1
          
        
        )
        ,
        …
        ,
        (
        
          
            
              
                x
                →
              
            
          
          
            n
          
        
        ,
        
          y
          
            n
          
        
        )
        }
      
    
    {\displaystyle S=\{({\vec {x}}_{1},y_{1}),\dots ,({\vec {x}}_{n},y_{n})\}}
  drawn from the data sample space, one seeks to minimize empirical risk

  
    
      
        
          I
          
            S
          
        
        [
        f
        ]
        =
        
          
            1
            n
          
        
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        V
        (
        f
        (
        
          
            
              
                x
                →
              
            
          
          
            i
          
        
        )
        ,
        
          y
          
            i
          
        
        )
      
    
    {\displaystyle I_{S}[f]={\frac {1}{n}}\sum _{i=1}^{n}V(f({\vec {x}}_{i}),y_{i})}
  as a proxy for expected risk. (See statistical learning theory for a more detailed description.)",Loss functions for classification,0,1
979,20412,"MATLAB (an abbreviation of ""MATrix LABoratory"") is a proprietary multi-paradigm programming language and numeric computing environment developed by MathWorks. MATLAB allows matrix manipulations, plotting of functions and data, implementation of algorithms, creation of user interfaces, and interfacing with programs written in other languages.
Although MATLAB is intended primarily for numeric computing, an optional toolbox uses the MuPAD symbolic engine allowing access to symbolic computing abilities. An additional package, Simulink, adds graphical multi-domain simulation and model-based design for dynamic and embedded systems.
As of 2020, MATLAB has more than 4 million users worldwide. They come from various backgrounds of engineering, science, and economics.",MATLAB,0,1
980,434274,"Computer Science and Artificial Intelligence Laboratory (CSAIL) is a research institute at the Massachusetts Institute of Technology (MIT) formed by the 2003 merger of the Laboratory for Computer Science (LCS) and the Artificial Intelligence Laboratory (AI Lab). Housed within the Ray and Maria Stata Center, CSAIL is the largest on-campus laboratory as measured by research scope and membership. It is part of the Schwarzman College of Computing but is also overseen by the MIT Vice President of Research.",MIT Computer Science and Artificial Intelligence Laboratory,0,1
981,57388949,"ML.NET is a free software machine learning library for the C# and F# programming languages. It also supports Python models when used together with NimbusML. The preview release of ML.NET included transforms for feature engineering like n-gram creation, and learners to handle binary classification, multi-class classification, and regression tasks. Additional ML tasks like anomaly detection and recommendation systems have since been added, and other approaches like deep learning will be included in future versions.

",ML.NET,0,1
982,40267442,"The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. It was created by ""re-mixing"" the samples from NIST's original datasets. The creators felt that since NIST's training dataset was taken from American Census Bureau employees, while the testing dataset was taken from American high school students, it was not well-suited for machine learning experiments. Furthermore, the black and white images from NIST were normalized to fit into a 28x28 pixel bounding box and anti-aliased, which introduced grayscale levels.The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset. The original creators of the database keep a list of some of the methods tested on it. In their original paper, they use a support-vector machine to get an error rate of 0.8%.Extended MNIST (EMNIST) is a newer dataset developed and released by NIST to be the (final) successor to MNIST. MNIST included images only of handwritten digits.  EMNIST includes all the images from NIST Special Database 19, which is a large database of handwritten uppercase and lower case letters as well as digits. The images in EMNIST were converted into the same 28x28 pixel format, by the same process, as were the MNIST images. Accordingly, tools which work with the older, smaller, MNIST dataset will likely work unmodified with EMNIST.",MNIST database,0,1
983,39227709,"Massive Online Analysis (MOA) is a free open-source software project specific for data stream mining with concept drift. It is written in Java and developed at the University of Waikato, New Zealand.",Massive Online Analysis,0,1
984,52513310,"Apache MXNet is an open-source deep learning software framework, used to train and deploy deep neural networks. It is scalable, allowing for fast model training and supports a flexible programming model and multiple programming languages (including C++, Python, Java, Julia, MATLAB, JavaScript, Go, R, Scala, Perl, and Wolfram Language). The MXNet library is portable and can scale to multiple GPUs as well as multiple machines. It was co-developed by Carlos Guestrin at University of Washington (along with GraphLab).

",Apache MXNet,0,1
985,5721403,"Machine Learning  is a peer-reviewed scientific journal, published since 1986.
In 2001, forty editors and members of the editorial board of Machine Learning resigned in order to support the Journal of Machine Learning Research (JMLR), saying that in the era of the internet, it was detrimental for researchers to continue publishing their papers in expensive journals with pay-access archives. Instead, they wrote, they supported the model of JMLR, in which authors retained copyright over their papers and archives were freely available on the internet.Following the mass resignation, Kluwer changed their publishing policy to allow authors to self-archive their papers online after peer-review.

",Machine Learning (journal),0,1
986,32237314,"Machine ethics (or machine morality, computational morality, or computational ethics) is a part of the ethics of artificial intelligence concerned with adding or ensuring moral behaviors of man-made machines that use artificial intelligence, otherwise known as artificial intelligent agents. Machine ethics differs from other ethical fields related to engineering and technology.  Machine ethics should not be confused with computer ethics, which focuses on human use of computers. It should also be distinguished from the philosophy of technology, which concerns itself with the grander social effects of technology.",Machine ethics,0,1
987,53802271,"Machine learning control (MLC) is a subfield of machine learning, intelligent control and control theory
which solves optimal control problems with methods of machine learning.
Key applications are complex nonlinear systems
for which linear control theory methods are not applicable.",Machine learning control,0,1
988,11920671,"Machine perception is the capability of a computer system to interpret data in a manner that is similar to the way humans use their senses to relate to the world around them. The basic method that the computers take in and respond to their environment is through the attached hardware.  Until recently input was limited to a keyboard, or a mouse, but advances in technology, both in hardware and software, have allowed computers to take in sensory input in a way similar to humans.Machine perception allows the computer to use this sensory input, as well as conventional computational means of gathering information, to gather information with greater accuracy and to present it in a way that is more comfortable for the user. These include computer vision, machine hearing, machine touch, and machine smelling, as artificial scents are, at a chemical compound, molecular, atomic level, indiscernible and identical.The end goal of machine perception is to give machines the ability to see, feel and perceive the world as humans do and therefore for them to be able to explain in a human way why they are making their decisions, to warn us when it is failing and more importantly, the reason why it is failing. This purpose is very similar to the proposed purposes for artificial intelligence generally, except that machine perception would only grant machines limited sentience, rather than bestow upon machines full consciousness, self-awareness, and intentionality. Solving P=NP may solve this problem and accomplish this goal, and would be best to keep as a zero-knowledge proof, and series of trade secret algorithms.

",Machine perception,0,1
989,19980,"Machine translation, sometimes referred to by the abbreviation MT (not to be confused with computer-aided translation, machine-aided human translation or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.
On a basic level, MT performs mechanical substitution of words in one language for words in another, but that alone rarely produces a good translation because recognition of whole phrases and their closest counterparts in the target language is needed. Not all words in one language have equivalent words in another language, and many words have more than one meaning. 
Solving this problem with corpus statistical and neural techniques is a rapidly-growing field that is leading to better translations, handling differences in linguistic typology, translation of idioms, and the isolation of anomalies.Current machine translation software often allows for customization by domain or profession (such as weather reports), improving output by limiting the scope of allowable substitutions. This technique is particularly effective in domains where formal or formulaic language is used. It follows that machine translation of government and legal documents more readily produces usable output than conversation or less standardised text.
Improved output quality can also be achieved by human intervention: for example, some systems are able to translate more accurately if the user has unambiguously identified which words in the text are proper names. With the assistance of these techniques, MT has proven useful as a tool to assist human translators and, in a very limited number of cases, can even produce output that can be used as is (e.g., weather reports).
The progress and potential of machine translation have been much debated through its history. Since the 1950s, a number of scholars, first and most notably Yehoshua Bar-Hillel, have questioned the possibility of achieving fully automatic machine translation of high quality.",Machine translation,0,1
990,5371104,"MALLET is a Java ""Machine Learning for Language Toolkit"".",Mallet (software project),0,1
991,2073470,"In mathematics, a manifold is a topological space that locally resembles Euclidean space near each point. More precisely, an n-dimensional manifold, or n-manifold for short, is a topological space with the property that each point has a neighborhood that is homeomorphic to an open subset of n-dimensional Euclidean space.
One-dimensional manifolds include lines and circles, but not lemniscates. Two-dimensional manifolds are also called surfaces. Examples include the plane, the sphere, and the torus, and also the Klein bottle and real projective plane.
The concept of a manifold is central to many parts of geometry and modern mathematical physics because it allows complicated structures to be described in terms of well-understood topological properties of simpler spaces. Manifolds naturally arise as solution sets of systems of equations and as graphs of functions. The concept has applications in computer-graphics given the need to associate pictures with coordinates (e.g. CT scans).
Manifolds can be equipped with additional structure. One important class of manifolds are differentiable manifolds; their differentiable structure allows calculus to be done. A Riemannian metric on a manifold allows distances and angles to be measured. Symplectic manifolds serve as the phase spaces in the Hamiltonian formalism of classical mechanics, while four-dimensional Lorentzian manifolds model spacetime in general relativity.
The study of manifolds requires working knowledge of calculus and topology.",Manifold,0,1
992,68581881,"In theoretical computer science and the study of machine learning, the manifold hypothesis is the hypothesis that many high-dimensional data sets that occur in the real world actually lie along low-dimensional manifolds inside that high-dimensional space. As a consequence of the manifold hypothesis, many data sets that appear to initially require many variables to describe, can actually be described by a comparatively small number of variables, likened to the local coordinate system of the underlying manifold. It is suggested that this principle underpins the effectiveness of machine learning algorithms in describing high-dimensional data sets by considering a few common features.
The manifold hypothesis is related to the effectiveness of nonlinear dimensionality reduction techniques in machine learning. Many techniques of dimensional reduction make the assumption that data lies along a low-dimensional submanifold, such as manifold sculpting, manifold alignment, and manifold regularization.",Manifold hypothesis,0,1
993,309261,"High-dimensional data, meaning data that requires more than two or three dimensions to represent, can be  difficult to interpret. One approach to simplification is to assume that the data of interest lies within lower-dimensional space. If the data of interest is of low enough dimension, the data can be visualised in the low-dimensional space.

Below is a summary of some notable methods for nonlinear dimensionality reduction. Many of these non-linear dimensionality reduction methods are related to the linear methods listed below. Non-linear methods can be broadly classified into two groups: those that provide a mapping (either from the high-dimensional space to the low-dimensional embedding or vice versa), and those that just give a visualisation.",Nonlinear dimensionality reduction,0,1
994,48777199,"In machine learning, Manifold regularization is a technique for using the shape of a dataset to constrain the functions that should be learned on that dataset. In many machine learning problems, the data to be learned do not cover the entire input space. For example, a facial recognition system may not need to classify any possible image, but only the subset of images that contain faces. The technique of manifold learning assumes that the relevant subset of data comes from a manifold, a mathematical structure with useful properties. The technique also assumes that the function to be learned is smooth: data with different labels are not likely to be close together, and so the labeling function should not change quickly in areas where there are likely to be many data points. Because of this assumption, a manifold regularization algorithm can use unlabeled data to inform where the learned function is allowed to change quickly and where it is not, using an extension of the technique of Tikhonov regularization. Manifold regularization algorithms can extend supervised learning algorithms in semi-supervised learning and transductive learning settings, where unlabeled data are available. The technique has been used for applications including medical imaging, geographical imaging, and object recognition.

",Manifold regularization,0,1
995,516931,"In mathematics, a map is often used as a synonym for a function, but may also refer to some generalizations. Originally, this was an abbreviation of mapping, which often refers to the action of applying a function to the elements of its domain. This terminology is not completely fixed, as these terms are generally not formally defined, and can be considered to be jargon. These terms may have originated as a generalization of the process of making a geographical map, which consists of mapping the Earth surface to a sheet of paper.Maps may either be functions or morphisms, though the terms share some overlap. The term map may be used to distinguish some special types of functions, such as homomorphisms. For example, a linear map is a homomorphism of vector spaces, while the term linear function may have this meaning as well as another one. In category theory, a map may refer to a morphism, which is a generalization of the idea of a function. In some occasions, the term transformation can also be used interchangeably. There are also a few less common uses in logic and graph theory.",Map (mathematics),0,1
996,15270086,"Affinity analysis falls under the umbrella term of data mining which uncovers meaningful correlations between different entities according to their co-occurrence in a data set. In almost all systems and processes, the application of affinity analysis can extract significant knowledge about the unexpected trends. In fact, affinity analysis takes advantages of studying attributes that go together which helps uncover the hidden pattens in a big data through generating association rules. Association rules mining procedure is two-fold: first, it finds all frequent attributes in a data set and, then generates association rules satisfying some predefined criteria, support and confidence, to identify the most important relationships in the frequent itemset. The first step in the process is to count the co-occurrence of attributes in the data set. Next, a subset is created called the frequent itemset. The association rules mining takes the form of if a condition or feature (A) is present then another condition or feature (B) exists. The first condition or feature (A) is called antecedent and the latter (B) is known as consequent. This process is repeated until no additional frequent itemsets are found.  There are two important metrics for performing the association rules mining technique: support and confidence. Also, a priori algorithm is used to reduce the search space for the problem.The support metric in the association rule learning algorithm is defined as the frequency of the antecedent or consequent appearing together in a data set. Moreover, confidence is expressed as the reliability of the association rules determined by the ratio of the data records containing both A and B. The minimum threshold for support and confidence are inputs to the model. Considering all the above-mentioned definitions, affinity analysis can develop rules that will predict the occurrence of an event based on the occurrence of other events. This data mining method has been explored in different fields including disease diagnosis, market basket analysis, retail industry, higher education, and financial analysis. In retail, affinity analysis is used to perform market basket analysis, in which retailers seek to understand the purchase behavior of customers. This information can then be used for purposes of cross-selling and up-selling, in addition to influencing sales promotions, loyalty programs, store design, and discount plans.

",Affinity analysis,0,1
997,59252,"Marketing is the process of exploring, creating, and delivering value to meet the needs of a target market in terms of goods and services; potentially including selection of a target audience; selection of certain attributes or themes to emphasize in advertising; operation of advertising campaigns; attendance at trade shows and public events; design of products and packaging attractive to buyers; defining the terms of sale, such as price, discounts, warranty, and return policy; product placement in media or with people believed to influence the buying habits of others; agreements with retailers, wholesale distributors, or resellers; and attempts to create awareness of, loyalty to, and positive feelings about a brand. Marketing is typically done by the seller, typically a retailer or manufacturer. Sometimes tasks are contracted to a dedicated marketing firm or advertising agency. More rarely, a trade association or government agency (such as the Agricultural Marketing Service) advertises on behalf of an entire industry or locality, often a specific type of food (e.g. Got Milk?), food from a specific area, or a city or region as a tourism destination.
It is one of the primary components of business management and commerce. Marketers can direct their product to other businesses (B2B marketing) or directly to consumers (B2C marketing). Regardless of who is being marketed to, several factors apply, including the perspective the marketers will use. Known as market orientations, they determine how marketers approach the planning stage of marketing.The marketing mix, which outlines the specifics of the product and how it will be sold, is affected by the environment surrounding the product, the results of marketing research and market research, and the characteristics of the product's target market. Once these factors are determined, marketers must then decide what methods of promoting the product, including use of coupons and other price inducements.The term marketing, what is commonly known as attracting customers, incorporates knowledge gained by studying the management of exchange relationships and is the business process of identifying, anticipating and satisfying customers' needs and wants.",Marketing,0,1
998,1125883,"In mathematics, a Markov decision process (MDP) is a discrete-time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are partly random and partly under the control of a decision maker. MDPs are useful for studying optimization problems solved via dynamic programming. MDPs were known at least as early as the 1950s; a core body of research on Markov decision processes resulted from Ronald Howard's 1960 book, Dynamic Programming and Markov Processes. They are used in many disciplines, including robotics, automatic control, economics and manufacturing. The name of MDPs comes from the Russian mathematician Andrey Markov as they are an extension of Markov chains.
At each time step, the process is in some state 
  
    
      
        s
      
    
    {\displaystyle s}
  , and the decision maker may choose any action 
  
    
      
        a
      
    
    {\displaystyle a}
   that is available in state 
  
    
      
        s
      
    
    {\displaystyle s}
  . The process responds at the next time step by randomly moving into a new state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
  , and giving the decision maker a corresponding reward 
  
    
      
        
          R
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    {\displaystyle R_{a}(s,s')}
  .
The probability that the process moves into its new state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   is influenced by the chosen action. Specifically, it is given by the state transition function 
  
    
      
        
          P
          
            a
          
        
        (
        s
        ,
        
          s
          ′
        
        )
      
    
    {\displaystyle P_{a}(s,s')}
  . Thus, the next state 
  
    
      
        
          s
          ′
        
      
    
    {\displaystyle s'}
   depends on the current state 
  
    
      
        s
      
    
    {\displaystyle s}
   and the decision maker's action 
  
    
      
        a
      
    
    {\displaystyle a}
  . But given 
  
    
      
        s
      
    
    {\displaystyle s}
   and 
  
    
      
        a
      
    
    {\displaystyle a}
  , it is conditionally independent of all previous states and actions; in other words, the state transitions of an MDP satisfy the Markov property.
Markov decision processes are an extension of Markov chains; the difference is the addition of actions (allowing choice) and rewards (giving motivation). Conversely, if only one action exists for each state (e.g. ""wait"") and all rewards are the same (e.g. ""zero""), a Markov decision process reduces to a Markov chain.",Markov decision process,0,1
1001,18881,"Mathematical induction is a mathematical proof technique. It is essentially used to prove that a statement P(n) holds for every natural number n = 0, 1, 2, 3, ... ; that is, the overall statement is a sequence of infinitely many cases P(0), P(1), P(2), P(3), ... . Informal metaphors help to explain this technique, such as falling dominoes or climbing a ladder:

Mathematical induction proves that we can climb as high as we like on a ladder, by proving that we can climb onto the bottom rung (the basis) and that from each rung we can climb up to the next one (the step).
A proof by induction consists of two cases. The first, the base case, proves the statement for n = 0 without assuming any knowledge of other cases. The second case, the induction step, proves that if the statement holds for any given case n = k, then it must also hold for the next case n = k + 1. These two steps establish that the statement holds for every natural number n. The base case does not necessarily begin with n = 0, but often with n = 1, and possibly with any fixed natural number n = N, establishing the truth of the statement for all natural numbers n ≥ N.
The method can be extended to prove statements about more general well-founded structures, such as trees; this generalization, known as structural induction, is used in mathematical logic and computer science. Mathematical induction in this extended sense is closely related to recursion. Mathematical induction is an inference rule used in formal proofs, and is the foundation of most correctness proofs for computer programs.Although its name may suggest otherwise, mathematical induction should not be confused with inductive reasoning as used in philosophy (see Problem of induction). The mathematical method examines infinitely many cases to prove a general statement, but does so by a finite chain of deductive reasoning involving the variable n, which can take infinitely many values.",Mathematical induction,0,1
1003,1727027,"Mathematical software is software used to model, analyze or calculate numeric, symbolic or geometric data.",Mathematical software,0,1
1004,20556859,"In mathematics, a matrix (plural matrices) is a rectangular array or table of numbers, symbols, or expressions, arranged in rows and columns, which is used to represent a mathematical object or a property of such an object.
For example, 

is a matrix with two rows and three columns. This is often referred to as a ""two by three matrix"", a ""2×3-matrix"", or a matrix of dimension 2×3.
Without further specifications, matrices represent linear maps, and allow explicit computations in linear algebra. Therefore, the study of matrices is a large part of linear algebra, and most properties and operations of abstract linear algebra can be expressed in terms of matrices. For example, matrix multiplication represents composition of linear maps.
Not all matrices are related to linear algebra. This is, in particular, the case in graph theory, of incidence matrices, and adjacency matrices. This article focuses on matrices related to linear algebra, and, unless otherwise specified, all matrices represent linear maps or may be viewed as such.
Square matrices, matrices with the same number of rows and columns, play a major role in matrix theory. Square matrices of a given dimension form a noncommutative ring, which is one of the most common examples of a noncommutative ring. The determinant of a square matrix is a number associated to the matrix, which is fundamental for the study of a square matrix; for example, a square matrix is invertible if and only if it has a nonzero determinant, and the eigenvalues of a square matrix are the roots of a polynomial determinant.
In geometry, matrices are widely used for specifying and representing geometric transformations (for example rotations) and coordinate changes. In numerical analysis, many computational problems are solved by reducing them to a matrix computation, and this involves often to compute with matrices of huge dimension. Matrices are used in most areas of mathematics and most scientific fields, either directly, or through their use in geometry and numerical analysis.",Matrix (mathematics),0,1
1005,253873,"In the mathematical discipline of linear algebra, a matrix decomposition or matrix factorization is a factorization of a matrix into a product of matrices. There are many different matrix decompositions; each finds use among a particular class of problems.",Matrix decomposition,0,1
1006,10999922,"Mean shift is a non-parametric feature-space mathematical analysis technique for locating the maxima of a density function, a so-called mode-seeking algorithm. Application domains include cluster analysis in computer vision and image processing.

",Mean shift,0,1
1007,19013767,"Medical diagnosis (abbreviated Dx, Dx, or Ds) is the process of determining which disease or condition explains a person's symptoms and signs. It is most often referred to as diagnosis with the medical context being implicit. The information required for diagnosis is typically collected from a history and physical examination of the person seeking medical care. Often, one or more diagnostic procedures, such as medical tests, are also done during the process. Sometimes posthumous diagnosis is considered a kind of medical diagnosis.
Diagnosis is often challenging, because many signs and symptoms are nonspecific. For example, redness of the skin (erythema), by itself, is a sign of many disorders and thus does not tell the healthcare professional what is wrong. Thus differential diagnosis, in which several possible explanations are compared and contrasted, must be performed. This involves the correlation of various pieces of information followed by the recognition and differentiation of patterns. Occasionally the process is made easy by a sign or symptom (or a group of several) that is pathognomonic.
Diagnosis is a major component of the procedure of a doctor's visit. From the point of view of statistics, the diagnostic procedure involves classification tests.

",Medical diagnosis,0,1
1008,20976642,"Mehryar Mohri is a Professor and theoretical computer scientist at the Courant Institute of Mathematical Sciences. He is also a Research Director 
at Google Research where he heads the Learning Theory team. Prior to joining the Courant Institute, Mohri was a Research Department Head and later Technology Leader at AT&T Bell Labs, where he was a Member of the Technical Staff for about ten years. Mohri has also taught as an Assistant Professor at the University of Paris 7 (1992-1993) and Ecole Polytechnique (1992-1994).Mohri's main area of research is machine learning, in particular learning theory. He is also an expert in automata theory and algorithms. He is the author of several core algorithms that have served as the foundation for the design of many deployed speech recognition and natural language processing systems.Mohri is the author of the reference book Foundations of Machine Learning used as a textbook in many graduate-level machine learning courses.
Mohri is also a member of the Lothaire group of mathematicians with the pseudonym
M. Lothaire and contributed to the book on Applied Combinatorics on Words. He is 
the author of more than 250 conference and journal publications.
Mohri is Editorial Board member of Machine Learning, Action Editor of the Journal of Machine Learning Research (JMLR) 
and a member of the advisory board for the Journal of Automata, Languages and Combinatorics.",Mehryar Mohri,0,1
1009,14246162,"A memristor (; a portmanteau of memory resistor) is a non-linear two-terminal electrical component relating electric charge and magnetic flux linkage. It was described and named in 1971 by Leon Chua, completing a theoretical quartet of fundamental electrical components which comprises also the resistor, capacitor and inductor.Chua and Kang later generalized the concept to memristive systems.  Such a system comprises a circuit, of multiple conventional components, which mimics key properties of the ideal memristor component and is also commonly referred to as a memristor. Several such memristor system technologies have been developed, notably ReRAM.
The identification of memristive properties in electronic devices has attracted controversy. Experimentally, the ideal memristor has yet to be demonstrated.",Memristor,0,1
1010,56906363,The memtransistor is an experimental multi-terminal electronic component that might be used in the construction of artificial neural networks. It is a combination of the memristor and transistor. Applications of memristor technology is limited by it being a two terminal device. The multiple connections of the memtransistor enable it to more accurately model a neuron with its multiple synaptic connections. A neural network produced from these would provide hardware-based artificial intelligence with a good foundation.Researchers at Northwestern University have fabricated a seven-terminal device fabricated on molybdenum disulfide (MoS2). One terminal controls the current between the other six.,Memtransistor,0,1
1011,70734095,"Meta AI is an artificial intelligence laboratory that belongs to Meta Platforms Inc. (formerly known as Facebook, Inc.) Meta AI seeks to develop artificial intelligence in the digital world, enhancing its augmented  and artificial reality technologies.  Meta AI is an academic research laboratory focused on generating knowledge for the broader AI community. This is in contrast to Facebook's Applied Machine Learning (AML) team, which focuses on practical applications on its products.

",Meta AI,0,1
1012,4615464,"Meta learning
is a subfield of machine learning where automatic learning algorithms are applied to metadata about machine learning experiments. As of 2017 the term had not found a standard interpretation, however the main goal is to use such metadata to understand how automatic learning can become flexible in solving learning problems, hence to improve the performance of existing learning algorithms or to learn (induce) the learning algorithm itself, hence the alternative term learning to learn.Flexibility is important because each learning algorithm is based on a set of assumptions about the data, its inductive bias. This means that it will only learn well if the bias matches the learning problem. A learning algorithm may perform very well in one domain, but not on the next. This poses strong restrictions on the use of machine learning or data mining techniques, since the relationship between the learning problem (often some kind of database) and the effectiveness of different learning algorithms is not yet understood.
By using different kinds of metadata, like properties of the learning problem, algorithm properties (like performance measures), or patterns previously derived from the data, it is possible to learn, select, alter or combine different learning algorithms to effectively solve a given learning problem. Critiques of meta learning approaches bear a strong resemblance to the critique of metaheuristic, a possibly related problem. A good analogy to meta-learning, and the inspiration for Jürgen Schmidhuber's early work (1987) and Yoshua Bengio et al.'s work (1991), considers that genetic evolution learns the learning procedure encoded in genes and executed in each individual's brain. In an open-ended hierarchical meta learning system using genetic programming, better evolutionary methods can be learned by meta evolution, which itself can be improved by meta meta evolution, etc.

",Meta learning (computer science),0,1
1013,20455,"Michael Jeffrey Jordan (born February 17, 1963), also known by his initials MJ, is an American businessman and former professional basketball player. He played fifteen seasons in the National Basketball Association (NBA), winning six NBA championships with the Chicago Bulls. Jordan is the principal owner and chairman of the Charlotte Hornets of the NBA and of 23XI Racing in the NASCAR Cup Series. His biography on the official NBA website states: ""By acclamation, Michael Jordan is the greatest basketball player of all time."" He was integral in popularizing the NBA around the world in the 1980s and 1990s, becoming a global cultural icon in the process.Jordan played college basketball for three seasons under coach Dean Smith with the North Carolina Tar Heels. As a freshman, he was a member of the Tar Heels' national championship team in 1982. Jordan joined the Bulls in 1984 as the third overall draft pick, and quickly emerged as a league star, entertaining crowds with his prolific scoring while gaining a reputation as one of the game's best defensive players. His leaping ability, demonstrated by performing slam dunks from the free-throw line in Slam Dunk Contests, earned him the nicknames ""Air Jordan"" and ""His Airness"". Jordan won his first NBA title with the Bulls in 1991, and followed that achievement with titles in 1992 and 1993, securing a three-peat. Jordan abruptly retired from basketball before the 1993–94 NBA season to play Minor League Baseball but returned to the Bulls in March 1995 and led them to three more championships in 1996, 1997, and 1998, as well as a then-record 72 regular season wins in the 1995–96 NBA season. He retired for the second time in January 1999 but returned for two more NBA seasons from 2001 to 2003 as a member of the Washington Wizards.Jordan's individual accolades and accomplishments include six NBA Finals Most Valuable Player (MVP) awards, ten NBA scoring titles (both all-time records), five NBA MVP awards, ten All-NBA First Team designations, nine All-Defensive First Team honors, fourteen NBA All-Star Game selections, three NBA All-Star Game MVP awards, three NBA steals titles, and the 1988 NBA Defensive Player of the Year Award. He holds the NBA records for career regular season scoring average (30.12 points per game) and career playoff scoring average (33.45 points per game). In 1999, he was named the 20th century's greatest North American athlete by ESPN, and was second to Babe Ruth on the Associated Press' list of athletes of the century. Jordan was twice inducted into the Naismith Memorial Basketball Hall of Fame, once in 2009 for his individual career, and again in 2010 as part of the 1992 United States men's Olympic basketball team (""The Dream Team""). He became a member of the United States Olympic Hall of Fame in 2009, a member of the North Carolina Sports Hall of Fame in 2010, and an individual member of the FIBA Hall of Fame in 2015 and a ""Dream Team"" member in 2017.One of the most effectively marketed athletes of his generation, Jordan is known for his product endorsements. He fueled the success of Nike's Air Jordan sneakers, which were introduced in 1984 and remain popular today. Jordan also starred as himself in the 1996 live-action animation hybrid film Space Jam and is the central focus of the Emmy Award-winning documentary miniseries The Last Dance (2020). He became part-owner and head of basketball operations for the Charlotte Bobcats (now named the Hornets) in 2006, and bought a controlling interest in 2010. In 2016, Jordan became the first billionaire player in NBA history. As of 2022, his net worth is estimated at $2.1 billion.",Michael Jordan,0,1
1014,69902039,"Michal Aharon is an Israeli computer scientist known for her research on sparse dictionary learning, image denoising, and the K-SVD algorithm in machine learning. She is a researcher on advertisement ranking for Yahoo! in Haifa.",Michal Aharon,0,1
1015,21017,"A microcontroller (MCU for microcontroller unit) is a small computer on a single metal-oxide-semiconductor (MOS) VLSI integrated circuit (IC) chip. A microcontroller contains one or more CPUs (processor cores) along with memory and programmable input/output peripherals. Program memory in the form of ferroelectric RAM, NOR flash or OTP ROM is also often included on chip, as well as a small amount of RAM. Microcontrollers are designed for embedded applications, in contrast to the microprocessors used in personal computers or other general purpose applications consisting of various discrete chips.
In modern terminology, a microcontroller is similar to, but less sophisticated than, a system on a chip (SoC). An SoC may connect the external microcontroller chips as the motherboard components, but an SoC usually integrates the advanced peripherals like graphics processing unit (GPU) and Wi-Fi interface controller as its internal microcontroller unit circuits.
Microcontrollers are used in automatically controlled products and devices, such as automobile engine control systems, implantable medical devices, remote controls, office machines, appliances, power tools, toys and other embedded systems. By reducing the size and cost compared to a design that uses a separate microprocessor, memory, and input/output devices, microcontrollers make it economical to digitally control even more devices and processes.  Mixed signal microcontrollers are common, integrating analog components needed to control non-digital electronic systems. In the context of the internet of things, microcontrollers are an economical and popular means of data collection, sensing and actuating the physical world as edge devices.
Some microcontrollers may use four-bit words and operate at frequencies as low as 4 kHz for low power consumption (single-digit milliwatts or microwatts). They generally have the ability to retain functionality while waiting for an event such as a button press or other interrupt; power consumption while sleeping (CPU clock and most peripherals off) may be just nanowatts, making many of them well suited for long lasting battery applications. Other microcontrollers may serve performance-critical roles, where they may need to act more like a digital signal processor (DSP), with higher clock speeds and power consumption.",Microcontroller,0,1
1016,49830146,"Microsoft Cognitive Toolkit, previously known as CNTK and sometimes styled as The Microsoft Cognitive Toolkit, is a deprecated deep learning framework developed by Microsoft Research. Microsoft Cognitive Toolkit describes neural networks as a series of computational steps via a directed graph.",Microsoft Cognitive Toolkit,0,1
1017,61289371,"Mila - Quebec AI Institute (originally Montreal Institute for Learning Algorithms) is a research institute in Montreal, Quebec, focusing mainly on machine learning research. Approximately 800 researchers, including faculty and students, were part of Mila in 2021.",Mila (research institute),0,1
1018,877295,"Mixed reality (MR) is the merging of real and virtual worlds to produce new environments and visualizations, where physical and digital objects co-exist and interact in real time. Mixed reality does not exclusively take place in either the physical world or virtual world, but is a hybrid of augmented reality and virtual reality. To mark the difference: Augmented reality takes place in the physical world, with information or objects added virtually like an overlay; Virtual Reality immerses you in a fully virtual world without the intervention of the physical world.
There are many practical applications of mixed reality, including design, entertainment, military training, and remote working. There are also different display technologies used to facilitate the interaction between users and mixed reality applications.

",Mixed reality,0,1
1019,41585002,"mlpack is a machine learning software library for C++, built on top of the Armadillo library and the ensmallen numerical optimization library. mlpack has an emphasis on scalability, speed, and ease-of-use. Its aim is to make machine learning possible for novice users by means of a simple, consistent API, while simultaneously exploiting C++ language features to provide maximum performance and maximum flexibility for expert users. Its intended target users are scientists and engineers.
It is open-source software distributed under the BSD license, making it useful for developing both open source and proprietary software. Releases 1.0.11 and before were released under the LGPL license.  The project is supported by the Georgia Institute of Technology and contributions from around the world.",Mlpack,0,1
1020,1773278,"In computer science, and more specifically in computability theory and computational complexity theory, a model of computation is a model which describes how an output of a mathematical function is computed given an input. A model describes how units of computations, memories, and communications are organized. The computational complexity of an algorithm can be measured given a model of computation. Using a model allows studying the performance of algorithms independently of the variations that are specific to particular implementations and specific technology.

",Model of computation,0,1
1021,346781,"A modeling language is any artificial language that can be used to express information or knowledge or systems in a structure that is defined by a consistent set of rules. The rules are used for interpretation of the meaning of components in the structure.

",Modeling language,0,1
1022,62285602,"Multi-agent reinforcement learning (MARL) is a sub-field of reinforcement learning. It focuses on studying the behavior of multiple agents that coexist in a shared environment. Each agent is motivated by its own rewards, and does actions to advance its own interests; these interests may be opposed to the interests of other agents, resulting in complex group dynamics.
Multi-agent reinforcement learning is closely related to game theory and especially repeated games. Its study combines the pursuit of finding ideal algorithms that maximize rewards with a more sociological set of concepts. While research in single-agent reinforcement learning is concerned with finding the algorithm that gets the biggest number of points for one agent, research in multi-agent reinforcement learning evaluates and quantifies social metrics, such as cooperation, reciprocity, equity, social influence, language and discrimination.",Multi-agent reinforcement learning,0,1
1023,938833,"A multi-agent system (MAS or ""self-organized system"") is a computerized system composed of multiple interacting intelligent agents. Multi-agent systems can solve problems that are difficult or impossible for an individual agent or a monolithic system to solve. Intelligence may include methodic, functional, procedural approaches, algorithmic search or reinforcement learning.Despite considerable overlap, a multi-agent system is not always the same as an agent-based model (ABM).  The goal of an ABM is to search for explanatory insight into the collective behavior of agents (which don't necessarily need to be ""intelligent"") obeying simple rules, typically in natural systems, rather than in solving specific practical or engineering problems. The terminology of ABM tends to be used more often in the science, and MAS in engineering and technology. Applications where multi-agent systems research may deliver an appropriate approach include online trading, disaster response, target surveillance   and social structure modelling.",Multi-agent system,0,1
1024,938663,"Multi-task learning (MTL) is a subfield of machine learning in which multiple learning tasks are solved at the same time, while exploiting commonalities and differences across tasks. This can result in improved learning efficiency and prediction accuracy for the task-specific models, when compared to training the models separately. Early versions of MTL were called ""hints"".
In a widely cited 1997 paper, Rich Caruana gave the following characterization:Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better.
In the classification context, MTL aims to improve the performance of multiple classification tasks by learning them jointly. One example is a spam-filter, which can be treated as distinct but related classification tasks across different users. To make this more concrete, consider that different people have different distributions of features which distinguish spam emails from legitimate ones, for example an English speaker may find that all emails in Russian are spam, not so for Russian speakers. Yet there is a definite commonality in this classification task across users, for example one common feature might be text related to money transfer. Solving each user's spam classification problem jointly via MTL can let the solutions inform each other and improve performance. Further examples of settings for MTL include multiclass classification and multi-label classification.Multi-task learning works because regularization induced by requiring an algorithm to perform well on a related task can be superior to regularization that prevents overfitting by penalizing all complexity uniformly. One situation where MTL may be particularly helpful is if the tasks share significant commonalities and are generally slightly under sampled. However, as discussed below, MTL has also been shown to be beneficial for learning unrelated tasks.",Multi-task learning,0,1
1025,2266644,"A multilayer perceptron (MLP) is a fully connected class of feedforward artificial neural network (ANN). The term MLP is used ambiguously, sometimes loosely to mean any feedforward ANN, sometimes strictly to refer to networks composed of multiple layers of perceptrons (with threshold activation); see § Terminology. Multilayer perceptrons are sometimes colloquially referred to as ""vanilla"" neural networks, especially when they have a single hidden layer.An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.",Multilayer perceptron,0,1
1026,30909817,"Multilinear subspace learning is an approach to dimensionality reduction.Dimensionality reduction can be performed on a data tensor whose observations have been vectorized and organized into a data tensor, or whose observations are matrices that are concatenated into a data tensor.  Here are some examples of data tensors whose observations are vectorized  or whose observations are matrices concatenated into data tensor images (2D/3D), video sequences (3D/4D), and hyperspectral cubes (3D/4D).
The mapping from a high-dimensional vector space to a set of lower dimensional vector spaces is a multilinear projection. When observations are retained in the same organizational structure as the sensor provides them;  as matrices or higher order tensors, their representations are computed by performing N multiple linear projections.Multilinear subspace learning algorithms are higher-order generalizations of linear subspace learning methods such as principal component analysis (PCA), independent component analysis (ICA), linear discriminant analysis (LDA) and canonical correlation analysis (CCA).

",Multilinear subspace learning,0,1
1027,42171777,"A Multimedia database (MMDB) is a collection of related for multimedia data. The multimedia data include one or more primary media data types such as text, images, graphic objects (including drawings, sketches and illustrations) animation sequences, audio and video.
A Multimedia Database Management System (MMDBMS) is a framework that manages different types of data potentially represented in a wide diversity of formats on a wide array of media sources. It provides support for multimedia data types, and facilitate for creation, storage, access, query and control of a multimedia database.

",Multimedia database,0,1
1028,64020,"Multiprocessing is the use of two or more central processing units (CPUs) within a single computer system. The term also refers to the ability of a system to support more than one processor or the ability to allocate tasks between them. There are many variations on this basic theme, and the definition of multiprocessing can vary with context, mostly as a function of how CPUs are defined (multiple cores on one die, multiple dies in one package, multiple packages in one system unit, etc.).
According to some on-line dictionaries, a multiprocessor is a computer system having two or more processing units (multiple processors) each sharing main memory and peripherals, in order to simultaneously process programs. A 2009 textbook defined multiprocessor system similarly, but noting that the processors may share ""some or all of the system’s memory and I/O facilities""; it also gave tightly coupled system as a synonymous term.At the operating system level, multiprocessing is sometimes used to refer to the execution of multiple concurrent processes in a system, with each process running on a separate CPU or core, as opposed to a single process at any one instant. When used with this definition, multiprocessing is sometimes contrasted with multitasking, which may use just a single processor but switch it in time slices between tasks (i.e. a time-sharing system). Multiprocessing however means true parallel execution of multiple processes using more than one processor. Multiprocessing doesn't necessarily mean that a single process or task uses more than one processor simultaneously; the term parallel processing is generally used to denote that scenario. Other authors prefer to refer to the operating system techniques as multiprogramming and reserve the term multiprocessing for the hardware aspect of having more than one processor. The remainder of this article discusses multiprocessing only in this hardware sense.
In Flynn's taxonomy, multiprocessors as defined above are MIMD machines. As the term ""multiprocessor"" normally refers to tightly coupled systems in which all processors share memory, multiprocessors are not the entire class of MIMD machines, which also contains message passing multicomputer systems.",Multiprocessing,0,1
1029,10520679,"In computer architecture, multithreading is the ability of a central processing unit (CPU) (or a single core in a multi-core processor) to provide multiple threads of execution concurrently, supported by the operating system. This approach differs from multiprocessing. In a multithreaded application, the threads share the resources of a single or multiple cores, which include the computing units, the CPU caches, and the translation lookaside buffer (TLB).
Where multiprocessing systems include multiple complete processing units in one or more cores, multithreading aims to increase utilization of a single core by using thread-level parallelism, as well as instruction-level parallelism. As the two techniques are complementary, they are combined in nearly all modern systems architectures with multiple multithreading CPUs and with CPUs with multiple multithreading cores.",Multithreading (computer architecture),0,1
1030,555213,"Mutation is a genetic operator used to maintain genetic diversity from one generation of a population of genetic algorithm chromosomes to the next. It is analogous to biological mutation.
The classic example of a mutation operator involves a probability that an arbitrary bit in a genetic sequence will be flipped from its original state. A common method of implementing the mutation operator involves generating a random variable for each bit in a sequence. This random variable tells whether or not a particular bit will be flipped. This mutation procedure, based on the biological point mutation, is called single point mutation. Other types are inversion and floating point mutation. When the gene encoding is restrictive as in permutation problems, mutations are swaps, inversions, and scrambles.
The purpose of mutation in GAs is to introduce diversity into the sampled population. Mutation operators are used in an attempt to avoid local minima by preventing the population of chromosomes from becoming too similar to each other, thus slowing or even stopping convergence to the global optimum. This reasoning also leads most GA systems to avoid only taking the fittest of the population in generating the next generation, but rather selecting a random (or semi-random) set with a weighting toward those that are fitter.For different genome types, different mutation types are suitable:

Bit string mutationThe mutation of bit strings ensue through bit flips at random positions.Example:The probability of a mutation of a bit is 
  
    
      
        
          
            1
            l
          
        
      
    
    {\displaystyle {\frac {1}{l}}}
  , where 
  
    
      
        l
      
    
    {\displaystyle l}
   is the length of the binary vector. Thus, a mutation rate of 
  
    
      
        1
      
    
    {\displaystyle 1}
   per mutation and individual selected for mutation is reached.ShrinkThis operator adds a random number taken from a Gaussian distribution with mean equal to the original value of each decision variable characterizing the entry parent vector.  

",Mutation (genetic algorithm),0,1
1031,87339,"In statistics, naive Bayes classifiers are a family of simple ""probabilistic classifiers"" based on applying Bayes' theorem with strong (naive) independence assumptions between the features (see Bayes classifier). They are among the simplest Bayesian network models, but coupled with kernel density estimation, they can achieve high accuracy levels.Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features/predictors) in a learning problem. Maximum-likelihood training can be done by evaluating a closed-form expression,: 718  which takes linear time, rather than by expensive iterative approximation as used for many other types of classifiers.
In the statistics literature, naive Bayes models are known under a variety of names, including simple Bayes and independence Bayes. All these names reference the use of Bayes' theorem in the classifier's decision rule, but naive Bayes is not (necessarily) a Bayesian method.",Naive Bayes classifier,0,1
1032,55825672,"Naomi Altman is a statistician known for her work on kernel smoothing[KS] and kernel regression,[KR]
and interested in applications of statistics to gene expression and genomics. She is a professor of statistics at Pennsylvania State University, and a regular columnist for the ""Points of Significance"" column in Nature Methods.",Naomi Altman,0,1
1033,98778,"Natural-language understanding (NLU) or natural-language interpretation (NLI) is a subtopic  of natural-language processing in artificial intelligence that deals with machine reading comprehension. Natural-language understanding is considered an AI-hard problem.There is considerable commercial interest in the field because of its application to automated reasoning, machine translation, question answering, news-gathering, text categorization, voice-activation, archiving, and large-scale content analysis.

",Natural-language understanding,0,1
1034,21652,"Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.  The goal is a computer capable of ""understanding"" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.
Challenges in natural language processing frequently involve speech recognition, natural-language understanding, and natural-language generation.",Natural language processing,0,1
1035,21147,"Natural selection is the differential survival and reproduction of individuals due to differences in phenotype. It is a key mechanism of evolution, the change in the heritable traits characteristic of a population over generations. Charles Darwin popularised the term ""natural selection"", contrasting it with artificial selection, which in his view is intentional, whereas natural selection is not.
Variation exists within all populations of organisms. This occurs partly because random mutations arise in the genome of an individual organism, and their offspring can inherit such mutations. Throughout the lives of the individuals, their genomes interact with their environments to cause variations in traits. The environment of a genome includes the molecular biology in the cell, other cells, other individuals, populations, species, as well as the abiotic environment. Because individuals with certain variants of the trait tend to survive and reproduce more than individuals with other less successful variants, the population evolves. Other factors affecting reproductive success include sexual selection (now often included in natural selection) and fecundity selection.
Natural selection acts on the phenotype, the characteristics of the organism which actually interact with the environment, but the genetic
(heritable) basis of any phenotype that gives that phenotype a reproductive advantage may become more common in a population. Over time, this process can result in populations that specialise for particular ecological niches (microevolution) and may eventually result in speciation  (the emergence of new species, macroevolution). In other words, natural selection is a key process in the evolution of a population.
Natural selection is a cornerstone of modern biology. The concept, published by Darwin and Alfred Russel Wallace in a joint presentation of papers in 1858, was elaborated in Darwin's influential 1859 book On the Origin of Species by Means of Natural Selection, or the Preservation of Favoured Races in the Struggle for Life. He described natural selection as analogous to artificial selection, a process by which animals and plants with traits considered desirable by human breeders are systematically favoured for reproduction. The concept of natural selection originally developed in the absence of a valid theory of heredity; at the time of Darwin's writing, science had yet to develop modern theories of genetics. The union of traditional Darwinian evolution with subsequent discoveries in classical genetics formed the modern synthesis of the mid-20th century. The addition of molecular genetics has led to evolutionary developmental biology, which explains evolution at the molecular level. While genotypes can slowly change by random genetic drift, natural selection remains the primary explanation for adaptive evolution.",Natural selection,0,1
1036,59973182,"Nature Machine Intelligence is a monthly peer-reviewed scientific journal published by Nature Portfolio covering machine learning and artificial intelligence. It was created in response to the machine learning explosion of the 2010s. It launched in January 2019, and its opening was met with controversy and boycotts within the machine learning research community due to opposition to Nature publishing the journal as closed access. To address this issue, now Nature Machine Intelligence gives authors an option to publish open access papers for an additional fees, and ""authors remain owners of the research reported, and the code and data supporting the main findings of an article should be openly available. Moreover, preprints are allowed, in fact encouraged, and a link to the preprint can be added below the abstract, visible to all readers.""According to the Journal Citation Reports, Nature Machine Intelligence has a 2020 impact factor of 15.508.",Nature Machine Intelligence,0,1
1037,23348504,"Nature Methods is a monthly peer-reviewed scientific journal covering new scientific techniques. It was established in 2004 and is published by Springer Nature under the Nature Portfolio. Like other Nature journals, there is no external editorial board and editorial decisions are made by an in-house team, although peer review by external experts forms a part of the review process.Every year, the journal highlights a field, approach, or technique that has enabled recent major advances in life sciences research as the ""Method of the Year"".
According to the Journal Citation Reports, the journal had a 2020 impact factor of 28.547, ranking it first in the category ""Biochemical Research Methods"".",Nature Methods,0,1
1038,175537,"Netflix, Inc. is an American subscription streaming service and production company. Launched on August 29, 1997, it offers a film and television series library through distribution deals as well as its own productions, known as Netflix Originals.
As of March 31, 2022, Netflix had over 221.6 million subscribers worldwide, including 74.6 million in the United States and Canada, 74.0 million in Europe, the Middle East and Africa, 39.9 million in Latin America and 32.7 million in Asia-Pacific. It is available worldwide aside from Mainland China, Syria, North Korea, and Russia. Netflix has played a prominent role in independent film distribution, and it is a member of the Motion Picture Association (MPA).
Netflix can be accessed via web browsers or via application software installed on smart TVs, set-top boxes connected to televisions, tablet computers, smartphones, digital media players, Blu-ray players, video game consoles and virtual reality headsets on the list of Netflix-compatible devices. It is available in 4K resolution. In the United States, the company provides DVD and Blu-ray rentals delivered individually via the United States Postal Service from regional warehouses.Netflix was founded on the aforementioned date by Reed Hastings and Marc Randolph in Scotts Valley, California. Netflix initially both sold and rented DVDs by mail, but the sales were eliminated within a year to focus on the DVD rental business. In 2007, Netflix introduced streaming media and video on demand. The company expanded to Canada in 2010, followed by Latin America and the Caribbean. Netflix entered the content-production industry in 2013, debuting its first series House of Cards. In January 2016, it expanded to an additional 130 countries and then operated in 190 countries.
The company is ranked 115th on the Fortune 500 and 219th on the Forbes Global 2000. It is the second largest entertainment/media company by market capitalization as of February, 2022. In 2021, Netflix was ranked as the eighth-most trusted brand globally by Morning Consult. During the 2010s, Netflix was the top-performing stock in the S&P 500 stock market index, with a total return of 3,693%.Netflix is headquartered in Los Gatos, California, in Santa Clara County, with the two CEOs, Hastings and Ted Sarandos, split between Los Gatos and Los Angeles, respectively. It also operates international offices in Asia, Europe and Latin America including in Canada, France, Brazil, Netherlands, India, Italy, Japan, South Korea and the United Kingdom. The company has production hubs in Los Angeles, Albuquerque, London, Madrid, Vancouver and Toronto. Compared to other distributors, Netflix pays more for TV shows up front, but keeps more ""upside"" (i.e. future revenue opportunities from possible syndication, merchandising, etc.) on big hits.",Netflix,0,1
1039,9399111,"The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous ratings without any other information about the users or films, i.e. without the users being identified except by numbers assigned for the contest.
The competition was held by Netflix, an online DVD-rental and video streaming service, and was open to anyone who is neither connected with Netflix (current and former employees, agents, close relatives of Netflix employees, etc.) nor a resident of certain blocked countries (such as Cuba or North Korea). On September 21, 2009, the grand prize of US$1,000,000 was given to the BellKor's Pragmatic Chaos team which bested Netflix's own algorithm for predicting ratings by 10.06%.",Netflix Prize,0,1
1040,41406,"Network architecture is the design of a computer network. It framework for the specification of a network's physical components and their functional organization and configuration, its operational principles and procedures, as well as communication protocols used. 
In telecommunication, the specification of a network architecture may also include a detailed description of products and services delivered via a communications network, as well as detailed rate and billing structures under which services are compensated.
The network architecture of the Internet is predominantly expressed by its use of the Internet protocol suite, rather than a specific model for interconnecting networks or nodes in the network, or the usage of specific types of hardware links.",Network architecture,0,1
1041,1522954,"Network performance refers to measures of service quality of a network as seen by the customer.
There are many different ways to measure the performance of a network, as each network is different in nature and design. Performance can also be modeled and simulated instead of measured; one example of this is using state transition diagrams to model queuing performance or to use a Network Simulator.",Network performance,0,1
1042,28030850,"A communication protocol is a system of rules that allows two or more entities of a communications system to transmit information via any kind of variation of a physical quantity. The protocol defines the rules, syntax, semantics and synchronization of communication and possible error recovery methods. Protocols may be implemented by hardware, software, or a combination of both.Communicating systems use well-defined formats for exchanging various messages. Each message has an exact meaning intended to elicit a response from a range of possible responses pre-determined for that particular situation. The specified behavior is typically independent of how it is to be implemented. Communication protocols have to be agreed upon by the parties involved. To reach an agreement, a protocol may be developed into a technical standard. A programming language describes the same for computations, so there is a close analogy between protocols and programming languages: protocols are to communication what programming languages are to computations. An alternate formulation states that protocols are to communication what algorithms are to computation.Multiple protocols often describe different aspects of a single communication. A group of protocols designed to work together is known as a protocol suite; when implemented in software they are a protocol stack.
Internet communication protocols are published by the Internet Engineering Task Force (IETF). The IEEE (Institute of Electrical and Electronics Engineers) handles wired and wireless networking and the International Organization for Standardization (ISO) handles other types. The ITU-T handles telecommunications protocols and formats for the public switched telephone network (PSTN). As the PSTN and Internet converge, the standards are also being driven towards convergence.

",Communication protocol,0,1
1043,38050347,"A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, is an arbiter on a node in a packet switching communication network. It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. There are several network schedulers available for the different operating systems, that implement many of the existing network scheduling algorithms.
The network scheduler logic decides which network packet to forward next. The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. Systems may have a single or multiple queues in which case each may hold the packets of one flow, classification, or priority.
In some cases it may not be possible to schedule all transmissions within the constraints of the system. In these cases the network scheduler is responsible for deciding which traffic to forward and what gets dropped.

",Network scheduler,0,1
1044,592687,"Network security consists of the policies, processes and practices adopted to prevent, detect and monitor unauthorized access, misuse, modification, or denial of a computer network and network-accessible resources. Network security involves the authorization of access to data in a network, which is controlled by the network administrator. Users choose or are assigned an ID and password or other authenticating information that allows them access to information and programs within their authority.  Network security covers a variety of computer networks, both public and private, that are used in everyday jobs: conducting transactions and communications among businesses, government agencies and individuals. Networks can be private, such as within a company, and others which might be open to public access. Network security is involved in organizations, enterprises, and other types of institutions. It does as its title explains: it secures the network, as well as protecting and overseeing operations being done. The most common and simple way of protecting a network resource is by assigning it a unique name and a corresponding password.

",Network security,0,1
1045,788676,"In computer networking, a network service is an application running at the network application layer and above, that provides data storage, manipulation, presentation, communication or other capability which is often implemented using a client–server or peer-to-peer architecture based on application layer network protocols.Each service is usually provided by a server component running on one or more computers (often a dedicated server computer offering multiple services) and accessed via a network by client components running on other devices. However, the client and server components can both be run on the same machine.
Clients and servers will often have a user interface, and sometimes other hardware associated with it.

",Network service,0,1
1046,1446517,"In computer network research, network simulation is a technique whereby a software program replicates the behavior of a real network. This is achieved by calculating the interactions between the different network entities such as routers, switches, nodes, access points, links, etc. Most simulators use discrete event simulation in which the modeling of systems in which state variables change at discrete points in time.  The behavior of the network and the various applications and services it supports can then be observed in a test lab; various attributes of the environment can also be modified in a controlled manner to assess how the network/protocols would behave under different conditions.

",Network simulation,0,1
1047,1699254,"Networking hardware, also known as network equipment or computer networking devices, are electronic devices which are required for communication and interaction between devices on a computer network. Specifically, they mediate data transmission in a computer network. Units which are the last receiver or generate data are called hosts, end systems or data terminal equipment.

",Networking hardware,0,1
1048,17422480,"Neural Computation is a monthly peer-reviewed scientific journal covering all aspects of neural computation, including modeling the brain and the design and construction of neurally-inspired information processing systems. It was established in 1989 and is published by MIT Press. The editor-in-chief is Terrence J. Sejnowski (Salk Institute for Biological Studies). According to the Journal Citation Reports, the journal has a 2014 impact factor of 2.207.

",Neural Computation (journal),0,1
1049,47012074,"Neural Designer is a software tool for machine learning based on neural networks, a main area of artificial intelligence research, and contains a graphical user interface which simplifies data entry and interpretation of results.
In 2015, Neural Designer was chosen by the European Commission, within the Horizon 2020 program, as a disruptive technology in the ICT field.",Neural Designer,0,1
1050,21227904,"Neural Lab is a no-cost neural network simulator that designs and trains artificial neural networks for use in engineering, business, computer science and technology. It integrates with Microsoft Visual Studio using C (Win32 - Wintempla) to incorporate artificial neural networks into custom applications, research simulations or end user interfaces. 
It provides a visual environment to design and test artificial neural networks. 
The latest Neural Lab version is 4.1.  
The two major versions are version 3.1 and 4.0.",Neural Lab,0,1
1051,50568884,"A Neural Turing machine (NTMs) is a recurrent neural network model of a Turing machine. The approach was published by Alex Graves et al. in 2014. NTMs combine the fuzzy pattern matching capabilities of neural networks with the algorithmic power of programmable computers. An NTM has a neural network controller coupled to external memory resources, which it interacts with through attentional mechanisms. The memory interactions are differentiable end-to-end, making it possible to optimize them using gradient descent. An NTM with a long short-term memory (LSTM) network controller can infer simple algorithms such as copying, sorting, and associative recall from examples alone.The authors of the original NTM paper did not publish their source code. The first stable open-source implementation was published in 2018 at the 27th International Conference on Artificial Neural Networks, receiving a best-paper award.  Other open source implementations of NTMs exist but as of 2018 they are not sufficiently stable for production use. The developers either report that the gradients of their implementation sometimes become NaN during training for unknown reasons and cause training to fail; report slow convergence; or do not report the speed of learning of their implementation.Differentiable neural computers are an outgrowth of Neural Turing machines, with attention mechanisms that control where the memory is active, and improve performance.",Neural Turing machine,0,1
1052,47961606,"Neural machine translation (NMT) is an approach to machine translation that uses an artificial neural network to predict the likelihood of a sequence of words, typically modeling entire sentences in a single integrated model.",Neural machine translation,0,1
1055,4390806,"NeuroSolutions is a neural network development environment developed by NeuroDimension. It combines a modular, icon-based (component-based) network design interface with an implementation of advanced learning procedures, such as conjugate gradients, Levenberg-Marquardt and backpropagation through time. The software is used to design, train and deploy neural network (supervised learning and unsupervised learning) models to perform a wide variety of tasks such as data mining, classification, function approximation, multivariate regression and time-series prediction.",NeuroSolutions,0,1
1056,453086,"Neuromorphic engineering, also known as neuromorphic computing, is the use of very-large-scale integration (VLSI) systems containing electronic analog circuits to mimic neuro-biological architectures present in the nervous system. A neuromorphic computer/chip is any device that uses physical artificial neurons (made from silicon) to do computations. In recent times, the term neuromorphic has been used to describe analog, digital, mixed-mode analog/digital VLSI, and software systems that implement models of neural systems (for perception, motor control, or multisensory integration). The implementation of neuromorphic computing on the hardware level can be realized by oxide-based memristors, spintronic memories, threshold switches, and transistors. Training software-based neuromorphic systems of spiking neural networks can be achieved using error backpropagation, e.g., using Python based frameworks such as snnTorch, or using canonical learning rules from the biological learning literature, e.g., using BindsNet.A key aspect of neuromorphic engineering is understanding how the morphology of individual neurons, circuits, applications, and overall architectures creates desirable computations, affects how information is represented, influences robustness to damage, incorporates learning and development, adapts to local change (plasticity), and facilitates evolutionary change.
Neuromorphic engineering is an interdisciplinary subject that takes inspiration from biology, physics, mathematics, computer science, and electronic engineering to design artificial neural systems, such as vision systems, head-eye systems, auditory processors, and autonomous robots, whose physical architecture and design principles are based on those of biological nervous systems. It was developed by Carver Mead in the late 1980s.",Neuromorphic engineering,0,1
1057,21120,"A neuron or nerve cell is an electrically excitable cell that communicates with other cells via specialized connections called synapses. The neuron is the main component of nervous tissue in all animals except sponges and placozoa. Plants and fungi do not have nerve cells.
Neurons are typically classified into three types based on their function. Sensory neurons respond to stimuli such as touch, sound, or light that affect the cells of the sensory organs, and they send signals to the spinal cord or brain. Motor neurons receive signals from the brain and spinal cord to control everything from muscle contractions to glandular output. Interneurons connect neurons to other neurons within the same region of the brain or spinal cord. When multiple neurons are connected together they form what is called a neural circuit.
A typical neuron consists of a cell body (soma), dendrites, and a single axon. The soma is a compact structure and the axon and dendrites are filaments extruding from the soma. Dendrites typically branch profusely and extend a few hundred micrometers from the soma. The axon leaves the soma at a swelling called the axon hillock and travels for as far as 1 meter in humans or more in other species. It branches but usually maintains a constant diameter. At the farthest tip of the axon's branches are axon terminals, where the neuron can transmit a signal across the synapse to another cell. Neurons may lack dendrites or have no axon. The term neurite is used to describe either a dendrite or an axon, particularly when the cell is undifferentiated.
Most neurons receive signals via the dendrites and soma and send out signals down the axon. At the majority of synapses, signals cross from the axon of one neuron to a dendrite of another. However, synapses can connect an axon to another axon or a dendrite to another dendrite.
The signaling process is partly electrical and partly chemical. Neurons are electrically excitable, due to maintenance of voltage gradients across their membranes. If the voltage changes by a large enough amount over a short interval, the neuron generates an all-or-nothing electrochemical pulse called an action potential. This potential travels rapidly along the axon and activates synaptic connections as it reaches them. Synaptic signals may be excitatory or inhibitory, increasing or reducing the net voltage that reaches the soma.
In most cases, neurons are generated by neural stem cells during brain development and childhood. Neurogenesis largely ceases during adulthood in most areas of the brain.",Neuron,1,1
1059,486492,The New England Journal of Medicine (NEJM) is a weekly medical journal published by the Massachusetts Medical Society. It is among the most prestigious peer-reviewed medical journals as well as the oldest continuously published one.,The New England Journal of Medicine,0,1
1060,30680,"The New York Times is an American daily newspaper based in New York City with a worldwide readership. It was founded in 1851 by Henry Jarvis Raymond and George Jones, and was initially published by Raymond, Jones & Company. The Times has won 132 Pulitzer Prizes, the most of any newspaper, and has long been regarded as a national ""newspaper of record"". It is ranked 18th in the world by circulation and 3rd in the U.S.The paper is owned by The New York Times Company, which is publicly traded. It has been governed by the Sulzberger family since 1896, through a dual-class share structure after its shares became publicly traded. A. G. Sulzberger and his father, Arthur Ochs Sulzberger Jr.—the paper's publisher and the company's chairman, respectively—are the fifth and fourth generations of the family to head the paper.Since the mid-1970s, The New York Times has expanded its layout and organization, adding special weekly sections on various topics supplementing the regular news, editorials, sports, and features. Since 2008, the Times has been organized into the following sections: News, Editorials/Opinions-Columns/Op-Ed, New York (metropolitan), Business, Sports, Arts, Science, Styles, Home, Travel, and other features. On Sundays, the Times is supplemented by the Sunday Review (formerly the Week in Review), The New York Times Book Review, The New York Times Magazine, and T: The New York Times Style Magazine.",The New York Times,0,1
1061,8016113,"Nils John Nilsson (February 6, 1933 – April 23, 2019) was an American computer scientist. He was one of the founding researchers in the discipline of artificial intelligence. He was the first Kumagai Professor of Engineering in computer science at Stanford University from 1991 until his retirement. He is particularly known for his contributions to search, planning, knowledge representation, and robotics.",Nils John Nilsson,0,1
1062,3681279,"Non-negative matrix factorization (NMF or NNMF), also non-negative matrix approximation is a group of algorithms in multivariate analysis and linear algebra where a matrix V is factorized into (usually) two matrices W and H, with the property that all three matrices have no negative elements. This non-negativity makes the resulting matrices easier to inspect. Also, in applications such as processing of audio spectrograms or muscular activity, non-negativity is inherent to the data being considered. Since the problem is not exactly solvable in general, it is commonly approximated numerically.
NMF finds applications in such fields as astronomy, computer vision, document clustering, missing data imputation, chemometrics, audio signal processing, recommender systems, and bioinformatics.",Non-negative matrix factorization,0,1
1064,22509799,"Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander.
Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: the problem of detecting meaningful clusters in data of varying density. To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. This is represented as a dendrogram.",OPTICS algorithm,0,1
1065,44577560,"In computational learning theory, Occam learning is a model of algorithmic learning where the objective of the learner is to output a succinct representation of received training data. This is closely related to probably approximately correct (PAC) learning, where the learner is evaluated on its predictive power of a test set.
Occam learnability implies PAC learning, and for a wide variety of concept classes, the converse is also true: PAC learnability implies Occam learnability.",Occam learning,0,1
1066,1555671,"Online advertising, also known as online marketing, Internet advertising, digital advertising or web advertising, is a form of marketing and advertising which uses the Internet to promote products and services to audiences and platform users.  Online advertising includes email marketing, search engine marketing (SEM), social media marketing, many types of display advertising (including web banner advertising), and mobile advertising. Advertisements are increasingly being delivered via automated software systems operating across multiple websites, media services and platforms, known as programmatic advertising.Like other advertising media, online advertising frequently involves a publisher, who integrates advertisements into its online content, and an advertiser, who provides the advertisements to be displayed on the publisher's content. Other potential participants include advertising agencies who help generate and place the ad copy, an ad server which technologically delivers the ad and tracks statistics, and advertising affiliates who do independent promotional work for the advertiser.
In 2016, Internet advertising revenues in the United States surpassed those of cable television and broadcast television.: 14    In 2017, Internet advertising revenues in the United States totaled $83.0 billion, a 14% increase over the $72.50 billion in revenues in 2016. And research estimates from 2019's online advertising spend puts it at $125.2 billion in the United States, some $54.8 billion higher than the spend on television ($70.4 billion).Many common online advertising practices are controversial and, as a result, have been increasingly subject to regulation. Many internet users also find online advertising disruptive and have increasingly turned to ad blocking for a variety of reasons. Online ad revenues also may not adequately replace other publishers' revenue streams. Declining ad revenue has led some publishers to place their content behind paywalls.",Online advertising,0,1
1067,19892153,"In computer science, online machine learning is a method of machine learning in which data becomes available in a sequential order and is used to update the best predictor for future data at each step, as opposed to batch learning techniques which generate the best predictor by learning on the entire training data set at once. Online learning is a common technique used in areas of machine learning where it is computationally infeasible to train over the entire dataset, requiring the need of out-of-core algorithms. It is also used in situations where it is necessary for the algorithm to dynamically adapt to new patterns in the data, or when the data itself is generated as a function of time, e.g., stock price prediction.
Online learning algorithms may be prone to catastrophic interference, a problem that can be addressed by incremental learning approaches.",Online machine learning,0,1
1068,277663,"Open-source software (OSS) is computer software that is released under a license in which the copyright holder grants users the rights to use, study, change, and distribute the software and its source code to anyone and for any purpose. Open-source software may be developed in a collaborative public manner. Open-source software is a prominent example of open collaboration, meaning any capable user is able to participate online in development, making the number of possible contributors indefinite. The ability to examine the code facilitates public trust in the software.Open-source software development can bring in diverse perspectives beyond those of a single company. A 2008 report by the Standish Group stated that adoption of open-source software models has resulted in savings of about $60 billion per year for consumers.Open source code can be used for studying and allows capable end users to adapt software to their personal needs in a similar way user scripts and custom style sheets allow for web sites, and eventually publish the modification as a fork for users with similar preferences, and directly submit possible improvements as pull requests.",Open-source software,0,1
1069,58714104,"OpenAI Five is a computer program by OpenAI that plays the five-on-five video game Dota 2. Its first public appearance occurred in 2017, where it was demonstrated in a live one-on-one game against the professional player, Dendi, who lost to it. The following year, the system had advanced to the point of performing as a full team of five, and began playing against and showing the capability to defeat professional teams.
By choosing a game as complex as Dota 2 to study machine learning, OpenAI thought they could more accurately capture the unpredictability and continuity seen in the real world, thus constructing more general problem-solving systems. The algorithms and code used by OpenAI Five were eventually borrowed by another neural network in development by the company, one which controlled a physical robotic hand. OpenAI Five has been compared to other similar cases of artificial intelligence (AI) playing against and defeating humans, such as AlphaStar in the video game StarCraft II, AlphaGo in the board game Go, Deep Blue in chess, and Watson on the television game show Jeopardy!.",OpenAI Five,0,1
1070,42129549,"OpenNN (Open Neural Networks Library) is a software library written in the C++ programming language which implements neural networks, a main area of deep learning research. The library is open-source, licensed under the GNU Lesser General Public License.",OpenNN,0,1
1071,22194,"An operating system (OS) is system software that manages computer hardware, software resources, and provides common services for computer programs.
Time-sharing operating systems schedule tasks for efficient use of the system and may also include accounting software for cost allocation of processor time, mass storage, printing, and other resources.
For hardware functions such as input and output and memory allocation, the operating system acts as an intermediary between programs and the computer hardware, although the application code is usually executed directly by the hardware and frequently makes system calls to an OS function or is interrupted by it. Operating systems are found on many devices that contain a computer –  from cellular phones and video game consoles to web servers and supercomputers.
The dominant general-purpose personal computer operating system is Microsoft Windows with a market share of around 76.45%. macOS by Apple Inc. is in second place (17.72%), and the varieties of Linux are collectively in third place (1.73%). In the mobile sector (including smartphones and tablets), Android's share is up to 72% in the year 2020. According to third quarter 2016 data, Android's share on smartphones is dominant with 87.5 percent with a growth rate of 10.3 percent per year, followed by Apple's iOS with 12.1 percent with per year decrease in market share of 5.2 percent, while other operating systems amount to just 0.3 percent. Linux distributions are dominant in the server and supercomputing sectors. Other specialized classes of operating systems (special-purpose operating systems), such as embedded and real-time systems, exist for many applications. Security-focused operating systems also exist. Some operating systems have low system requirements (e.g. light-weight Linux distribution). Others may have higher system requirements.
Some operating systems require installation or may come pre-installed with purchased computers (OEM-installation), whereas others may run directly from media (i.e. live CD) or flash memory (i.e. USB stick).",Operating system,0,1
1072,65692,"An operational definition specifies concrete, replicable procedures designed to represent a construct. In the words of American psychologist S.S. Stevens (1935), ""An operation is the performance which we execute in order to make known a concept."" For example, an operational definition of ""fear"" (the construct) often includes measurable physiologic responses that occur in response to a perceived threat. Thus, ""fear"" might be operationally defined as specified changes in heart rate, galvanic skin response, pupil dilation, and blood pressure.

",Operational definition,0,1
1074,49091,"Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast).Widely used as a form of data entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision.
Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.",Optical character recognition,0,1
1075,51687009,"Oracle Cloud is a cloud computing service offered by Oracle Corporation providing servers, storage, network, applications and services through a global network of Oracle Corporation managed data centers. The company allows these services to be provisioned on demand over the Internet. 
Oracle Cloud provides Infrastructure as a Service (IaaS), Platform as a Service (PaaS), Software as a Service (SaaS), and Data as a Service (DaaS). These services are used to build, deploy, integrate, and extend applications in the cloud. This platform supports numerous open standards (SQL, HTML5, REST, etc.), open-source applications (Kubernetes, Spark, Hadoop, Kafka, MySQL, Terraform, etc.), and a variety of programming languages, databases, tools, and frameworks including Oracle-specific, Open Source, and third-party software and systems.

",Oracle Cloud,0,1
1076,8740192,"Oracle Data Mining (ODM) is an option of Oracle Database Enterprise Edition. It contains several data mining and data analysis algorithms for classification, prediction, regression, associations, feature selection, anomaly detection, feature extraction, and specialized analytics. It provides means for the creation, management and operational deployment of data mining models inside the database environment.",Oracle Data Mining,0,1
1077,2539154,"Orange is an open-source data visualization, machine learning and data mining toolkit. It features a visual programming front-end for explorative rapid qualitative data analysis and interactive data visualization.",Orange (software),0,1
1079,6585513,"The following outline is provided as an overview of and topical guide to artificial intelligence:
Artificial intelligence (AI) – intelligence exhibited by machines or software. It is also the name of the scientific field which studies how to create computers and computer software that are capable of intelligent behaviour.",Outline of artificial intelligence,0,1
1080,169633,"Computer science (also called computing science) is the study of the theoretical foundations of information and computation and their implementation and application in computer systems. One well known subject classification system for computer science is the ACM Computing Classification System devised by the Association for Computing Machinery.

",Outline of computer science,0,1
1081,53587467,"The following outline is provided as an overview of and topical guide to machine learning. Machine learning is a subfield of soft computing within computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence. In 1959, Arthur Samuel defined machine learning as a ""field of study that gives computers the ability to learn without being explicitly programmed"". Machine learning explores the study and construction of algorithms that can learn from and make predictions on data. Such algorithms operate by building a model from an example training set of input observations in order to make data-driven predictions or decisions expressed as outputs, rather than following strictly static program instructions.",Outline of machine learning,0,1
1082,173332,"In mathematical modeling, overfitting is ""the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit to additional data or predict future observations reliably"". An overfitted model is a mathematical model that contains more parameters than can be justified by the data. The essence of overfitting is to have unknowingly extracted some of the residual variation (i.e., the noise) as if that variation represented underlying model structure.: 45 Underfitting occurs when a mathematical model cannot adequately capture the underlying structure of the data. An under-fitted model is a model where some parameters or terms that would appear in a correctly specified model are missing. Under-fitting would occur, for example, when fitting a linear model to non-linear data. Such a model will tend to have poor predictive performance.
The possibility of over-fitting exists because the criterion used for selecting the model is not the same as the criterion used to judge the suitability of a model. For example, a model might be selected by maximizing its performance on some set of training data, and yet its suitability might be determined by its ability to perform well on unseen data; then over-fitting occurs when a model begins to ""memorize"" training data rather than ""learning"" to generalize from a trend. 
As an extreme example, if the number of parameters is the same as or greater than the number of observations, then a model can perfectly predict the training data simply by memorizing the data in its entirety. (For an illustration, see Figure 2.) Such a model, though, will typically fail severely when making predictions. 
The potential for overfitting depends not only on the number of parameters and data but also the conformability of the model structure with the data shape, and the magnitude of model error compared to the expected level of noise or error in the data. Even when the fitted model does not have an excessive number of parameters, it is to be expected that the fitted relationship will appear to perform less well on a new data set than on the data set used for fitting (a phenomenon sometimes known as shrinkage). In particular, the value of the coefficient of determination will shrink relative to the original data.
To lessen the chance or amount of overfitting, several techniques are available (e.g., model comparison, cross-validation, regularization, early stopping, pruning, Bayesian priors, or dropout). The basis of some techniques is either (1) to explicitly penalize overly complex models or (2) to test the model's ability to generalize by evaluating its performance on a set of data not used for training, which is assumed to approximate the typical unseen data that a model will encounter.

",Overfitting,0,1
1083,38833779,"pandas is a software library written for the Python programming language for data manipulation and analysis. In particular, it offers data structures and operations for manipulating numerical tables and time series. It is free software released under the three-clause BSD license. The name is derived from the term ""panel data"", an econometrics term for data sets that include observations over multiple time periods for the same individuals. Its name is a play on the phrase ""Python data analysis"" itself. Wes McKinney started building what would become pandas at AQR Capital while he was a researcher there from 2007 to 2010.

",Pandas (software),0,1
1084,145162,"Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.Parallel computing is closely related to concurrent computing—they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency, and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.
Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.
In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law.",Parallel computing,0,1
1085,707411,"A paraphrase () is a restatement of the meaning of a text or passage using other words. The term itself is derived via Latin paraphrasis, from Ancient Greek  παράφρασις (paráphrasis) 'additional manner of expression'. The act of paraphrasing is also called paraphrasis.",Paraphrase,0,1
1087,47790413,Pedro Domingos is a Professor Emeritus of computer science and engineering at the University of Washington. He is a researcher in machine learning known for Markov logic network enabling uncertain inference.,Pedro Domingos,0,1
1088,24511,"Protein primary structure is the linear sequence of amino acids in a peptide or protein. By convention, the primary structure of a protein is reported starting from the amino-terminal (N) end to the carboxyl-terminal (C) end. Protein biosynthesis is most commonly performed by ribosomes in cells. Peptides can also be synthesized in the laboratory. Protein primary structures can be directly sequenced, or inferred from DNA sequences.",Protein primary structure,0,1
1089,172777,"In machine learning, the perceptron is an algorithm for supervised learning of binary classifiers.  A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.  It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector.",Perceptron,0,1
1090,60644,"A peripheral or peripheral device is an auxiliary device used to put information into and get information out of a computer. The term peripheral device refers to all hardware components that are attached to a computer and are controlled by the computer system, but they are not the core components of the computer, such as the CPU or power supply unit. In other words, peripherals can also be defined as devices that can be easily removed and plugged into a computer system.
Several categories of peripheral devices may be identified, based on their relationship with the computer:

An input device sends data or instructions to the computer, such as a mouse, keyboard, graphics tablet, image scanner, barcode reader, game controller, light pen, light gun, microphone and webcam;
An output device provides output data from the computer, such as a computer monitor, projector, printer, headphones and computer speaker;
An input/output device performs both input and output functions, such as a computer data storage device (including a disk drive, solid-state drive, USB flash drive, memory card and tape drive), modem, network adapter and multi-function printer.Many modern electronic devices, such as Internet-enabled digital watches, keyboards, and tablet computers, have interfaces for use as computer peripheral 
devices.",Peripheral,0,1
1091,11897790,"Peter E. Hart (born c. 1940s) is an American computer scientist and entrepreneur. He was chairman and president of Ricoh Innovations, which he founded in 1997. He made significant contributions in the field of computer science in a series of widely cited publications from the years 1967–75 while associated with the Artificial Intelligence Center of SRI International, a laboratory where he also served as director.",Peter E. Hart,0,1
1092,566666,"Peter Norvig (born December 14, 1956) is an American computer scientist and Distinguished Education Fellow at the Stanford Institute for Human-Centered AI. He previously served as a director of research and search quality at Google. Norvig is the co-author with Stuart J. Russell of the most popular textbook in the field of AI: Artificial Intelligence: A Modern Approach used in more than 1,500 universities in 135 countries.",Peter Norvig,0,1
1093,2958015,"The philosophy of artificial intelligence is a branch of the philosophy of technology that explores artificial intelligence and its implications for knowledge and understanding of intelligence, ethics, consciousness, epistemology, and free will. Furthermore, the technology is concerned with the creation of artificial animals or artificial people (or, at least, artificial creatures; see artificial life) so the discipline is of considerable interest to philosophers. These factors contributed to the emergence of the philosophy of artificial intelligence. Some scholars argue that the AI community's dismissal of philosophy is detrimental.The philosophy of artificial intelligence attempts to answer such questions as follows:
Can a machine act intelligently? Can it solve any problem that a person would solve by thinking?
Are human intelligence and machine intelligence the same?  Is the human brain essentially a computer?
Can a machine have a mind, mental states, and consciousness in the same sense that a human being can? Can it feel how things are?Questions like these reflect the divergent interests of AI researchers, cognitive scientists and philosophers respectively. The scientific answers to these questions depend on the definition of ""intelligence"" and ""consciousness"" and exactly which ""machines"" are under discussion.
Important propositions in the philosophy of AI include some of the following:

Turing's ""polite convention"": If a machine behaves as intelligently as a human being, then it is as intelligent as a human being.
The Dartmouth proposal: ""Every aspect of learning or any other feature of intelligence can be so precisely described that a machine can be made to simulate it.""
Allen Newell and Herbert A. Simon's physical symbol system hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""
John Searle's strong AI hypothesis: ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds.""
Hobbes' mechanism: ""For 'reason' ... is nothing but 'reckoning,' that is adding and subtracting, of the consequences of general names agreed upon for the 'marking' and 'signifying' of our thoughts...""",Philosophy of artificial intelligence,0,1
1094,577742,"Photograph manipulation involves the transformation or alteration of a photograph using various methods and techniques to achieve desired results. Some photograph manipulations are considered to be skillful artwork, while others are considered to be unethical practices, especially when used to deceive the public. Other examples include being used for political propaganda, or to improve the appearance of a product or person, or simply as entertainment or practical jokes.
Depending on the application and intent, some photograph manipulations are considered an art form because it involves the creation of unique images and in some instances, signature expressions of art by photographic artists. Ansel Adams employed some of the more common manipulations using darkroom exposure techniques, burning (darkening) and dodging (lightening) a photograph. Other examples of photo manipulation include retouching photographs using ink or paint, airbrushing, double exposure, piecing photos or negatives together in the darkroom, scratching instant films, or through the use of software-based manipulation tools applied to digital images. Modern examples include software applications that may be developed for the manipulation of digital images, ranging from professional applications to very basic imaging software for casual users.",Photograph manipulation,0,1
1095,24833746,"A physical neural network is a type of artificial neural network in which an electrically adjustable material is used to emulate the function of a neural synapse. ""Physical"" neural network is used to emphasize the reliance on physical hardware used to emulate neurons as opposed to software-based approaches. More generally the term is applicable to other artificial neural networks in which a memristor or other electrically adjustable resistance material is used to emulate a neural synapse.

",Physical neural network,0,1
1096,404130,"In mathematics, a piecewise-defined function (also called a piecewise function, a hybrid function, or definition by cases) is a function defined by multiple sub-functions, where each sub-function applies to a different interval in the domain. Piecewise definition is actually a way of expressing the function, rather than a characteristic of the function itself.
A distinct, but related notion is that of a property holding piecewise for a function, used when the domain can be divided into intervals on which the property holds. Unlike for the notion above, this is actually a property of the function itself. A piecewise linear function (which happens to be also continuous) is depicted as an example.",Piecewise,0,1
1097,42167618,"In machine learning, Platt scaling or Platt calibration is a way of transforming the outputs of a classification model into a probability distribution over classes. The method was invented by John Platt in the context of support vector machines,
replacing an earlier method by Vapnik,
but can be applied to other classification models.
Platt scaling works by fitting a logistic regression model to a classifier's scores.",Platt scaling,0,1
1099,144068,"Precision agriculture (PA) is a farming management concept based on observing, measuring and responding to inter and intra-field variability in crops. First conceptual work on PA and practical applications go back in the late 1980s. The goal of precision agriculture research is to define a decision support system (DSS) for whole farm management with the goal of optimizing returns on inputs while preserving resources.Among these many approaches is a phytogeomorphological approach which ties multi-year crop growth stability/characteristics to topological terrain attributes. The interest in the phytogeomorphological approach stems from the fact that the geomorphology component typically dictates the hydrology of the farm field.The practice of precision agriculture has been enabled by the advent of GPS and GNSS. The farmer's and/or researcher's ability to locate their precise position in a field allows for the creation of maps of the spatial variability of as many variables as can be measured (e.g. crop yield, terrain features/topography, organic matter content, moisture levels, nitrogen levels, pH, EC, Mg, K, and others). Similar data is collected by sensor arrays mounted on GPS-equipped combine harvesters. These arrays consist of real-time sensors that measure everything from chlorophyll levels to plant water status, along with multispectral imagery. This data is used in conjunction with satellite imagery by variable rate technology (VRT) including seeders, sprayers, etc. to optimally distribute resources. However, recent technological advances have enabled the use of real-time sensors directly in soil, which can wirelessly transmit data without the need of human presence.Precision agriculture has also been enabled by unmanned aerial vehicles that are relatively inexpensive and can be operated by novice pilots.  These agricultural drones can be equipped with multispectral or RGB cameras to capture many images of a field that can be stitched together using photogrammetric methods to create orthophotos. These multispectral images contain multiple values per pixel in addition to the traditional red, green blue values such as near infrared and red-edge spectrum values used to process and analyze vegetative indexes such as NDVI maps. These drones are capable of capturing imagery and providing additional geographical references such as elevation, which allows software to perform map algebra functions to build precise topography maps. These topographic maps can be used to correlate crop health with topography, the results of which can be used to optimize crop inputs such as water, fertilizer or chemicals such as herbicides and growth regulators through variable rate applications.",Precision agriculture,0,1
1100,4141563,"Predictive analytics encompasses a variety of statistical techniques from data mining, predictive modeling, and machine learning that analyze current and historical facts to make predictions about future or otherwise unknown events.
In business, predictive models exploit patterns found in historical and transactional data to identify risks and opportunities. Models capture relationships among many factors to allow assessment of risk or potential associated with a particular set of conditions, guiding decision-making for candidate transactions.
The defining functional effect of these technical approaches is that predictive analytics provides a predictive score (probability) for each individual (customer, employee, healthcare patient, product SKU, vehicle, component, machine, or other organizational unit) in order to determine, inform, or influence organizational processes that pertain across large numbers of individuals, such as in marketing, credit risk assessment, fraud detection, manufacturing, healthcare, and government operations including law enforcement.

",Predictive analytics,0,1
1101,2538775,"Predictive modelling uses scientifically proved mathematical statistics to predict events outcomes. Most often one event that a mathematician wants to predict or apply predictive analysis on it is in the future (also here physics and mathematical notion of future can be applied), but predictive modelling can be applied to any type of mathematically stated as ""unknown"" event, (almost) regardless of when it occurred. 
In fact, one mathematician may use multiple predictive models or apply different functions in predictive modelling, that would expand her abilities rather than the simplistic models of if or not. 
In many cases the model is chosen on the basis of detection theory to try to guess the probability of an outcome given a set amount of input data, for example given an email determining how likely that it is spam.
Models can use one or more classifiers (or otherwise to say events databases in statistics) in trying to determine the probability of a set of data belonging to another set. For example, one computational model might be used to determine whether an email is spam or non-spam.
Depending on definitional boundaries, predictive modelling is synonymous with, or largely overlapping with, the field of machine learning, as it is more commonly referred to in academic or research and development contexts. When deployed commercially, predictive modelling is often referred to as predictive analytics.
Predictive mathematical modelling is often contrasted with causal modelling/analysis.  In the former, one may be entirely satisfied to make use of indicators of, or proxies for, the outcome of interest. In the latter, one seeks to determine most close to true cause-and-effect relationships. This distinction has given rise to a scientific literature in the fields of research methods and mathematical and computational statistics and to the common statement that ""correlation does not imply causation"".",Predictive modelling,0,1
1102,239887,"Pricing is the process whereby a business sets the price at which it will sell its products and services, and may be part of the business's marketing plan. In setting prices, the business will take into account the price at which it could acquire the goods, the manufacturing cost, the marketplace, competition, market condition, brand, and quality of product.
Pricing is a fundamental aspect of product management and is one of the four Ps of the marketing mix, the other three aspects being product, promotion, and place. Price is the only revenue generating element amongst the four Ps, the rest being cost centers. However, the other Ps of marketing will contribute to decreasing price elasticity and so enable price increases to drive greater revenue and profits.
Pricing can be a manual or automatic process of applying prices to purchase and sales orders, based on factors such as: a fixed amount, quantity break, promotion or sales campaign, specific vendor quote, price prevailing on entry, shipment or invoice date, combination of multiple orders or lines, and many others. An automated pricing system requires more setup and maintenance but may prevent pricing errors. The needs of the consumer can be converted into demand only if the consumer has the willingness and capacity to buy the product. Thus, pricing is the most important concept in the field of marketing, it is used as a tactical decision in response to changing competitive, market and organizational situations.",Pricing,0,1
1105,65910,"A printed circuit board (PCB) or printed wiring board (PWB) is a laminated sandwich structure of conductive and insulating layers. PCBs have two complementary functions. The first is to affix electronic components in designated locations on the outer layers by means of soldering. The second is to provide reliable electrical connections (and also reliable open circuits) between the component's terminals in a controlled manner often referred to as PCB design. Each of the conductive layers is designed with an artwork pattern of conductors (similar to wires on a flat surface) that provides electrical connections on that conductive layer. Another manufacturing process adds vias, plated-through holes that allow interconnections between layers. 
PCBs mechanically support electronic components using conductive pads in the shape designed to accept the component's terminals, and also electrically connect them using traces, planes and other features etched from one or more sheet layers of copper laminated onto and/or between sheet layers of a non-conductive substrate. Components are generally soldered onto the PCB to both electrically connect and mechanically fasten them to it. Printed circuit boards are used in nearly all electronic products and in some electrical products, such as passive switch boxes.
Alternatives to PCBs include wire wrap and point-to-point construction, both once popular but now rarely used. PCBs require additional design effort to lay out the circuit, but manufacturing and assembly can be automated. Electronic computer-aided design software is available to do much of the work of layout. Mass-producing circuits with PCBs is cheaper and faster than with other wiring methods, as components are mounted and wired in one operation.  Large numbers of PCBs can be fabricated at the same time, and the layout only has to be done once.  PCBs can also be made manually in small quantities, with reduced benefits.PCBs can be single-sided (one copper layer), double-sided (two copper layers on both sides of one substrate layer), or multi-layer (outer and inner layers of copper, alternating with layers of substrate).  Multi-layer PCBs allow for much higher component density, because circuit traces on the inner layers would otherwise take up surface space between components. The rise in popularity of multilayer PCBs with more than two, and especially with more than four, copper planes was concurrent with the adoption of surface mount technology.  However, multilayer PCBs make repair, analysis, and field modification of circuits much more difficult and usually impractical.
The world market for bare PCBs exceeded $60.2 billion in 2014 and is estimated to reach $79 billion by 2024.",Printed circuit board,0,1
1106,42378348,"In machine learning, a probabilistic classifier is a classifier that is able to predict, given an observation of an input, a probability distribution over a set of classes, rather than only outputting the most likely class that the observation should belong to. Probabilistic classifiers provide classification that can be useful in its own right or when combining classifiers into ensembles.",Probabilistic classification,0,1
1107,5017608,"Probabilistic logic (also probability logic and probabilistic reasoning) involves the use of probability and logic to deal with uncertain situations. Compared to traditional logic, probabilistic logics result in a richer and more expressive formalism with a broad range of possible application areas. Probabilistic logics attempt to find a natural extension of traditional logic truth tables: the results they define are derived through probabilistic expressions instead. A difficulty of probabilistic logics is their tendency to multiply the computational complexities of their probabilistic and logical components.  Other difficulties include the possibility of counter-intuitive results, such as in case of belief fusion in Dempster–Shafer theory. Source trust and epistemic uncertainty about the probabilities they provide, such as defined in subjective logic, are additional elements to consider. The need to deal with a broad variety of contexts and issues has led to many different proposals.",Probabilistic logic,0,1
1110,380008,"In computational learning theory, probably approximately correct (PAC) learning is a framework for mathematical analysis of machine learning. It was proposed in 1984 by Leslie Valiant.In this framework, the learner receives samples and must select a generalization function (called the hypothesis) from a certain class of possible functions. The goal is that, with high probability (the ""probably"" part), the selected function will have low generalization error (the ""approximately correct"" part). The learner must be able to learn the concept given any arbitrary approximation ratio, probability of success, or distribution of the samples.
The model was later extended to treat noise (misclassified samples).
An important innovation of the PAC framework is the introduction of computational complexity theory concepts to machine learning. In particular, the learner is expected to find efficient functions (time and space requirements bounded to a polynomial of the example size), and the learner itself must implement an efficient procedure (requiring an example count bounded to a polynomial of the concept size, modified by the approximation and likelihood bounds).",Probably approximately correct learning,0,1
1111,658183,"An industrial process control in continuous production processes is a discipline that uses industrial control systems  to achieve a production level of consistency, economy and safety which could not be achieved purely by human manual control. It is implemented widely in industries such as automotive, mining, dredging, oil refining, pulp and paper manufacturing, chemical processing and power generating plants.There is a wide range of size, type and complexity, but it enables a small number of operators to manage complex processes to a high degree of consistency. The development of large industrial process control systems was instrumental in enabling the design of large high volume and complex processes, which could not be otherwise economically or safely operated.The applications can range from controlling the temperature and level of a single process vessel, to a complete chemical processing plant with several thousand control loops.

",Process control,0,1
1112,276598,"Product placement, also known as embedded marketing, is a marketing technique where references to specific brands or products are incorporated into another work, such as a film or television program, with specific promotional intent.  Much of this is done by loaning products, especially when expensive items, such as vehicles, are involved.  In 2021, the agreements between brand owners and films and television programs were worth more than US$20 billion.While references to brands (real or fictional) may be voluntarily incorporated into works to maintain a feeling of realism or be a subject of commentary, product placement is the deliberate incorporation of references to a brand or product in exchange for compensation. Product placements may range from unobtrusive appearances within an environment, to prominent integration and acknowledgement of the product within the work. Common categories of products used for placements include automobiles and consumer electronics. Works produced by vertically integrated companies (such as Sony) may use placements to promote their other divisions as a form of corporate synergy.

During the 21st century, the use of product placement on television has grown, particularly to combat the wider use of digital video recorders that can skip traditional commercial breaks, as well as to engage with younger demographics. Digital editing  technology is also being used to tailor product placement to specific demographics or markets, and in some cases, add placements to works that did not originally have embedded advertising, or update existing placements.",Product placement,0,1
1113,23015,"A programming language is any set of rules that converts strings, or graphical program elements in the case of visual programming languages, to various kinds of machine code output. Programming languages are one kind of computer language, and are used in computer programming to implement algorithms.
Most programming languages consist of instructions for computers. There are programmable machines that use a set of specific instructions, rather than general programming languages. Since the early 1800s, programs have been used to direct the behavior of machines such as Jacquard looms, music boxes and player pianos.Thousands of different programming languages have been created, and more are being created every year. Many programming languages are written in an imperative form (i.e., as a sequence of operations to perform) while other languages use the declarative form (i.e. the desired result is specified, not how to achieve it).
The description of a programming language is usually split into the two components of syntax (form) and semantics (meaning), which are usually defined by a formal language. Some languages are defined by a specification document (for example, the C programming language is specified by an ISO Standard) while other languages (such as Perl) have a dominant implementation that is treated as a reference. Some languages have both, with the basic language defined by a standard and extensions taken from the dominant implementation being common. 
Programming language theory is a subfield of computer science that deals with the design, implementation, analysis, characterization, and classification of programming languages.",Programming language,0,1
1114,3882218,"Programming language theory (PLT) is a branch of computer science that deals with the design, implementation, analysis, characterization, and classification of formal languages known as programming languages and of their individual features. It falls within the discipline of computer science, both depending on and affecting mathematics, software engineering, linguistics and even cognitive science. It has become a well-recognized branch of computer science, and an active research area, with results published in numerous journals dedicated to PLT, as well as in general computer science and engineering publications.

",Programming language theory,0,1
1115,189897,"Programming paradigms are a way to classify programming languages based on their features. Languages can be classified into multiple paradigms.
Some paradigms are concerned mainly with implications for the execution model of the language, such as allowing side effects, or whether the sequence of operations is defined by the execution model.  Other paradigms are concerned mainly with the way that code is organized, such as grouping a code into units along with the state that is modified by the code.  Yet others are concerned mainly with the style of syntax and grammar.
Common programming paradigms include:
imperative in which the programmer instructs the machine how to change its state,
procedural which groups instructions into procedures,
object-oriented which groups instructions with the part of the state they operate on,
declarative in which the programmer merely declares properties of the desired result, but not how to compute it
functional in which the desired result is declared as the value of a series of function applications,
logic in which the desired result is declared as the answer to a question about a system of facts and rules,
mathematical in which the desired result is declared as the solution of an optimization problem
reactive in which the desired result is declared with data streams and the propagation of changeSymbolic techniques such as reflection, which allow the program to refer to itself, might also be considered as a programming paradigm. However, this is compatible with the major paradigms and thus is not a real paradigm in its own right.
For example, languages that fall into the imperative paradigm have two main features: they state the order in which operations occur, with constructs that explicitly control that order, and they allow side effects, in which state can be modified at one point in time, within one unit of code, and then later read at a different point in time inside a different unit of code.  The communication between the units of code is not explicit.  Meanwhile, in object-oriented programming, code is organized into objects that contain a state that is only modified by the code that is part of the object.  Most object-oriented languages are also imperative languages.  In contrast, languages that fit the declarative paradigm do not state the order in which to execute operations.  Instead, they supply a number of available operations in the system, along with the conditions under which each is allowed to execute.  The implementation of the language's execution model tracks which operations are free to execute and chooses the order independently. More at  Comparison of multi-paradigm programming languages.",Programming paradigm,0,1
1116,26615065,"A programming team is a team of people who develop or maintain computer software.  They may be organised in numerous ways, but the egoless programming team and chief programmer team have been common structures.",Programming team,0,1
1117,370882,"A programming tool or software development tool is a computer program that software developers use to create, debug, maintain, or otherwise support other programs and applications. The term usually refers to relatively simple programs, that can be combined to accomplish a task, much as one might use multiple hands to fix a physical object. The most basic tools are a source code editor and a compiler or interpreter, which are used ubiquitously and continuously. Other tools are used more or less depending on the language, development methodology, and individual engineer, often used for a discrete task, like a debugger or profiler. Tools may be discrete programs, executed separately – often from the command line – or may be parts of a single large program, called an integrated development environment (IDE). In many cases, particularly for simpler use, simple ad hoc techniques are used instead of a tool, such as print debugging instead of using a debugger, manual timing (of overall program or section of code) instead of a profiler, or tracking bugs in a  text file or spreadsheet instead of a bug tracking system.
The distinction between tools and applications is murky. For example, developers use simple databases (such as a file containing a list of important values) all the time as tools. However a full-blown database is usually thought of as an application or software in its own right. For many years, computer-assisted software engineering (CASE) tools were sought after. Successful tools have proven elusive. In one sense, CASE tools emphasized design and architecture support, such as for UML. But the most successful of these tools are IDEs.",Programming tool,0,1
1118,16598232,"Artificial intelligence applications have been used in a wide range of fields including medical diagnosis, stock trading, robot control, law, scientific discovery, video games, and toys.  However, many AI applications are not perceived as AI:  ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore."" ""Many thousands of AI applications are deeply embedded in the infrastructure of every industry."" In the late 1990s and early 21st century, AI technology became widely used as elements of larger systems, but the field was rarely credited for these successes at the time.
Kaplan and Haenlein structure artificial intelligence along three evolutionary stages: 1) artificial narrow intelligence – applying AI only to specific tasks; 2) artificial general intelligence – applying AI to several areas and able to autonomously solve problems they were never even designed for; and 3) artificial super intelligence – applying AI to any area capable of scientific creativity, social skills, and general wisdom.To allow comparison with human performance, artificial intelligence can be evaluated on constrained and well-defined problems.  Such tests have been termed subject matter expert Turing tests.  Also, smaller problems provide more achievable goals and there are an ever-increasing number of positive results.",Progress in artificial intelligence,0,1
1119,57719367,"Project Debater is an IBM artificial intelligence project, designed to participate in a full live debate with expert human debaters. It follows on from the Watson project which played Jeopardy!",Project Debater,0,1
1120,55330205,"The proper generalized decomposition (PGD) is an iterative numerical method for solving boundary value problems (BVPs), that is, partial differential equations constrained by a set of boundary conditions, such as the Poisson's equation or the Laplace's equation.
The PGD algorithm computes an approximation of the solution of the BVP by successive enrichment. This means that, in each iteration, a new component (or mode) is computed and added to the approximation. In principle, the more modes obtained, the closer the approximation is to its theoretical solution. Unlike POD principal components, PGD modes are not necessarily orthogonal to each other.
By selecting only the most relevant PGD modes, a reduced order model of the solution is obtained. Because of this, PGD is considered a dimensionality reduction algorithm.",Proper generalized decomposition,0,1
1121,49761,"Punched tape or perforated paper tape is a form of data storage that consists of a long strip of paper in which holes are punched. It developed from and was subsequently used alongside punched cards, differing in that the tape is continuous.
Punched cards, and chains of punched cards, were used for control of looms in the 18th century. Use for telegraphy systems started in 1842. Punched tape was used throughout the 19th and for much of the 20th centuries for programmable looms, teleprinter communication, for input to computers of the 1950s and 1960s, and later as a storage medium for minicomputers and CNC machine tools. During the Second World War, high-speed punched tape systems using optical readout methods were used in code breaking systems. Punched tape was used to transmit data for manufacture of read-only memory chips.",Punched tape,0,1
1122,30051,"A torch is a stick with combustible material at one end, which is ignited and used as a light source. Torches have been used throughout history, and are still used in processions, symbolic and religious events, and in juggling entertainment. In some countries ""torch"" in modern usage is the term for a battery-operated portable light.",Torch,0,1
1123,23862,"Python is a high-level, interpreted, general-purpose programming language. Its design philosophy emphasizes code readability with the use of significant indentation.Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly procedural), object-oriented and functional programming. It is often described as a ""batteries included"" language due to its comprehensive standard library.Guido van Rossum began working on Python in the late 1980s as a successor to the ABC programming language and first released it in 1991 as Python 0.9.0. Python 2.0 was released in 2000 and introduced new features such as list comprehensions, cycle-detecting garbage collection, reference counting, and Unicode support. Python 3.0, released in 2008, was a major revision that is not completely backward-compatible with earlier versions. Python 2 was discontinued with version 2.7.18 in 2020.Python consistently ranks as one of the most popular programming languages.

",Python (programming language),0,1
1124,1281850,"Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. It does not require a model of the environment (hence ""model-free""), and it can handle problems with stochastic transitions and rewards without requiring adaptations.
For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value of the total reward over any and all successive steps, starting from the current state. Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy. ""Q"" refers to the function that the algorithm computes – the expected rewards for an action taken in a given state.

",Q-learning,0,1
1125,44300158,"Root Cause Analysis Solver Engine (informally RCASE) is a proprietary algorithm developed from research originally at the Warwick Manufacturing Group (WMG) at Warwick University. RCASE development commenced in 2003 to provide an automated version of root cause analysis, the method of problem solving that tries to identify the root causes of faults or problems. RCASE is now owned by the spin-out company Warwick Analytics where it is being applied to automated predictive analytics software.",RCASE,0,1
1126,33283283,"Rakesh Agrawal is a computer scientist who until recently was a Technical Fellow at the Microsoft Search Labs. Rakesh is well known for developing fundamental data mining concepts and technologies and pioneering key concepts in data privacy, including Hippocratic Database, Sovereign Information Sharing, and Privacy-Preserving Data Mining. IBM's commercial data mining product, Intelligent Miner, grew out of his work. His research has been incorporated into other IBM products, including DB2 Mining Extender, DB2 OLAP Server and WebSphere Commerce Server, and has influenced several other commercial and academic products, prototypes and applications. His other technical contributions include Polyglot object-oriented type system, Alert active database system, Ode (Object database and environment), Alpha (extension of relational databases with generalized transitive closure), Nest distributed system, transaction management, and database machines.
Prior to joining Microsoft in March 2006, Rakesh was an IBM Fellow and led the Quest group at the IBM Almaden Research Center. Earlier, he was with the Bell Laboratories, Murray Hill from 1983 to 1989. He also worked for three years at a leading Indian company, the Bharat Heavy Electricals Ltd. He received his M.S. and Ph.D. degrees in Computer Science from the University of Wisconsin-Madison in 1983. He also holds a B.E. degree in Electronics and Communication Engineering from IIT-Roorkee, and a two-year Post Graduate Diploma in Industrial Engineering from the National Institute of Industrial Engineering (NITIE), Bombay.",Rakesh Agrawal (computer scientist),0,1
1127,1363880,"Random forests or random decision forests is an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time. For classification tasks, the output of the random forest is the class selected by most trees. For regression tasks, the mean or average prediction of the individual trees is returned. Random decision forests correct for decision trees' habit of overfitting to their training set.: 587–588  Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees. However, data characteristics can affect their performance.The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, is a way to implement the ""stochastic discrimination"" approach to classification proposed by Eugene Kleinberg.An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered ""Random Forests"" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). The extension combines Breiman's ""bagging"" idea and random selection of features, introduced first by Ho and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.
Random forests are frequently used as ""blackbox"" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration.",Random forest,0,1
1129,495383,"A randomized algorithm is an algorithm that employs a degree of randomness as part of its logic or procedure. The algorithm typically uses uniformly random bits as an auxiliary input to guide its behavior, in the hope of achieving good performance in the ""average case"" over all possible choices of random determined by the random bits; thus either the running time, or the output (or both) are random variables.
One has to distinguish between algorithms that use the random input so that they always terminate with the correct answer, but where the expected running time is finite (Las Vegas algorithms, for example Quicksort), and algorithms which have a chance of producing an incorrect result (Monte Carlo algorithms, for example the Monte Carlo algorithm for the MFAS problem) or fail to produce a result either by signaling a failure or failing to terminate. In some cases, probabilistic algorithms are the only practical means of solving a problem.In common practice, randomized algorithms are approximated using a pseudorandom number generator in place of a true source of random bits; such an implementation may deviate from the expected theoretical behavior and mathematical guarantees which may depend on the existence of an ideal true random number generator.

",Randomized algorithm,0,1
1130,5100865,"RapidMiner is a data science platform designed for enterprises that analyses the collective impact of organizations’ employees, expertise and data. Rapid Miner's data science platform is intended to support many analytics users across a broad AI lifecycle. Rapid Miner claims that more than 1 million people use the platform.",RapidMiner,0,1
1131,402673,"Ray Solomonoff (July 25, 1926 – December 7, 2009) was the inventor of algorithmic probability, his General Theory of Inductive Inference (also known as Universal Inductive Inference),  and was a founder of algorithmic information theory. He was an originator of the branch of artificial intelligence based on machine learning, prediction and probability. He circulated the first report on non-semantic machine learning in 1956.Solomonoff first described algorithmic probability in 1960, publishing the theorem that launched Kolmogorov complexity and algorithmic information theory.  He first described these results at a conference at Caltech in 1960, and in a report, Feb. 1960, ""A Preliminary Report on a General Theory of Inductive Inference."" He clarified these ideas more fully in his 1964 publications, ""A Formal Theory of Inductive Inference,"" Part I and Part II.Algorithmic probability is a mathematically formalized combination of Occam's razor, and the Principle of Multiple Explanations.
It is a machine independent method of assigning a probability value to each hypothesis (algorithm/program) that explains a given observation, with the simplest hypothesis (the shortest program) having the highest probability and the increasingly complex hypotheses receiving increasingly small probabilities.
Solomonoff founded the theory of universal inductive inference, which is based on solid philosophical foundations and has its root in Kolmogorov complexity and algorithmic information theory. The theory uses algorithmic probability in a Bayesian framework. The universal prior is taken over the class of all computable measures; no hypothesis will have a zero probability. This enables Bayes' rule (of causation) to be used to predict the most likely next event in a series of events, and how likely it will be.Although he is best known for algorithmic probability and his general theory of inductive inference, he made many other important discoveries throughout his life, most of them directed toward his goal in artificial intelligence: to develop a machine that could solve hard problems using probabilistic methods.

",Ray Solomonoff,0,1
1132,248454,"The Raytheon Company was a major U.S. defense contractor and industrial corporation with manufacturing concentrations in weapons and military and commercial electronics. It was previously involved in corporate and special-mission aircraft until early 2007. Raytheon was the world's largest producer of guided missiles. In April 2020, the company merged with United Technologies Corporation to form Raytheon Technologies.Established in 1922, the company reincorporated in 1928 and adopted its present name in 1959. During 2018, the company had around 67,000 employees worldwide and annual revenues of approximately US$25.35 billion. More than 90% of Raytheon's revenues were obtained from military contracts and, as of 2012, it was the fifth-largest military contractor in the world. As of 2015, it was the third largest defense contractor in the United States by defense revenue.In 2003, Raytheon's headquarters moved from Lexington, Massachusetts, to Waltham, Massachusetts. The company had previously been headquartered in Cambridge, Massachusetts, from 1922 to 1928, Newton, Massachusetts, from 1928 to 1941, Waltham from 1941 to 1961 and Lexington from 1961 to 2003.

",Raytheon Company,0,1
1133,25767,"Real-time computing (RTC) is the computer science term for hardware and software systems subject to a ""real-time constraint"", for example from event to system response. Real-time programs must guarantee response within specified time constraints, often referred to as ""deadlines"".Real-time responses are often understood to be in the order of milliseconds, and sometimes microseconds. A system not specified as operating in real time cannot usually guarantee a response within any timeframe, although typical or expected response times may be given. Real-time processing fails if not completed within a specified deadline relative to an event; deadlines must always be met, regardless of system load.
A real-time system has been described as one which ""controls an environment by receiving data, processing them, and returning the results sufficiently quickly to affect the environment at that time"". The term ""real-time"" is also used in simulation to mean that the simulation's clock runs at the same speed as a real clock, and in process control and enterprise systems to mean ""without significant delay"".
Real-time software may use one or more of the following: synchronous programming languages, real-time operating systems (RTOSes), and real-time networks, each of which provide essential frameworks on which to build a real-time software application.
Systems used for many safety-critical applications must be real-time, such as for control of fly-by-wire aircraft, or anti-lock brakes, both of which demand immediate and accurate mechanical response.

",Real-time computing,0,1
1134,20646438,"In mathematics, a real number is a value of a continuous quantity that can represent a distance along a line (or alternatively, a quantity that can be represented as an infinite decimal expansion). The adjective real in this context was introduced in the 17th century by René Descartes, who distinguished between real and imaginary roots of polynomials. The real numbers include all the rational numbers, such as the integer −5 and the fraction 4/3, and all the irrational numbers, such as 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
   (1.41421356..., the square root of 2, an irrational algebraic number). Included within the irrationals are the real transcendental numbers, such as π (3.14159265...).  In addition to measuring distance, real numbers can be used to measure quantities such as time, mass, energy, velocity, and many more. The set of real numbers is denoted using the symbol R or 
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   and is sometimes called ""the reals"".Real numbers can be thought of as points on an infinitely long line called the number line or real line, where the points corresponding to integers are equally spaced. Any real number can be determined by a possibly infinite decimal representation, such as that of 8.632, where each consecutive digit is measured in units one-tenth the size of the previous one. The real line can be thought of as a part of the complex plane, and the real numbers can be thought of as a part of the complex numbers.

These descriptions of the real numbers are not sufficiently rigorous by the modern standards of pure mathematics. The discovery of a suitably rigorous definition of the real numbers—indeed, the realization that a better definition was needed—was one of the most important developments of 19th-century mathematics. The current standard axiomatic definition is that real numbers form the unique Dedekind-complete ordered field (
  
    
      
        
          R
        
      
    
    {\displaystyle \mathbb {R} }
   ; + ; · ; <), up to an isomorphism, whereas popular constructive definitions of real numbers include declaring them as equivalence classes of Cauchy sequences (of rational numbers), Dedekind cuts, or infinite decimal representations, together with precise interpretations for the arithmetic operations and the order relation. All these definitions satisfy the axiomatic definition and are thus equivalent.
The set of all real numbers is uncountable, in the sense that while both the set of all natural numbers and the set of all real numbers are infinite sets, there can be no one-to-one function from the real numbers to the natural numbers. In fact, the cardinality of the set of all real numbers, denoted by 
  
    
      
        
          
            c
          
        
      
    
    {\displaystyle {\mathfrak {c}}}
   and called the cardinality of the continuum, is strictly greater than the cardinality of the set of all natural numbers (denoted 
  
    
      
        
          ℵ
          
            0
          
        
      
    
    {\displaystyle \aleph _{0}}
  , 'aleph-naught').
The statement that there is no subset of the reals with cardinality strictly greater than 
  
    
      
        
          ℵ
          
            0
          
        
      
    
    {\displaystyle \aleph _{0}}
   and strictly smaller than 
  
    
      
        
          
            c
          
        
      
    
    {\displaystyle {\mathfrak {c}}}
   is known as the continuum hypothesis (CH). It is neither provable nor refutable using the axioms of Zermelo–Fraenkel set theory including the axiom of choice (ZFC)—the standard foundation of modern mathematics. In fact, some models of ZFC satisfy CH, while others violate it.",Real number,1,1
1136,922505,"A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied.  The method was originally developed for operators of military radar receivers starting in 1941, which led to its name.
The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, recall or probability of detection. The false-positive rate is also known as probability of false alarm and can be calculated as (1 − specificity). It can also be thought of as a plot of the power as a function of the Type I Error of the decision rule (when the performance is calculated from just a sample of the population, it can be thought of as estimators of these quantities). The ROC curve is thus the sensitivity or recall as a function of fall-out. In general, if the probability distributions for both detection and false alarm are known, the ROC curve can be generated by plotting the cumulative distribution function (area under the probability distribution from 
  
    
      
        −
        ∞
      
    
    {\displaystyle -\infty }
   to the discrimination threshold) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis.
ROC analysis provides tools to select possibly optimal models and to discard suboptimal ones independently from (and prior to specifying) the cost context or the class distribution. ROC analysis is related in a direct and natural way to cost/benefit analysis of diagnostic decision making.
The ROC curve was first developed by electrical engineers and radar engineers during World War II for detecting enemy objects in battlefields and was soon introduced to psychology to account for perceptual detection of stimuli. ROC analysis since then has been used in medicine, radiology, biometrics, forecasting of natural hazards, meteorology, model performance assessment, and other areas for many decades and is increasingly used in machine learning and data mining research.
The ROC is also known as a relative operating characteristic curve, because it is a comparison of two operating characteristics (TPR and FPR) as the criterion changes.

",Receiver operating characteristic,1,1
1137,596646,"A recommender system, or a recommendation system (sometimes replacing 'system' with a synonym such as platform or engine), is a subclass of information filtering system that provide suggestions for items that are most pertinent to a particular user. Typically, the suggestions refer to various decision-making processes, such as what product to purchase, what music to listen to, or what online news to read. Recommender systems are particularly useful when an individual needs to choose an item from a potentially overwhelming number of items that a service may offer.Recommender systems are used in a variety of areas, with commonly recognised examples taking the form of playlist generators for video and music services, product recommenders for online stores, or content recommenders for social media platforms and open web content recommenders. These systems can operate using a single input, like music, or multiple inputs within and across platforms like news, books and search queries. There are also popular recommender systems for specific topics like restaurants and online dating. Recommender systems have also been developed to explore research articles and experts, collaborators, and financial services.",Recommender system,0,1
1138,37862937,"In the context of artificial neural networks, the rectifier or ReLU (Rectified Linear Unit) activation function is an activation function defined as the positive part of its argument:

  
    
      
        f
        (
        x
        )
        =
        
          x
          
            +
          
        
        =
        max
        (
        0
        ,
        x
        )
      
    
    {\displaystyle f(x)=x^{+}=\max(0,x)}
  where x is the input to a neuron. This is also known as a ramp function and is analogous to half-wave rectification in electrical engineering.
This activation function started showing up in the context of visual feature extraction in hierarchical neural networks starting in the late 1960s. It was later argued that it  has  strong biological motivations and mathematical justifications. In 2011 it was found to enable better training of deeper networks, compared to the widely used activation functions prior to 2011, e.g., the logistic sigmoid (which is inspired by probability theory; see logistic regression) and its more practical counterpart, the hyperbolic tangent. The rectifier is, as of 2017, the most popular activation function for deep neural networks.Rectified linear units find applications in computer vision and speech recognition using deep neural nets and computational neuroscience.",Rectifier (neural networks),0,1
1139,1706303,"A recurrent neural network (RNN) is a class of artificial neural networks where connections between nodes form a directed or undirected graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Derived from feedforward neural networks, RNNs can use their internal state (memory) to process variable length sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. Recurrent neural networks are theoretically Turing complete and can run arbitrary programs to process arbitrary sequences of inputs.The term ""recurrent neural network"" is used to refer to the class of networks with an infinite impulse response, whereas ""convolutional neural network"" refers to the class of finite impulse response. Both classes of networks exhibit temporal dynamic behavior. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network, while an infinite impulse recurrent network is a directed cyclic graph that can not be unrolled.
Both finite impulse and infinite impulse recurrent networks can have additional stored states, and the storage can be under direct control by the neural network. The storage can also be replaced by another network or graph if that incorporates time delays or has feedback loops. Such controlled states are referred to as gated state or gated memory, and are part of long short-term memory networks (LSTMs) and gated recurrent units. This is also called Feedback Neural Network (FNN).",Recurrent neural network,0,1
1141,2009061,"In mathematics, statistics, finance, computer science, particularly in machine learning and inverse problems, regularization is a process that changes the result answer to be ""simpler"". It is often used to obtain results for ill-posed problems or to prevent overfitting.Although regularization procedures can be divided in many ways, one particular delineation is particularly helpful:

Explicit regularization is regularization whenever one explicitly adds a term to the optimization problem.  These terms could be priors, penalties, or constraints. Explicit regularization is commonly employed with ill-posed optimization problems. The regularization term, or penalty, imposes a cost on the optimization function to make the optimal solution unique.
Implicit regularization is all other forms of regularization.  This includes, for example, early stopping, using a robust loss function, and discarding outliers. Implicit regularization is essentially ubiquitous in modern machine learning approaches, including stochastic gradient descent for training deep neural networks, and ensemble methods (such as random forests and gradient boosted trees).In explicit regularization, independent of the problem or model, there is always a data term, that corresponds to a likelihood of the measurement and a regularization term that corresponds to a prior. By combining both using Bayesian statistics, one can compute a posterior, that includes both information sources and therefore stabilizes the estimation process. By trading off both objectives, one chooses to be more addictive to the data or to enforce generalization (to prevent overfitting). There is a whole research branch dealing with all possible regularizations. The work flow usually is, that one tries a specific regularization and then figures out the probability density that corresponds to that regularization to justify the choice. It can also be physically motivated by common sense or intuition.
In machine learning, the data term corresponds to the training data and the regularization is either the choice of the model or modifications to the algorithm. It is always intended to reduce the generalization error, i.e. the error score with the trained model on the evaluation set and not the training data.One of the earliest uses of regularization is Tikhonov regularization, related to the method of least squares.",Regularization (mathematics),0,1
1142,66294,"Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought to take actions in an environment in order to maximize the notion of cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.
Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge). Partially supervised RL algorithms can combine the advantages of supervised and RL algorithms.
The environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques. The main difference between the classical dynamic programming methods  and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.",Reinforcement learning,0,1
1143,4195092,"In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.
The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
It is actually equivalent to a Gaussian process model with covariance function:

  
    
      
        k
        (
        
          x
        
        ,
        
          
            x
            ′
          
        
        )
        =
        
          ∑
          
            j
            =
            1
          
          
            N
          
        
        
          
            1
            
              α
              
                j
              
            
          
        
        φ
        (
        
          x
        
        ,
        
          
            x
          
          
            j
          
        
        )
        φ
        (
        
          
            x
          
          ′
        
        ,
        
          
            x
          
          
            j
          
        
        )
      
    
    {\displaystyle k(\mathbf {x} ,\mathbf {x'} )=\sum _{j=1}^{N}{\frac {1}{\alpha _{j}}}\varphi (\mathbf {x} ,\mathbf {x} _{j})\varphi (\mathbf {x} ',\mathbf {x} _{j})}
  where 
  
    
      
        φ
      
    
    {\displaystyle \varphi }
   is the kernel function (usually Gaussian), 
  
    
      
        
          α
          
            j
          
        
      
    
    {\displaystyle \alpha _{j}}
   are the variances of the prior on the weight vector

  
    
      
        w
        ∼
        N
        (
        0
        ,
        
          α
          
            −
            1
          
        
        I
        )
      
    
    {\displaystyle w\sim N(0,\alpha ^{-1}I)}
  , and 
  
    
      
        
          
            x
          
          
            1
          
        
        ,
        …
        ,
        
          
            x
          
          
            N
          
        
      
    
    {\displaystyle \mathbf {x} _{1},\ldots ,\mathbf {x} _{N}}
   are the input vectors of the training set.Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, which are guaranteed to find a global optimum (of the convex problem).
The relevance vector machine was patented in the United States by Microsoft (patent expired September 4, 2019).",Relevance vector machine,0,1
1144,6604,"Rendering or image synthesis is the process of generating a photorealistic or non-photorealistic image from a 2D or 3D model by means of a computer program.  The resulting image is referred to as the render.  Multiple models can be defined in a scene file containing objects in a strictly defined language or data structure.  The scene file contains geometry, viewpoint, texture, lighting, and shading information describing the virtual scene. The data contained in the scene file is then passed to a rendering program to be processed and output to a digital image or raster graphics image file. The term ""rendering"" is analogous to the concept of an artist's impression of a scene.  The term ""rendering"" is also used to describe the process of calculating effects in a video editing program to produce the final video output.
Rendering is one of the major sub-topics of 3D computer graphics, and in practice it is always connected to the others. It is the last major step in the graphics pipeline, giving models and animation their final appearance. With the increasing sophistication of computer graphics since the 1970s, it has become a more distinct subject.
Rendering has uses in architecture, video games, simulators, movie and TV visual effects, and design visualization, each employing a different balance of features and techniques. A wide variety of renderers are available for use. Some are integrated into larger modeling and animation packages, some are stand-alone, and some are free open-source projects. On the inside, a renderer is a carefully engineered program based on multiple disciplines, including light physics, visual perception, mathematics, and software development.
Though the technical details of rendering methods vary, the general challenges to overcome in producing a 2D image on a screen from a 3D representation stored in a scene file are handled by the graphics pipeline in a rendering device such as a GPU. A GPU is a purpose-built device that assists a CPU in performing complex rendering calculations. If a scene is to look relatively realistic and predictable under virtual lighting, the rendering software must solve the rendering equation. The rendering equation doesn't account for all lighting phenomena, but instead acts as a general lighting model for computer-generated imagery.
In the case of 3D graphics, scenes can be pre-rendered or generated in realtime.  Pre-rendering is a slow, computationally intensive process that is typically used for movie creation, where scenes can be generated ahead of time, while real-time rendering is often done for 3D video games and other applications that must dynamically create scenes.  3D hardware accelerators can improve realtime rendering performance.

",Rendering (computer graphics),0,1
1145,522449,"In systems engineering and software engineering, requirements analysis focuses on the tasks that determine the needs or conditions to meet the new or altered product or project, taking account of the possibly conflicting requirements of the various stakeholders, analyzing, documenting, validating and managing software or system requirements.Requirements analysis is critical to the success or failure of a systems or software project. The requirements should be documented, actionable, measurable, testable, traceable, related to identified business needs or opportunities, and defined to a level of detail sufficient for system design.",Requirements analysis,0,1
1146,10667750,"Reservoir computing is a framework for computation derived from recurrent neural network theory that maps input signals into higher dimensional computational spaces through the dynamics of a fixed, non-linear system called a reservoir. After the input signal is fed into the reservoir, which is treated as a ""black box,"" a simple readout mechanism is trained to read the state of the reservoir and map it to the desired output. The first key benefit of this framework is that training is performed only at the readout stage, as the reservoir dynamics are fixed. The second is that the computational power of naturally available systems, both classical and quantum mechanical, can be used to reduce the effective computational cost.

",Reservoir computing,0,1
1147,33742232,"A restricted Boltzmann machine (RBM) is a generative stochastic artificial neural network that can learn a probability distribution over its set of inputs.
RBMs were initially invented under the name Harmonium by Paul Smolensky in 1986,
and rose to prominence after Geoffrey Hinton and collaborators invented fast learning algorithms for them in the mid-2000. RBMs have found applications in dimensionality reduction,classification,collaborative filtering,  feature learning,topic modelling
and even many body quantum mechanics. They can be trained in either supervised or unsupervised ways, depending on the task.
As their name implies, RBMs are a variant of Boltzmann machines, with the restriction that their neurons must form a bipartite graph: 
a pair of nodes from each of the two groups of units (commonly referred to as the ""visible"" and ""hidden"" units respectively) may have a symmetric connection between them; and there are no connections between nodes within a group. By contrast, ""unrestricted"" Boltzmann machines may have connections between hidden units. This restriction allows for more efficient training algorithms than are available for the general class of Boltzmann machines, in particular the gradient-based contrastive divergence algorithm.Restricted Boltzmann machines can also be used in deep learning networks. In particular, deep belief networks can be formed by ""stacking"" RBMs and optionally fine-tuning the resulting deep network with gradient descent and backpropagation.",Restricted Boltzmann machine,0,1
1148,21280907,"Richard O. Duda  is Professor Emeritus of Electrical Engineering at San Jose State University renowned for his work on sound localization and pattern recognition. He lives in Menlo Park, California.",Richard O. Duda,0,1
1149,954328,"Ridge regression is a method of estimating the coefficients of multiple-regression models in scenarios where the independent variables are highly correlated. It has been used in many fields including econometrics, chemistry, and engineering.The theory was first introduced by Hoerl and Kennard in 1970 in their Technometrics papers “RIDGE regressions: biased estimation of nonorthogonal problems” and “RIDGE regressions: applications in nonorthogonal problems”. This was the result of ten years of research into the field of ridge analysis.Ridge regression was developed as a possible solution to the imprecision of least square estimators when linear regression models have some multicollinear (highly correlated) independent variables—by creating a ridge regression estimator (RR). This provides a more precise ridge parameters estimate, as its variance and mean square estimator are often smaller than the least square estimators previously derived.

",Ridge regression,0,1
1150,31404681,"Robert Tibshirani  (born July 10, 1956) is a professor in the Departments of Statistics and Biomedical Data Science at Stanford University. He was a professor at the University of Toronto from 1985 to 1998. In his work, he develops statistical tools for the analysis of complex datasets, most recently in genomics and proteomics.
His most well-known contributions are the
Lasso method, which proposed the use of  L1  penalization in regression and related problems, and Significance Analysis of Microarrays.",Robert Tibshirani,0,1
1151,175885,"Robotic control is the system that contributes to the movement of robots. This involves the mechanical aspects and programmable systems that makes it possible to control robots. Robotics could be controlled in various ways, which includes using manual control, wireless control, semi-autonomous (which is a mix of fully automatic and wireless control), and fully autonomous (which is when it uses artificial intelligence to move on its own, but there could be options to make it manually controlled). In the present day, as technological advancements progress, robots and their methods of control continue to develop and advance.",Robot control,0,1
1152,3290880,"Robot learning is a research field at the intersection of machine learning and robotics. It studies techniques allowing a robot to acquire novel skills or adapt to its environment through learning algorithms. The embodiment of the robot, situated in a physical embedding, provides at the same time specific difficulties (e.g. high-dimensionality, real time constraints for collecting data and learning) and opportunities for guiding the learning process (e.g. sensorimotor synergies, motor primitives).
Example of skills that are targeted by learning algorithms include sensorimotor skills such as locomotion, grasping, active object categorization,  as well as interactive skills such as joint manipulation of an object with a human peer, and linguistic skills such as the grounded and situated meaning of human language. Learning can happen either through autonomous self-exploration or through guidance from a human teacher, like for example in robot learning by imitation.
Robot learning can be closely related to adaptive control, reinforcement learning as well as developmental robotics which considers the problem of autonomous lifelong acquisition of repertoires of skills.
While machine learning is frequently used by computer vision algorithms employed in the context of robotics, these applications are usually not referred to as ""robot learning"".

",Robot learning,0,1
1153,1167036,"Robot locomotion is the collective name for the various methods that robots use to transport themselves from place to place.
Wheeled robots are typically quite energy efficient and simple to control. However, other forms of locomotion may be more appropriate for a number of reasons, for example traversing rough terrain, as well as moving and interacting in human environments. Furthermore, studying bipedal and insect-like robots may beneficially impact on biomechanics.
A major goal in this field is in developing capabilities for robots to autonomously decide how, when, and where to move.  However, coordinating numerous robot joints for even simple matters, like negotiating stairs, is difficult. Autonomous robot locomotion is a major technological obstacle for many areas of robotics, such as humanoids (like Honda's Asimo).

",Robot locomotion,0,1
1154,20903754,"Robotics is an interdisciplinary branch of computer science and engineering. Robotics involves design, construction, operation, and use of robots. The goal of robotics is to design machines that can help and assist humans. Robotics integrates fields of mechanical engineering, electrical engineering, information engineering, mechatronics, electronics, bioengineering, computer engineering, control engineering, software engineering, mathematics, etc.
Robotics develops machines that can substitute for humans and replicate human actions. Robots can be used in many situations for many purposes, but today many are used in dangerous environments (including inspection of radioactive materials, bomb detection and deactivation), manufacturing processes, or where humans cannot survive (e.g. in space, underwater, in high heat, and clean up and containment of hazardous materials and radiation). Robots can take any form, but some are made to resemble humans in appearance. This is claimed to help in the acceptance of robots in certain replicative behaviors which are usually performed by people. Such robots attempt to replicate walking, lifting, speech, cognition, or any other human activity. Many of today's robots are inspired by nature, contributing to the field of bio-inspired robotics.
Certain robots require user input to operate while other robots function autonomously. The concept of creating robots that can operate autonomously dates back to classical times, but research into the functionality and potential uses of robots did not grow substantially until the 20th century. Throughout history, it has been frequently assumed by various scholars, inventors, engineers, and technicians that robots will one day be able to mimic human behavior and manage tasks in a human-like fashion. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes, whether domestically, commercially, or militarily. Many robots are built to do jobs that are hazardous to people, such as defusing bombs, finding survivors in unstable ruins, and exploring mines and shipwrecks. Robotics is also used in STEM (science, technology, engineering, and mathematics) as a teaching aid.",Robotics,0,1
1155,51997474,"Rule-based machine learning (RBML) is a term in computer science intended to encompass any machine learning method that identifies, learns, or evolves 'rules' to store, manipulate or apply.  The defining characteristic of a rule-based machine learner is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system.  This is in contrast to other machine learners that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.Rule-based machine learning approaches include learning classifier systems, association rule learning, artificial immune systems, and any other method that relies on a set of rules, each covering contextual knowledge.
While rule-based machine learning is conceptually a type of rule-based system, it is distinct from traditional rule-based systems, which are often hand-crafted, and other rule-based decision makers.  This is because rule-based machine learning applies some form of learning algorithm to automatically identify useful rules, rather than a human needing to apply prior domain knowledge to manually construct rules and curate a rule set.",Rule-based machine learning,0,1
1157,20588127,"IBM SPSS Modeler is a data mining and text analytics software application from IBM. It is used to build predictive models and conduct other analytic tasks. It has a visual interface which allows users to leverage statistical and data mining algorithms without programming.
One of its main aims from the outset was to get rid of unnecessary complexity in data transformations, and to make complex predictive models very easy to use.
The first version incorporated decision trees (ID3), and neural networks (backprop), which could both be trained without underlying knowledge of how those techniques worked.
IBM SPSS Modeler was originally named Clementine by its creators, Integral Solutions Limited. This name continued for a while after SPSS's acquisition of the product. SPSS later changed the name to SPSS Clementine, and then later to PASW Modeler. Following IBM's 2009 acquisition of SPSS, the product was renamed IBM SPSS Modeler, its current name.

",SPSS Modeler,0,1
1158,2079997,"Statistica is an advanced analytics software package originally developed by StatSoft and currently maintained by TIBCO Software Inc.
Statistica provides data analysis, data management, statistics, data mining, machine learning, text analytics and data visualization procedures.",Statistica,0,1
1160,33490859,"Scikit-learn (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language.
It features various classification, regression and clustering algorithms including support-vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy. Scikit-learn is a NumFOCUS fiscally sponsored project.

",Scikit-learn,0,1
1161,28249,"In computer science, a search algorithm is an algorithm (if more than one, algorithms) designed to solve a search problem. Search algorithms work to retrieve information stored within particular data structure, or calculated in the search space of a problem domain, with either discrete or continuous values.
While the search problems described above and web search are both problems in information retrieval, they are generally studied as separate subfields and are solved and evaluated differently. Web search problems are generally focused on filtering and finding documents highly relevant to human queries. Classic search algorithms are evaluated on how fast they can find a solution, and whether the solution found is optimal. Though information retrieval algorithms must be fast, the quality of ranking, and whether good results have been left out and bad results included, is more important.
The appropriate search algorithm often depends on the data structure being searched, and may also include prior knowledge about the data. Search algorithms can be made faster or more efficient by specially constructed database structures, such as search trees, hash maps, and database indexes.Search algorithms can be classified based on their mechanism of searching into three types of algorithms: linear, binary, and hashing. Linear search algorithms check every record for the one associated with a target key in a linear fashion. Binary, or half-interval, searches repeatedly target the center of the search structure and divide the search space in half. Comparison search algorithms improve on linear searching by successively eliminating records based on comparisons of the keys until the target record is found, and can be applied on data structures with a defined order. Digital search algorithms work based on the properties of digits in data structures by using numerical keys. Finally, hashing directly maps keys to records based on a hash function.Algorithms are often evaluated by their computational complexity, or maximum theoretical run time. Binary search functions, for example, have a maximum complexity of O(log n), or logarithmic time. In simple terms, the maximum number of operations needed to find the search target is a logarithmic function of the size of the search space.",Search algorithm,0,1
1162,4059023,"A search engine is a software system designed to carry out web searches.  They search the World Wide Web in a systematic way for particular information specified in a textual web search query. The search results are generally presented in a line of results, often referred to as search engine results pages (SERPs). The information may be a mix of links to web pages, images, videos, infographics, articles, research papers, and other types of files. Some search engines also mine data available in databases or open directories. Unlike web directories, which are maintained only by human editors, search engines also maintain real-time information by running an algorithm on a web crawler. Any internet-based content that can't be indexed and searched by a web search engine falls under the category of deep web.

",Search engine,0,1
1163,29015809,"Security service is a service, provided by a layer of communicating open systems, which ensures adequate security of the systems or of data transfers as defined by ITU-T X.800 Recommendation. 
X.800 and ISO 7498-2 (Information processing systems – Open systems interconnection – Basic Reference Model – Part 2: Security architecture)  are technically aligned. This model is widely recognized A more general definition is in CNSS Instruction No. 4009 dated 26 April 2010 by Committee on National Security Systems of United States of America:
A capability that supports one, or more, of the security requirements (Confidentiality, Integrity, Availability). Examples of security services are key management, access control, and authentication.Another authoritative definition is in W3C Web service Glossary  adopted by NIST SP 800-95:
A processing or communication service that is provided by a system to give a specific kind of protection to resources, where said resources may reside with said system or reside with other systems, for example, an authentication service or a PKI-based document attribution and authentication service. A security service is a superset of AAA services. Security services typically implement portions of security policies and are implemented via security mechanisms.

",Security service (telecommunication),0,1
1165,76996,"A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) representation of a higher dimensional data set while preserving the topological structure of the data. For example, a data set with p variables measured in n observations could be represented as clusters of observations with similar values for the variables. These clusters then could be visualized as a two-dimensional ""map"" such that observations in proximal clusters have more similar values than observations in distal clusters. This can make high-dimensional data easier to visualize and analyze.
An SOM is a type of artificial neural network but is trained using competitive learning rather than the error-correction learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s and therefore is sometimes called a Kohonen map or Kohonen network. The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s and morphogenesis models dating back to Alan Turing in the 1950s.",Self-organizing map,0,1
1166,70721875,"Self-play is a technique for improving the performance of reinforcement learning agents. Intuitively, agents learn to improve their performance by playing ""against themselves"".

",Self-play (reinforcement learning technique),0,1
1167,397608,"In programming language theory, semantics is the field concerned with the rigorous mathematical study of the meaning of programming languages. It does so by evaluating the meaning of syntactically valid strings defined by a specific programming language, showing the computation involved. In such a case that the evaluation would be of syntactically invalid strings, the result would be non-computation. Semantics describes the processes a computer follows when executing a program in that specific language. This can be shown by describing the relationship between the input and output of a program, or an explanation of how the program will be executed on a certain platform, hence creating a model of computation.",Semantics (computer science),0,1
1168,2829632,"Semi-supervised learning is an approach to machine learning that combines a small amount of labeled data with a large amount of unlabeled data during training. Semi-supervised learning falls between unsupervised learning (with no labeled training data) and supervised learning (with only labeled training data). It is a special instance of weak supervision.
Unlabeled data, when used in conjunction with a small amount of labeled data, can produce considerable improvement in learning accuracy. The acquisition of labeled data for a learning problem often requires a skilled human agent (e.g. to transcribe an audio segment) or a physical experiment (e.g. determining the 3D structure of a protein or determining whether there is oil at a particular location). The cost associated with the labeling process thus may render large, fully labeled training sets infeasible, whereas acquisition of unlabeled data is relatively inexpensive. In such situations, semi-supervised learning can be of great practical value. Semi-supervised learning is also of theoretical interest in machine learning and as a model for human learning.
A set of 
  
    
      
        l
      
    
    {\displaystyle l}
   independently identically distributed examples 
  
    
      
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            l
          
        
        ∈
        X
      
    
    {\displaystyle x_{1},\dots ,x_{l}\in X}
   with corresponding labels 
  
    
      
        
          y
          
            1
          
        
        ,
        …
        ,
        
          y
          
            l
          
        
        ∈
        Y
      
    
    {\displaystyle y_{1},\dots ,y_{l}\in Y}
   and 
  
    
      
        u
      
    
    {\displaystyle u}
   unlabeled examples 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
        ∈
        X
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}\in X}
   are processed. Semi-supervised learning combines this information to surpass the classification performance that can be obtained either by discarding the unlabeled data and doing supervised learning or by discarding the labels and doing unsupervised learning.
Semi-supervised learning may refer to either transductive learning or inductive learning. The goal of transductive learning is to infer the correct labels for the given unlabeled data 
  
    
      
        
          x
          
            l
            +
            1
          
        
        ,
        …
        ,
        
          x
          
            l
            +
            u
          
        
      
    
    {\displaystyle x_{l+1},\dots ,x_{l+u}}
   only. The goal of inductive learning is to infer the correct mapping from 
  
    
      
        X
      
    
    {\displaystyle X}
   to 
  
    
      
        Y
      
    
    {\displaystyle Y}
  .
Intuitively, the learning problem can be seen as an exam and labeled data as sample problems that the teacher solves for the class as an aid in solving another set of problems. In the transductive setting, these unsolved problems act as exam questions. In the inductive setting, they become practice problems of the sort that will make up the exam.
It is unnecessary (and, according to Vapnik's principle, imprudent) to perform transductive learning by way of inferring a classification rule over the entire input space; however, in practice, algorithms formally designed for transduction or induction are often used interchangeably.",Semi-supervised learning,0,1
1169,5599330,"Sensitivity and specificity mathematically describe the accuracy of a test which reports the presence or absence of a condition. Individuals for which the condition is satisfied are considered ""positive"" and those for which it is not are considered ""negative"".

Sensitivity (true positive rate) refers to the probability of a positive test, conditioned on truly being positive.
Specificity (true negative rate) refers to the probability of a negative test, conditioned on truly being negative.If the true condition can not be known, a ""gold standard test"" is assumed to be correct. In a diagnostic test, sensitivity is a measure of how well a test can identify true positives and specificity is a measure of how well a test can identify true negatives. For all testing, both diagnostic and screening, there is usually a trade-off between sensitivity and specificity, such that higher sensitivities will mean lower specificities and vice versa.
If the goal of the test is to identify everyone who has a condition, the number of false negatives should be low, which requires high sensitivity. That is, people who have the condition should be highly likely to be identified as such by the test. This is especially important when the consequence of failing to treat the condition are serious and/or the treatment is very effective and has minimal side effects.
If the goal of the test is to accurately identify people who do not have the condition, the number of false positives should be very low, which requires a high specificity. That is, people who do not have the condition should be highly likely to be excluded by the test. This is especially important when people who are identified as having a condition may be subjected to more testing, expense, stigma, anxiety, etc.

The terms ""sensitivity"" and ""specificity"" were introduced by American biostatistician Jacob Yerushalmy in 1947.",Sensitivity and specificity,0,1
1170,6435232,"Sentiment analysis (also known as opinion mining or emotion AI) is the use of natural language processing, text analysis, computational linguistics, and biometrics to systematically identify, extract, quantify, and study affective states and subjective information. Sentiment analysis is widely applied to voice of the customer materials such as reviews and survey responses, online and social media, and healthcare materials for applications that range from marketing to customer service to clinical medicine. With the rise of deep language models, such as RoBERTa, also more difficult data domains can be analyzed, e.g., news texts where authors typically express their opinion/sentiment less explicitly.

",Sentiment analysis,0,1
1171,37895661,"SequenceL is a general purpose functional programming language and auto-parallelizing (Parallel computing) compiler and tool set, whose primary design objectives are performance on multi-core processor hardware, ease of programming, platform portability/optimization, and code clarity and readability.  Its main advantage is that it can be used to write straightforward code that automatically takes full advantage of all the processing power available, without programmers needing to be concerned with identifying parallelisms, specifying vectorization, avoiding race conditions, and other challenges of manual directive-based programming approaches such as OpenMP.
Programs written in SequenceL can be compiled to multithreaded code that runs in parallel, with no explicit indications from a programmer of how or what to parallelize. As of 2015, versions of the SequenceL compiler generate parallel code in C++ and OpenCL, which allows it to work with most popular programming languages, including C, C++, C#, Fortran, Java, and Python.  A platform-specific runtime manages the threads safely, automatically providing parallel performance according to the number of cores available, currently supporting x86, POWER8, and ARM platforms.",SequenceL,0,1
1172,4986804,"Sequential pattern mining is a topic of data mining concerned with finding statistically relevant patterns between data examples where the values are delivered in a sequence. It is usually presumed that the values are discrete, and thus time series mining is closely related, but usually considered a different activity.  Sequential pattern mining is a special case of structured data mining.
There are several key traditional computational problems addressed within this field.  These include building efficient databases and indexes for sequence information, extracting the frequently occurring patterns, comparing sequences for similarity, and recovering missing sequence members.  In general, sequence mining problems can be classified as string mining which is typically based on string processing algorithms  and itemset mining which is typically based on association rule learning. Local process models  extend sequential pattern mining to more complex patterns that can include (exclusive) choices, loops, and concurrency constructs in addition to the sequential ordering construct. 

",Sequential pattern mining,0,1
1173,9517150,"Shogun is a free, open-source machine learning software library  written in C++. It offers numerous algorithms and data structures for machine learning problems. It offers interfaces for Octave, Python, R, Java, Lua, Ruby and C# using SWIG.
It is licensed under the terms of the GNU General Public License version 3 or later.",Shogun (toolbox),0,1
1174,87210,"A sigmoid function is a mathematical function having a characteristic ""S""-shaped curve or sigmoid curve.  
A common example of a sigmoid function is the logistic function shown in the first figure and defined by the formula:

  
    
      
        S
        (
        x
        )
        =
        
          
            1
            
              1
              +
              
                e
                
                  −
                  x
                
              
            
          
        
        =
        
          
            
              e
              
                x
              
            
            
              
                e
                
                  x
                
              
              +
              1
            
          
        
        =
        1
        −
        S
        (
        −
        x
        )
        .
      
    
    {\displaystyle S(x)={\frac {1}{1+e^{-x}}}={\frac {e^{x}}{e^{x}+1}}=1-S(-x).}
  Other standard sigmoid functions are given in the Examples section. In some fields, most notably in the context of artificial neural networks, the term ""sigmoid function"" is used as an alias for the logistic function.
Special cases of the sigmoid function include the Gompertz curve (used in modeling systems that saturate at large values of x) and the ogee curve (used in the spillway of some dams). Sigmoid functions have domain of all real numbers, with return (response) value commonly monotonically increasing but could be decreasing. Sigmoid functions most often show a return value (y axis) in the range 0 to 1. Another commonly used range is from −1 to 1.
A wide variety of sigmoid functions including the logistic and hyperbolic tangent functions have been used as the activation function of artificial neurons. Sigmoid curves are also common in statistics as cumulative distribution functions (which go from 0 to 1), such as the integrals of the logistic density, the normal density, and Student's t probability density functions.  The logistic sigmoid function is invertible, and its inverse is the logit function.

",Sigmoid function,0,1
1175,38059657,"Similarity learning is an area of supervised machine learning in artificial intelligence. It is closely related to regression and classification, but the goal is to learn a similarity function that measures how similar or related two objects are. It has applications in ranking, in recommendation systems, visual identity tracking, face verification, and speaker verification.

",Similarity learning,0,1
1176,49648894,"Simulation-based optimization (also known as simply simulation optimization) integrates optimization techniques into simulation modeling and analysis. Because of the complexity of the simulation, the objective function may become difficult and expensive to evaluate. Usually, the underlying simulation model is stochastic, so that the objective function must be estimated using statistical estimation techniques (called output analysis in simulation methodology).
Once a system is mathematically modeled, computer-based simulations provide information about its behavior. Parametric simulation methods can be used to improve the performance of a system. In this method, the input of each variable is varied with other parameters remaining constant and the effect on the design objective is observed. This is a time-consuming method and improves the performance partially. To obtain the optimal solution with minimum computation and time, the problem is solved iteratively where in each iteration the solution moves closer to the optimum solution. Such methods are known as ‘numerical optimization’ or ‘simulation-based optimization’.In simulation experiment, the goal is to evaluate the effect of different values of input variables on a system. However, the interest is sometimes in finding the optimal value for input variables in terms of the system outcomes. One way could be running simulation experiments for all possible input variables. However, this approach is not always practical due to several possible situations and it just makes it intractable to run experiments for each scenario. For example, there might be too many possible values for input variables, or the simulation model might be too complicated and expensive to run for suboptimal input variable values. In these cases, the goal is to find optimal values for the input variables rather than trying all possible values. This process is called simulation optimization.Specific simulation–based optimization methods can be chosen according to Figure 1 based on the decision variable types.

Optimization exists in two main branches of operations research:
Optimization parametric (static) – The objective is to find the values of the parameters, which are “static” for all states, with the goal of maximizing or minimizing a function. In this case, one can use mathematical programming, such as linear programming. In this scenario, simulation helps when the parameters contain noise or the evaluation of the problem would demand excessive computer time, due to its complexity.Optimization control (dynamic) – This is used largely in computer science and electrical engineering. The optimal control is per state and the results change in each of them. One can use mathematical programming, as well as dynamic programming. In this scenario, simulation can generate random samples and solve complex and large-scale problems.

",Simulation-based optimization,0,1
1177,1416993,"Social computing is an area of computer science that is concerned with the intersection of social behavior and computational systems. It is based on creating or recreating social conventions and social contexts through the use of software and technology. Thus, blogs, email, instant messaging, social network services, wikis, social bookmarking and other instances of what is often called social software illustrate ideas from social computing.",Social computing,0,1
1178,34327569,"A social network  is a social structure made up of a set of social actors (such as individuals or organizations), sets of dyadic ties, and other social interactions between actors. The social network perspective provides a set of methods for analyzing the structure of whole social entities as well as a variety of theories explaining the patterns observed in these structures. The study of these structures uses social network analysis to identify local and global patterns, locate influential entities, and examine network dynamics.
Social networks and the analysis of them is an inherently interdisciplinary academic field which emerged from social psychology, sociology, statistics, and graph theory. Georg Simmel authored early structural theories in sociology emphasizing the dynamics of triads and ""web of group affiliations"". Jacob Moreno is credited with developing the first sociograms in the 1930s to study interpersonal relationships. These approaches were mathematically formalized in the 1950s and theories and methods of social networks became pervasive in the social and behavioral sciences by the 1980s. Social network analysis is now one of the major paradigms in contemporary sociology, and is also employed in a number of other social and formal sciences. Together with other complex networks, it forms part of the nascent field of network science.",Social network,0,1
1179,222034,"Social software, also known as social apps, include communication and interactive tools often based on the Internet. Communication tools typically handle the capturing, storing and presentation of communication, usually written but increasingly including audio and video as well. Interactive tools handle mediated interactions between a pair or group of users. They focus on establishing and maintaining a connection among users, facilitating the mechanics of conversation and talk. Social software generally refers to software that makes collaborative behaviour, the organisation and moulding of communities, self-expression, social interaction and feedback possible for individuals. Another element of the existing definition of social software is that it allows for the structured mediation of opinion between people, in a centralized or self-regulating manner. The most improved area for social software is that Web 2.0 applications can all promote cooperation between people and the creation of online communities more than ever before. The opportunities offered by social software are instant connection and the opportunity to learn.An additional defining feature of social software is that apart from interaction and collaboration, it aggregates the collective behaviour of its users, allowing not only crowds to learn from an individual but individuals to learn from the crowds as well. Hence, the interactions enabled by social software can be one-on-one, one-to-many, or many-to-many.",Social software,0,1
1180,6152185,"The softmax function, also known as softargmax: 184  or normalized exponential function,: 198  converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.",Softmax function,0,1
1181,430106,"In computer science, a software agent  is a computer program that acts for a user or other program in a relationship of agency, which derives from the Latin agere (to do): an agreement to act on one's behalf. Such ""action on behalf of"" implies the authority to decide which, if any, action is appropriate. Agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or  as software such as a chatbot executing on a phone (e.g. Siri)  or other computing device.  Software agents may be autonomous or work together with other agents or people.  Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).
Related and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).",Software agent,0,1
1182,165180,"In software engineering, software configuration management (SCM or S/W CM) is the task of tracking and controlling changes in the software, part of the larger cross-disciplinary field of configuration management.  SCM practices include revision control and the establishment of baselines.  If something goes wrong, SCM can determine the ""what, when, why and who"" of the change.  If a configuration is working well, SCM can determine how to replicate it across many hosts.
The acronym ""SCM"" is also expanded as source configuration management process and software change and configuration management.  However, ""configuration"" is generally understood to cover changes typically made by a system administrator.

",Software configuration management,0,1
1183,33408418,"Software construction is a software engineering discipline. It is the detailed creation of working meaningful software through a combination of coding, verification, unit testing, integration testing, and debugging. It is linked to all the other software engineering disciplines, most strongly to software design and software testing.

",Software construction,0,1
1184,1688759,"Software deployment is all of the activities that make a software system available for use.The general deployment process consists of several interrelated activities with possible transitions between them. These activities can occur on the producer side or on the consumer side or both. Because every software system is unique, the precise processes or procedures within each activity can hardly be defined. Therefore, ""deployment"" should be interpreted as a general process that has to be customized according to specific requirements or characteristics.",Software deployment,0,1
1185,223325,"Software design is the process by which an agent creates a specification of a software artifact intended to accomplish goals, using a set of primitive components and subject to constraints. Software design may refer to either ""all the activity involved in conceptualizing, framing, implementing, commissioning, and ultimately modifying complex systems"" or ""the activity following requirements specification and before programming, as ... [in] a stylized software engineering process.""Software design usually involves problem-solving and planning a software solution. This includes both a low-level component and algorithm design and a high-level, architecture design.

",Software design,0,1
1186,248932,"Software development is the process of conceiving, specifying, designing, programming, documenting, testing, and bug fixing involved in creating and maintaining applications, frameworks, or other software components. Software development involves writing and maintaining the source code, but in a broader sense, it includes all processes from the conception of the desired software through to the final manifestation of the software, typically in a planned and structured process. Software development also includes research, new development, prototyping, modification, reuse, re-engineering, maintenance, or any other activities that result in software products.

",Software development,0,1
1187,23407868,"In software engineering, a software development process is a process of dividing software development work into smaller, parallel, or sequential steps or sub-processes to improve design, product management.  It is also known as a software development life cycle (SDLC).  The methodology may include the pre-definition of specific deliverables and artifacts that are created and completed by a project team to develop or maintain an application.Most modern development processes can be vaguely described as agile. Other methodologies include waterfall, prototyping, iterative and incremental development, spiral development, rapid application development, and extreme programming.
A life-cycle ""model"" is sometimes considered a more general term for a category of methodologies and a software development ""process"" a more specific term to refer to a specific process chosen by a specific organization. For example, there are many specific software development processes that fit the spiral life-cycle model. The field is often considered a subset of the systems development life cycle.",Software development process,0,1
1188,27010,"Software engineering is a systematic engineering approach to software development.A software engineer is a person who applies the principles of software engineering to design, develop, maintain, test, and evaluate computer software. The term programmer is sometimes used as a synonym, but may also lack connotations of engineering education or skills.
Engineering techniques are used to inform the software development process which involves the definition, implementation, assessment, measurement, management, change, and improvement of the software life cycle process itself. It heavily uses software configuration management which is about systematically controlling changes to the configuration, and maintaining the integrity and traceability of the configuration and code throughout the system life cycle. Modern processes use software versioning.

",Software engineering,0,1
1189,768211,"In computer programming, a software framework is an abstraction in which software, providing generic functionality, can be selectively changed by additional user-written code, thus providing application-specific software. It provides a standard way to build and deploy applications and is a universal, reusable software environment that provides particular functionality as part of a larger software platform to facilitate the development of software applications, products and solutions. Software frameworks may include support programs, compilers, code libraries, toolsets, and application programming interfaces (APIs) that bring together all the different components to enable development of a project or system.
Frameworks have key distinguishing features that separate them from normal libraries:

inversion of control: In a framework, unlike in libraries or in standard user applications, the overall program's flow of control is not dictated by the caller, but by the framework. This is usually achieved with the Template Method Pattern.
default behaviour: This can be provided with the invariant methods of the Template Method Pattern in an abstract class which is provided by the framework.
extensibility: A user can extend the framework – usually by selective overriding – or programmers can add specialized user code to provide specific functionality. This is usually achieved by a hook method in a subclass that overrides a template method in the superclass.
non-modifiable framework code: The framework code, in general, is not supposed to be modified, while accepting user-implemented extensions. In other words, users can extend the framework, but cannot modify its code.

",Software framework,0,1
1190,780960,"Software maintenance in software engineering is the modification of a software product after delivery to correct faults, to improve performance or other attributes.A common perception of maintenance is that it merely involves fixing defects. However, one study indicated that over 80% of maintenance effort is used for non-corrective actions. This perception is perpetuated by users submitting problem reports that in reality are functionality enhancements to the system. More recent studies put the bug-fixing proportion closer to 21%.",Software maintenance,0,1
1191,1301906,"In the context of software engineering, software quality refers to two related but distinct notions:
Software functional quality reflects how well it complies with or conforms to a given design, based on functional requirements or specifications. That attribute can also be described as the fitness for purpose of a piece of software or how it compares to competitors in the marketplace as a worthwhile product. It is the degree to which the correct software was produced.
Software structural quality refers to how it meets non-functional requirements that support the delivery of the functional requirements, such as robustness or maintainability. It has a lot more to do with the degree to which the software works as needed.Many aspects of structural quality can be evaluated only statically through the analysis of the software inner structure, its source code (see Software metrics), at the unit level, system level (sometimes referred to as end-to-end testing), which is in effect how its architecture adheres to sound principles of software architecture outlined in a paper on the topic by Object Management Group (OMG).However some structural qualities, such as usability, can be assessed only dynamically (users or others acting in their behalf interact with the software or, at least, some prototype or partial implementation; even the interaction with a mock version made in cardboard represents a dynamic test because such version can be considered a prototype). Other aspects, such as reliability, might involve not only the software but also the underlying hardware, therefore, it can be assessed both statically and dynamically (stress test).Functional quality is typically assessed dynamically but it is also possible to use static tests (such as software reviews).Historically, the structure, classification and terminology of attributes and metrics applicable to software quality management have been derived or extracted from the ISO 9126 and the subsequent ISO/IEC 25000 standard. Based on these models (see Models), the Consortium for IT Software Quality (CISQ) has defined five major desirable structural characteristics needed for a piece of software to provide business value: Reliability, Efficiency, Security, Maintainability and (adequate) Size.Software quality measurement quantifies to what extent a software program or system rates along each of these five dimensions. An aggregated measure of software quality can be computed through a qualitative or a quantitative scoring scheme or a mix of both and then a weighting system reflecting the priorities. This view of software quality being positioned on a linear continuum is supplemented by the analysis of ""critical programming errors"" that under specific circumstances can lead to catastrophic outages or performance degradations that make a given system unsuitable for use regardless of rating based on aggregated measurements. Such programming errors found at the system level represent up to 90 percent of production issues, whilst at the unit-level, even if far more numerous, programming errors account for less than 10 percent of production issues (see also Ninety–ninety rule). As a consequence, code quality without the context of the whole system, as W. Edwards Deming described it, has limited value.To view, explore, analyze, and communicate software quality measurements, concepts and techniques of information visualization provide visual, interactive means useful, in particular, if several software quality measures have to be related to each other or to components of a software or system. For example, software maps represent a specialized approach that ""can express and combine information about software development, software quality, and system dynamics"".Software quality also plays a role in the release phase of a software project. Specifically, the quality and establishment of the release processes (also patch processes), configuration management are important parts of an overall software engineering process.",Software quality,0,1
1192,9570763,"A software repository, or repo for short, is a storage location for software packages. Often a table of contents is also stored, along with metadata. A software repository is typically managed by source control or repository managers. Package managers allow automatically installing and updating repositories (sometimes called ""packages"").",Software repository,0,1
1193,442656,"A software suite (Also known as an application suite) is a collection of computer programs (application software, or programming software) of related functionality, sharing a similar user interface and the ability to easily exchange data with each other.",Software suite,0,1
1194,457579,"Solid modeling (or solid modelling) is a consistent set of principles for mathematical and computer modeling of three-dimensional solids. Solid modeling is distinguished from related areas of geometric modeling and computer graphics, such as 3D modeling, by its emphasis on physical fidelity. Together, the principles of geometric and solid modeling form the foundation of 3D-computer-aided design and in general support the creation, exchange, visualization, animation, interrogation, and annotation of digital models of physical objects.",Solid modeling,0,1
1195,6147487,"Neural coding is the transduction of environmental signals and internal signals of the body into neural activity patterns as representations forming a model of reality suitable for purposeful actions and adaptation, preserving the integrity and normal functioning of the body. It also describes the study of information processing by neurons along with learning on what the information is used for and how it is transformed when it is being passed through from one another.
Deciphering the neural code is considered a large-scale task because of its complexity and significance. It is difficult to overestimate the prospects that the ability to read and write neural code would open to us. They concern the treatment to all diseases of the body, from the main three (cardiovascular, cancer and diabetes) to the less significant ones, but still affecting the quality of life. And last but not least, it will give a new perspective for the development of artificial intelligence technologies and their integration with natural technologies of the brain.",Neural coding,0,1
1197,341015,"In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero. There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. By contrast, if most of the elements are non-zero, the matrix is considered dense. The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix.
Conceptually, sparsity corresponds to systems with few pairwise interactions. For example, consider a line of balls connected by springs from one to the next: this is a sparse system as only adjacent balls are coupled. By contrast, if the same line of balls were to have springs connecting each ball to all other balls, the system would correspond to a dense matrix. The concept of sparsity is useful in combinatorics and application areas such as network theory and numerical analysis, which typically have a low density of significant data or connections. Large sparse matrices often appear in scientific or engineering applications when solving partial differential equations.
When storing and manipulating sparse matrices on a computer, it is beneficial and often necessary to use specialized algorithms and data structures that take advantage of the sparse structure of the matrix. Specialized computers have been made for sparse matrices, as they are common in the machine learning field. Operations using standard dense-matrix structures and algorithms are slow and inefficient when applied to large sparse matrices as processing and memory are wasted on the zeros. Sparse data is by nature more easily compressed and thus requires significantly less storage. Some very large sparse matrices are infeasible to manipulate using standard dense-matrix algorithms.",Sparse matrix,0,1
1198,29468,"Speech recognition is an interdisciplinary subfield of computer science and computational linguistics that develops methodologies and technologies that enable the recognition and translation of spoken language into text by computers with the main benefit of searchability. It is also known as automatic speech recognition (ASR), computer speech recognition or speech to text (STT). It incorporates knowledge and research in the computer science, linguistics and computer engineering fields. The reverse process is speech synthesis.
Some speech recognition systems require ""training"" (also called ""enrollment"") where an individual speaker reads text or isolated vocabulary into the system. The system analyzes the person's specific voice and uses it to fine-tune the recognition of that person's speech, resulting in increased accuracy. Systems that do not use training are called ""speaker-independent"" systems. Systems that use training are called ""speaker dependent"".
Speech recognition applications include voice user interfaces such as voice dialing (e.g. ""call home""), call routing (e.g. ""I would like to make a collect call""), domotic appliance control, search key words (e.g. find a podcast where particular words were spoken), simple data entry (e.g., entering a credit card number), preparation of structured documents (e.g. a radiology report), determining speaker characteristics, speech-to-text processing (e.g., word processors or emails), and aircraft (usually termed direct voice input).
The term voice recognition or speaker identification refers to identifying the speaker, rather than what they are saying. Recognizing the speaker can simplify the task of translating speech in systems that have been trained on a specific person's voice or it can be used to authenticate or verify the identity of a speaker as part of a security process.
From the technology perspective, speech recognition has a long history with several waves of major innovations. Most recently, the field has benefited from advances in deep learning and big data. The advances are evidenced not only by the surge of academic papers published in the field, but more importantly by the worldwide industry adoption of a variety of deep learning methods in designing and deploying speech recognition systems.",Speech recognition,0,1
1199,42799,"Speech synthesis is the artificial production of human speech. A computer system used for this purpose is called a speech computer or speech synthesizer, and can be implemented in software or hardware products. A text-to-speech (TTS) system converts normal language text into speech; other systems render symbolic linguistic representations like phonetic transcriptions into speech. The reverse process is speech recognition.
Synthesized speech can be created by concatenating pieces of recorded speech that are stored in a database. Systems differ in the size of the stored speech units; a system that stores phones or diphones provides the largest output range, but may lack clarity. For specific usage domains, the storage of entire words or sentences allows for high-quality output. Alternatively, a synthesizer can incorporate a model of the vocal tract and other human voice characteristics to create a completely ""synthetic"" voice output.The quality of a speech synthesizer is judged by its similarity to the human voice and by its ability to be understood clearly. An intelligible text-to-speech program allows people with visual impairments or reading disabilities to listen to written words on a home computer. Many computer operating systems have included speech synthesizers since the early 1990s.

A text-to-speech system (or ""engine"") is composed of two parts: a front-end and a back-end. The front-end has two major tasks. First, it converts raw text containing symbols like numbers and abbreviations into the equivalent of written-out words. This process is often called text normalization, pre-processing, or tokenization. The front-end then assigns phonetic transcriptions to each word, and divides and marks the text into prosodic units, like phrases, clauses, and sentences. The process of assigning phonetic transcriptions to words is called text-to-phoneme or grapheme-to-phoneme conversion. Phonetic transcriptions and prosody information together make up the symbolic linguistic representation that is output by the front-end. The back-end—often referred to as the synthesizer—then converts the symbolic linguistic representation into sound. In certain systems, this part includes the computation of the target prosody (pitch contour, phoneme durations), which is then imposed on the output speech.",Speech synthesis,0,1
1200,35891416,"SpiNNaker (spiking neural network architecture) is a massively parallel, manycore supercomputer architecture designed by the Advanced Processor Technologies Research Group (APT) at the Department of Computer Science, University of Manchester.  It is composed of 57,600 processing nodes, each with 18 ARM9 processors (specifically ARM968) and 128 MB of mobile DDR SDRAM, totalling 1,036,800 cores and over 7 TB of RAM.  The computing platform is based on spiking neural networks, useful in simulating the human brain (see Human Brain Project).The completed design is housed in 10 19-inch racks, with each rack holding over 100,000 cores. The cards holding the chips are held in 5 blade enclosures, and each core emulates 1,000 neurons.  In total, the goal is to simulate the behaviour of aggregates of up to a billion neurons in real time.  This machine requires about 100 kW from a 240 V supply and an air-conditioned environment.SpiNNaker is being used as one component of the neuromorphic computing platform for the Human Brain Project.On 14 October 2018 the HBP announced that the million core milestone had been achieved.On 24 September 2019 HBP announced that a 8 million euro grant, that will fund construction of the second generation machine, (called SpiNNcloud) has been given to TU Dresden.",SpiNNaker,0,1
1201,10159567,"Spiking neural networks (SNNs) are artificial neural networks that more closely mimic natural neural networks. In addition to neuronal and synaptic state, SNNs incorporate the concept of time into their operating model. The idea is that neurons in the SNN do not transmit information at each propagation cycle (as it happens with typical multi-layer perceptron networks), but rather transmit information only when a membrane potential – an intrinsic quality of the neuron related to its membrane electrical charge – reaches a specific value, called the threshold. When the membrane potential reaches the threshold, the neuron fires, and generates a signal that travels to other neurons which, in turn, increase or decrease their potentials in response to this signal. A neuron model  that fires at the moment of threshold crossing is also called a spiking neuron model.The most prominent spiking neuron model is the leaky integrate-and-fire model. In the integrate-and-fire model, the momentary activation level (modeled as a differential equation) is normally considered to be the neuron's state, with incoming spikes pushing this value higher or lower, until the state eventually either decays or  - if the firing threshold is reached - the neuron fires. After firing the state variable is reset to a lower value.
Various decoding methods exist for interpreting the outgoing spike train as a real-value number, relying on either the frequency of spikes (rate-code), the time-to-first-spike after stimulation, or the interval between spikes.

",Spiking neural network,0,1
1202,19135734,"Splunk Inc. is an American software company based in San Francisco, California, that produces software for searching, monitoring, and analyzing machine-generated data via a Web-style interface.Its software helps capture, index and correlate real-time data in a searchable repository, from which it can generate graphs, reports, alerts, dashboards and visualizations.
Splunk uses machine data for identifying data patterns, providing metrics, diagnosing problems and providing intelligence for business operations. Splunk is a horizontal technology used for application management, security and compliance, as well as business and web analytics.

",Splunk,0,1
1203,10584297,"State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note with the name ""Modified Connectionist Q-Learning"" (MCQ-L). The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.This name reflects the fact that the main function for updating the Q-value depends on the current state of the agent ""S1"", the action the agent chooses ""A1"", the reward ""R"" the agent gets for choosing this action, the state ""S2"" that the agent enters after taking that action, and finally the next action ""A2"" the agent chooses in its new state. The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA. Some authors use a slightly different convention and write the quintuple (st, at, rt+1, st+1, at+1), depending to which time step the reward is formally assigned. The rest of the article uses the former convention.",State–action–reward–state–action,0,1
1207,38523090,"Statistical learning is the ability for humans and other animals to extract statistical regularities from the world around them to learn about the environment. Although statistical learning is now thought to be a generalized learning mechanism, the phenomenon was first identified in  human infant language acquisition.
The earliest evidence for these statistical learning abilities comes from a study by Jenny Saffran, Richard Aslin, and Elissa Newport, in which 8-month-old infants were presented with nonsense streams of monotone speech. Each stream was composed of four three-syllable ""pseudowords"" that were repeated randomly. After exposure to the speech streams for two minutes, infants reacted differently to hearing ""pseudowords"" as opposed to ""nonwords"" from the speech stream, where nonwords were composed of the same syllables that the infants had been exposed to, but in a different order. This suggests that infants are able to learn statistical relationships between syllables even with very limited exposure to a language. That is, infants learn which syllables are always paired together and which ones only occur together relatively rarely, suggesting that they are parts of two different units. This method of learning is thought to be one way that children learn which groups of syllables form individual words.Since the initial discovery of the role of statistical learning in lexical acquisition, the same mechanism has been proposed for elements of phonological acquisition,  and syntactical acquisition, as well as in non-linguistic domains. Further research has also indicated that statistical learning is likely a domain-general and even species-general learning mechanism, occurring for visual as well as auditory information, and in both primates and non-primates.",Statistical learning in language acquisition,0,1
1208,1053303,"Statistical learning theory is a framework for machine learning drawing from the fields of statistics and functional analysis. Statistical learning theory deals with the statistical inference problem of finding a predictive function based on data. Statistical learning theory has led to successful applications in fields such as computer vision, speech recognition, and bioinformatics.",Statistical learning theory,0,1
1209,36408395,"In mathematics, a statistical manifold is a Riemannian manifold, each of whose points is a probability distribution.  Statistical manifolds provide a setting for the field of information geometry.  The Fisher information metric provides a metric on these manifolds. Following this definition, the log-likelihood function is a differentiable map and the score is an inclusion.",Statistical manifold,0,1
1212,179426,"Stevan Robert Harnad (Hernád István Róbert, Hesslein István, born June 2, 1945, Budapest) is a Hungarian-born cognitive scientist based in Montreal, Canada.",Stevan Harnad,0,1
1213,1180641,"Stochastic gradient descent (often abbreviated SGD) is an iterative method for optimizing an objective function with suitable smoothness properties (e.g. differentiable or subdifferentiable). It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in trade for a lower convergence rate.While the basic idea behind stochastic approximation can be traced back to the Robbins–Monro algorithm of the 1950s, stochastic gradient descent has become an important optimization method in machine learning.",Stochastic gradient descent,0,1
1214,6319351,"In computational complexity, strong NP-completeness is a property of computational problems that is a special case of NP-completeness. A general computational problem may have numerical parameters.  For example, the input to the bin packing problem is a list of objects of specific sizes and a size for the bins that must contain the objects—these object sizes and bin size are numerical parameters.
A problem is said to be strongly NP-complete (NP-complete in the strong sense), if it remains NP-complete even when all of its numerical parameters are bounded by a polynomial in the length of the input. A problem is said to be strongly NP-hard if a strongly NP-complete problem has a polynomial reduction to it; in combinatorial optimization, particularly,  the phrase ""strongly NP-hard"" is reserved for problems that are not known to have a polynomial reduction to another strongly NP-complete problem.
Normally numerical parameters to a problem are given in positional notation, so a problem of input size n might contain parameters whose size is exponential in n.  If we redefine the problem to have the parameters given in unary notation, then the parameters must be bounded by the input size.  Thus strong NP-completeness or NP-hardness may also be defined as the NP-completeness or NP-hardness of this unary version of the problem.
For example, bin packing is strongly NP-complete while the 0-1 Knapsack problem is only weakly NP-complete.  Thus the version of bin packing where the object and bin sizes are integers bounded by a polynomial remains NP-complete, while the corresponding version of the Knapsack problem can be solved in pseudo-polynomial time by dynamic programming.
From a theoretical perspective any strongly NP-hard optimization problem with a polynomially bounded objective function cannot have a fully polynomial-time approximation scheme (or FPTAS) unless P = NP. However, the converse fails: e.g. if P does not equal NP,  knapsack with two constraints is not strongly NP-hard, but has no FPTAS even when the optimal objective is polynomially bounded.Some strongly NP-complete problems may still be easy to solve on average, but it's more likely that difficult instances will be encountered in practice.",Strong NP-completeness,0,1
1215,3069503,"Structural health monitoring (SHM) involves the observation and analysis of a system over time using periodically sampled response measurements to monitor changes to the material and geometric properties of engineering structures such as bridges and buildings.
For long term SHM, the output of this process is periodically updated information regarding the ability of the structure to perform its intended function in light of the inevitable aging and degradation resulting from operational environments. After extreme events, such as earthquakes or blast loading, SHM is used for rapid condition screening and aims to provide, in near real time, reliable information regarding the integrity of the structure.The SHM process involves selecting the excitation methods, the sensor types, number and locations, and the data acquisition/storage/transmittal hardware commonly called health and usage monitoring systems. Measurements may be taken to either directly detect any degradation or damage that may occur to a system or indirectly by measuring the size and frequency of loads experienced to allow the state of the system to be predicted.
To directly monitor the state of a system it is necessary to identify features in the acquired data that allows one to distinguish between the undamaged and damaged structure. One of the most common feature extraction methods is based on correlating measured system response quantities, such a vibration amplitude or frequency, with observations of the degraded system. Damage accumulation testing, during which significant structural components of the system under study are degraded by subjecting them to realistic loading conditions, can also be used to identify appropriate features. This process may involve induced-damage testing, fatigue testing, corrosion growth, or temperature cycling to accumulate certain types of damage in an accelerated fashion.

",Structural health monitoring,0,1
1216,27260435,"Structured prediction or structured (output) learning is an umbrella term for supervised machine learning techniques that involves predicting structured objects, rather than scalar discrete or real values.Similar to commonly used supervised learning techniques, structured prediction models are typically trained by means of observed data in which the true prediction value is used to adjust model parameters. Due to the complexity of the model and the interrelations of predicted variables the process of prediction using a trained model and of training itself is often computationally infeasible and approximate inference and learning methods are used.

",Structured prediction,0,1
1217,566689,"Stuart Jonathan Russell  (born 1962) is a British computer scientist known for his contributions to artificial intelligence (AI). He is a professor of computer science at the University of California, Berkeley and adjunct professor of neurological surgery at the University of California, San Francisco. He holds the Smith-Zadeh Chair in Engineering at University of California, Berkeley. He founded and leads the Center for Human-Compatible Artificial Intelligence (CHAI) at UC Berkeley. Russell is the co-author with Peter Norvig of the most popular textbook in the field of AI: Artificial Intelligence: A Modern Approach used in more than 1,500 universities in 135 countries.",Stuart J. Russell,0,1
1218,26980,"Sun Microsystems, Inc. (Sun for short) was an American technology company that sold computers, computer components, software, and information technology services and created the Java programming language, the Solaris operating system, ZFS, the Network File System (NFS), VirtualBox, and SPARC microprocessors. Sun contributed significantly to the evolution of several key computing technologies, among them Unix, RISC processors, thin client computing, and virtualized computing. Sun was founded on February 24, 1982. At its height, the Sun headquarters were in Santa Clara, California (part of Silicon Valley), on the former west campus of the Agnews Developmental Center.
Sun products included computer servers and workstations built on its own RISC-based SPARC processor architecture, as well as on x86-based AMD Opteron and Intel Xeon processors. Sun also developed its own storage systems and a suite of software products, including the Solaris operating system, developer tools, Web infrastructure software, and identity management applications. Technologies included the Java platform and NFS.
In general, Sun was a proponent of open systems, particularly Unix. It was also a major contributor to open-source software, as evidenced by its $1 billion purchase, in 2008, of MySQL, an open-source relational database management system.At various times, Sun had manufacturing facilities in several locations worldwide, including Newark, California; Hillsboro, Oregon; and Linlithgow, Scotland. However, by the time the company was acquired by Oracle, it had outsourced most manufacturing responsibilities.
On April 20, 2009, it was announced that Oracle Corporation would acquire Sun for US$7.4 billion. The deal was completed on January 27, 2010.",Sun Microsystems,0,1
1219,20926,"Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs. It infers a function from labeled training data consisting of a set of training examples. In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. This requires the learning algorithm to generalize from the training data to unseen situations in a ""reasonable"" way (see inductive bias). This statistical quality of an algorithm is measured through the so-called generalization error.
The parallel task in human and animal psychology is often referred to as concept learning.

",Supervised learning,1,1
1220,65309,"In machine learning, support-vector machines (SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995, Vapnik et al., 1997) SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces.
When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The support-vector clustering algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, developed in the support vector machines algorithm, to categorize unlabeled data.",Support-vector machine,0,1
1221,762988,"Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. The concept is employed in work on artificial intelligence. The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment. The inspiration often comes from nature, especially biological systems. The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, and to a certain degree random, interactions between such agents lead to the emergence of ""intelligent"" global behavior, unknown to the individual agents. Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.
The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. Swarm prediction has been used in the context of forecasting problems. Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.",Swarm intelligence,0,1
1222,339417,"In artificial intelligence, symbolic artificial intelligence is the term for the collection of all methods in artificial intelligence research that are based on high-level symbolic (human-readable) representations of problems, logic and search. Symbolic AI used tools such as logic programming, production rules, semantic nets and frames, and it developed applications such as knowledge-based systems (in particular, expert systems),  symbolic mathematics, automated theorem provers, ontologies, the semantic web, and automated planning and scheduling systems. The Symbolic AI paradigm led to seminal ideas in search, symbolic programming languages, agents, multi-agent systems, the semantic web, and the strengths and limitations of formal knowledge and reasoning systems.
Symbolic AI was the dominant paradigm of AI research from the mid-1950s until the middle 1990s. Later,  starting from about 2012, the spectacular successes of deep learning in handling vision, speech recognition, speech synthesis, image generation, and machine translation were used to argue that the symbolic approach was no longer relevant and both research and commercial funding shifted heavily towards deep learning and away from symbolic AI. Since then, difficulties with bias, explanation, comprehensibility, and robustness have become more apparent with deep learning approaches and there has been a shift to consider combining the best of both the symbolic and neural approaches.John Haugeland gave the name GOFAI (""Good Old-Fashioned Artificial Intelligence"") to symbolic AI in his 1985 book Artificial Intelligence: The Very Idea, which explored the philosophical implications of artificial intelligence research. In robotics the analogous term is GOFR (""Good Old-Fashioned Robotics"").Researchers in the 1960s and the 1970s were convinced that symbolic approaches would eventually succeed in creating a machine with artificial general intelligence and considered this the goal of their field. It was succeeded by highly mathematical statistical AI which is largely directed at specific problems with specific goals, rather than general intelligence. Research into general intelligence is now studied in the exploratory sub-field of artificial general intelligence.

",Symbolic artificial intelligence,0,1
1223,5654027,"In the nervous system, a synapse is a structure that permits a neuron (or nerve cell) to pass an electrical or chemical signal to another neuron or to the target effector cell.
Synapses are essential to the transmission of nervous impulses from one neuron to another. Neurons are specialized to pass signals to individual target cells, and synapses are the means by which they do so. At a synapse, the plasma membrane of the signal-passing neuron (the presynaptic neuron) comes into close apposition with the membrane of the target (postsynaptic) cell. Both the presynaptic and postsynaptic sites contain extensive arrays of molecular machinery that link the two membranes together and carry out the signaling process.  In many synapses, the presynaptic part is located on an axon and the postsynaptic part is located on a dendrite or soma. Astrocytes also exchange information with the synaptic neurons, responding to synaptic activity and, in turn, regulating neurotransmission. Synapses (at least chemical synapses) are stabilized in position by synaptic adhesion molecules (SAMs) projecting from both the pre- and post-synaptic neuron and sticking together where they overlap; SAMs may also assist in the generation and functioning of synapses.",Synapse,0,1
1224,6069850,"Syntactic pattern recognition or structural pattern recognition is a form of pattern recognition, in which each object can be represented by a variable-cardinality set of symbolic, nominal features. This allows for representing pattern structures, taking into account more complex interrelationships between attributes than is possible in the case of flat, numerical feature vectors of fixed dimensionality, that are used in statistical classification.
Syntactic pattern recognition can be used instead of statistical pattern recognition if there is clear structure in the patterns. One way to present such structure is by means of a strings of symbols from a formal language. In this case the differences in the structures of the classes are encoded as different grammars.
An example of this would be diagnosis of the heart with ECG measurements. ECG waveforms can be approximated with diagonal and vertical line segments. If normal and unhealthy waveforms can be described as formal grammars, measured ECG signal can be classified as healthy or unhealthy by first describing it in term of the basic line segments and then trying to parse the descriptions according to the grammars. Another example is tessellation of tiling patterns.
A second way to represent relations are graphs, where nodes are connected if corresponding subpatterns are related. An item can be labeled as belonging to a class if its graph representation is isomorphic with prototype graphs of the class.
Typically, patterns are constructed from simpler sub patterns in a hierarchical fashion. This helps in dividing the recognition task into easier subtask of first identifying sub patterns and only then the actual patterns.
Structural methods provide descriptions of items, which may be useful in their own right. For example, syntactic pattern recognition can be used to find out what objects are present in an image. Furthermore, structural methods are strong in finding a correspondence mapping between two images of an object. Under natural conditions, corresponding features will be in different positions and/or may be occluded in the two images, due to camera-attitude and perspective, as in face recognition. A graph matching algorithm will yield the optimal correspondence.

",Syntactic pattern recognition,0,1
1225,100563,"A system on a chip (SoC;  es-oh-SEE or  sock), also written as system-on-a-chip and system-on-chip, is an integrated circuit (also known as a ""chip"") that integrates all or most components of a computer or other electronic system. These components almost always include a central processing unit (CPU), memory interfaces, on-chip input/output devices, input/output interfaces, and secondary storage interfaces, often alongside other components such as radio modems and a graphics processing unit (GPU) – all on a single substrate or microchip. It may contain digital, analog, mixed-signal, and often radio frequency signal processing functions (otherwise it is considered only an application processor).
Higher-performance SoCs are often paired with dedicated and physically separate memory and secondary storage (such as LPDDR and eUFS or eMMC, respectively) chips, that may be layered on top of the SoC in what's known as a package on package (PoP) configuration, or be placed close to the SoC. Additionally, SoCs may use separate wireless modems.SoCs are in contrast to the common traditional motherboard-based PC architecture, which separates components based on function and connects them through a central interfacing circuit board. Whereas a motherboard houses and connects detachable or replaceable components, SoCs integrate all of these components into a single integrated circuit. An SoC will typically integrate a CPU, graphics and memory interfaces, secondary storage and USB connectivity, random-access and read-only memories and secondary storage and/or their controllers on a single circuit die, whereas a motherboard would connect these modules as discrete components or expansion cards.
An SoC integrates a microcontroller, microprocessor or perhaps several processor cores with peripherals like a GPU, Wi-Fi and cellular network radio modems, and/or one or more coprocessors. Similar to how a microcontroller integrates a microprocessor with peripheral circuits and memory, an SoC can be seen as integrating a microcontroller with even more advanced peripherals. For an overview of integrating system components, see system integration.
More tightly integrated computer system designs improve performance and reduce power consumption as well as semiconductor die area than multi-chip designs with equivalent functionality. This comes at the cost of reduced replaceability of components. By definition, SoC designs are fully or nearly fully integrated across different component modules. For these reasons, there has been a general trend towards tighter integration of components in the computer hardware industry, in part due to the influence of SoCs and lessons learned from the mobile and embedded computing markets. SoCs can be viewed as part of a larger trend towards embedded computing and hardware acceleration. 
SoCs are very common in the mobile computing (such as in smartphones and tablet computers) and edge computing markets. They are also commonly used in embedded systems such as WiFi routers and the Internet of things.

",System on a chip,0,1
1226,39758474,"t-distributed stochastic neighbor embedding (t-SNE) is a statistical method for visualizing high-dimensional data by giving each datapoint a location in a two or three-dimensional map. It is based on Stochastic Neighbor Embedding originally developed by Sam Roweis and Geoffrey Hinton, where Laurens van der Maaten proposed the t-distributed variant. It is a nonlinear dimensionality reduction technique well-suited for embedding high-dimensional data for visualization in a low-dimensional space of two or three dimensions. Specifically, it models each high-dimensional object by a two- or three-dimensional point in such a way that similar objects are modeled by nearby points and dissimilar objects are modeled by distant points with high probability.
The t-SNE algorithm comprises two main stages. First, t-SNE constructs a probability distribution over pairs of high-dimensional objects in such a way that similar objects are assigned a higher probability while dissimilar points are assigned a lower probability. Second, t-SNE defines a similar probability distribution over the points in the low-dimensional map, and it minimizes the Kullback–Leibler divergence (KL divergence) between the two distributions with respect to the locations of the points in the map. While the original algorithm uses the Euclidean distance between objects as the base of its similarity metric, this can be changed as appropriate.
t-SNE has been used for visualization in a wide range of applications, including genomics, computer security research, natural language processing, music analysis, cancer research, bioinformatics, geological domain interpretation, and biomedical signal processing.While t-SNE plots often seem to display clusters, the visual clusters can be influenced strongly by the chosen parameterization and therefore a good understanding of the parameters for t-SNE is necessary. Such ""clusters"" can be shown to even appear in non-clustered data, and thus may be false findings. Interactive exploration may thus be necessary to choose parameters and validate results. It has been demonstrated that t-SNE is often able to recover well-separated clusters, and with special parameter choices, approximates a simple form of spectral clustering.

",T-distributed stochastic neighbor embedding,0,1
1227,33094374,"Telecommunication is the transmission of information by various types of technologies over wire, radio, optical, or other electromagnetic systems. It has its origin in the desire of humans for communication over a distance greater than that feasible with the human voice, but with a similar scale of expediency; thus, slow systems (such as postal mail) are excluded from the field.
The transmission media in telecommunication have evolved through numerous stages of technology, from beacons and other visual signals (such as smoke signals, semaphore telegraphs, signal flags, and optical heliographs), to electrical cable and electromagnetic radiation, including light. Such transmission paths are often divided into communication channels, which afford the advantages of multiplexing multiple concurrent communication sessions. Telecommunication is often used in its plural form.
Other examples of pre-modern long-distance communication included audio messages, such as coded drumbeats, lung-blown horns, and loud whistles. 20th- and 21st-century technologies for long-distance communication usually involve electrical and electromagnetic technologies, such as telegraph, telephone, television and teleprinter, networks, radio, microwave transmission, optical fiber, and communications satellites.
A revolution in wireless communication began in the first decade of the 20th century with the pioneering developments in radio communications by Guglielmo Marconi, who won the Nobel Prize in Physics in 1909, and other notable pioneering inventors and developers in the field of electrical and electronic telecommunications. These included Charles Wheatstone and Samuel Morse (inventors of the telegraph), Antonio Meucci and Alexander Graham Bell (some of the inventors and developers of the telephone, see Invention of the telephone), Edwin Armstrong and Lee de Forest (inventors of radio), as well as Vladimir K. Zworykin, John Logie Baird and Philo Farnsworth (some of the inventors of television).
According to Article 1.3 of the Radio Regulations (RR), telecommunication is defined as « Any transmission, emission or reception of signs, signals, writings, images and sounds or intelligence of any nature by wire, radio, optical, or other electromagnetic systems.» This definition is identical to those contained in the Annex to the Constitution and Convention of the International Telecommunication Union (Geneva, 1992).
The early telecommunication networks were created with copper wires as the physical medium for signal transmission. For many years, these networks were used for basic phone services, namely voice and telegrams. Since the mid-1990s, as the internet has grown in popularity, voice has been gradually supplanted by data. This soon demonstrated the limitations of copper in data transmission, prompting the development of optics.",Telecommunications,0,1
1228,1209759,"Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods which learn by bootstrapping from the current estimate of the value function. These methods sample from the environment, like Monte Carlo methods, and perform updates based on current estimates, like dynamic programming methods.While Monte Carlo methods only adjust their estimates once the final outcome is known, TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known. This is a form of bootstrapping, as illustrated with the following example:

""Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – and thus be able to change, say, Saturday's model before Saturday arrives.""Temporal difference methods are related to the temporal difference model of animal learning.",Temporal difference learning,0,1
1229,486876,"Omeprazole, sold under the brand names Prilosec and Losec, among others, is a medication used in the treatment of gastroesophageal reflux disease (GERD), peptic ulcer disease, and Zollinger–Ellison syndrome. It is also used to prevent upper gastrointestinal bleeding in people who are at high risk. Omeprazole is a proton-pump inhibitor (PPI) and its effectiveness is similar to other PPIs. It can be taken by mouth or by injection into a vein.Common side effects include nausea, vomiting, headaches, abdominal pain, and increased intestinal gas. Serious side effects may include Clostridium difficile colitis, an increased risk of pneumonia, an increased risk of bone fractures, and the potential of masking stomach cancer. It is unclear if it is safe for use in pregnancy. It works by blocking the release of stomach acid.Omeprazole was patented in 1978, and approved for medical use in 1988. It is on the World Health Organization's List of Essential Medicines. It is available as a generic medication. In 2019, it was the eighth most commonly prescribed medication in the United States, with more than 52 million prescriptions. It is also available without a prescription in the United States.

",Omeprazole,0,1
1230,50673241,"Tensor Processing Unit (TPU) is an AI accelerator application-specific integrated circuit (ASIC) developed by Google for neural network machine learning, using Google's own TensorFlow software. Google began using TPUs internally in 2015, and in 2018 made them available for third party use, both as part of its cloud infrastructure and by offering a smaller version of the chip for sale.",Tensor Processing Unit,0,1
1231,499010,"In mathematics, tensor calculus, tensor analysis, or Ricci calculus is an extension of vector calculus to tensor fields (tensors that may vary over a manifold, e.g. in spacetime).
Developed by Gregorio Ricci-Curbastro and his student Tullio Levi-Civita, it was used by Albert Einstein to develop his general theory of relativity. Unlike the infinitesimal calculus, tensor calculus allows presentation of physics equations in a form that is independent of the choice of coordinates on the manifold.
Tensor calculus has many applications in physics, engineering and computer science including elasticity, continuum mechanics, electromagnetism (see mathematical descriptions of the electromagnetic field), general relativity (see mathematics of general relativity), quantum field theory, and machine learning.

Working with a main proponent of the exterior calculus Elie Cartan, the influential geometer Shiing-Shen Chern summarizes the role of tensor calculus:In our subject of differential geometry, where you talk about manifolds, one difficulty is that the geometry is described by coordinates, but the coordinates do not have meaning. They are allowed to undergo transformation. And in order to handle this kind of situation, an important tool is the so-called tensor analysis, or Ricci calculus, which was new to mathematicians. In mathematics you have a function, you write down the function, you calculate, or you add, or you multiply, or you can differentiate. You have something very concrete. In geometry the geometric situation is described by numbers, but you can change your numbers arbitrarily. So to handle this, you need the Ricci calculus.",Tensor calculus,0,1
1232,1514392,"In machine learning, a common task is the study and construction of algorithms that can learn from and make predictions on data. Such algorithms function by making data-driven predictions or decisions, through building a mathematical model from input data. These input data used to build the model are usually divided in multiple data sets. In particular, three data sets are commonly used in different stages of the creation of the model: training, validation and test sets.
The model is initially fit on a training data set, which is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model. The model (e.g. a naive Bayes classifier) is trained on the training data set using a supervised learning method, for example using optimization methods such as gradient descent or stochastic gradient descent. In practice, the training data set often consists of pairs of an input vector (or scalar) and the corresponding output vector (or scalar), where the answer key is commonly denoted as the target (or label). The current model is run with the training data set and produces a result, which is then compared with the target, for each input vector in the training data set. Based on the result of the comparison and the specific learning algorithm being used, the parameters of the model are adjusted. The model fitting can include both variable selection and parameter estimation.
Successively, the fitted model is used to predict the responses for the observations in a second data set called the validation data set. The validation data set provides an unbiased evaluation of a model fit on the training data set while tuning the model's hyperparameters (e.g. the number of hidden units—layers and layer widths—in a neural network). Validation datasets can be used for regularization by early stopping (stopping training when the error on the validation data set increases, as this is a sign of over-fitting to the training data set).
This simple procedure is complicated in practice by the fact that the validation dataset's error may fluctuate during training, producing multiple local minima. This complication has led to the creation of many ad-hoc rules for deciding when over-fitting has truly begun.Finally, the test data set is a data set used to provide an unbiased evaluation of a final model fit on the training data set. If the data in the test data set has never been used in training (for example in cross-validation), the test data set is also called a holdout data set. The term ""validation set"" is sometimes used instead of ""test set"" in some literature (e.g., if the original data set was partitioned into only two subsets, the test set might be referred to as the validation set).Deciding the sizes and strategies for data set division in training, test and validation sets is very dependent on the problem and data available.","Training, validation, and test data sets",1,1
1233,53887,"In linguistics, a corpus (plural corpora) or text corpus is a language resource consisting of a large and structured set of texts (nowadays usually electronically stored and processed). In corpus linguistics, they are used to do statistical analysis and hypothesis testing, checking occurrences or validating linguistic rules within a specific language territory.
In search technology, a corpus is the collection of documents which is being searched.

",Text corpus,0,1
1234,47937215,"The Master Algorithm: How the Quest for the Ultimate Learning Machine Will Remake Our World is a book by Pedro Domingos released in 2015. Domingos wrote the book in order to generate interest from people outside the field.

",The Master Algorithm,0,1
1235,47089,"The Register is a British technology news website co-founded in 1994 by Mike Magee, John Lettice and Ross Alderson. The online newspaper's masthead sublogo is ""Biting the hand that feeds IT."" Their primary focus is information technology news and opinions.
Situation Publishing Ltd is listed as the site's publisher. Drew Cullen is an owner and Linus Birtles is the managing director. Andrew Orlowski was the executive editor before leaving the website in May 2019.

",The Register,0,1
1236,173070,"The Wall Street Journal is an American business-focused, international daily newspaper based in New York City, with international editions also available in Chinese and Japanese. The Journal, along with its Asian editions, is published six days a week by Dow Jones & Company, a division of News Corp. The newspaper is published in the broadsheet format and online. The Journal has been printed continuously since its inception on July 8, 1889, by Charles Dow, Edward Jones, and Charles Bergstresser.The Wall Street Journal is one of the largest newspapers in the United States by circulation, with a circulation of about 2.834 million copies (including nearly 1,829,000 digital sales) as of August 2019, compared with USA Today's 1.7 million. The Journal publishes the luxury news and lifestyle magazine WSJ, which was originally launched as a quarterly but expanded to 12 issues in 2014. An online version was launched in 1995, which has been accessible only to subscribers since it began.: 37 It is regarded as a newspaper of record, particularly in terms of business and financial news. The newspaper has won 37 Pulitzer Prizes (as of 2019). The editorial pages of the Journal are typically American conservative in their position. The Journal has published opinion pieces that were at odds with the scientific consensus on multiple environmental issues.",The Wall Street Journal,0,1
1237,33520809,"Theano is a Python library and optimizing compiler for manipulating and evaluating mathematical expressions, especially matrix-valued ones.
In Theano, computations are expressed using a NumPy-esque syntax and compiled to run efficiently on either CPU or GPU architectures.
Theano is an open source project primarily developed by the Montreal Institute for Learning Algorithms (MILA) at the Université de Montréal.The name of the software references the ancient philosopher Theano, long associated with the development of the golden mean.
On 28 September 2017, Pascal Lamblin posted a message from Yoshua Bengio, Head of MILA:  major development would cease after the 1.0 release due to competing offerings by strong industrial players. Theano 1.0.0 was then released on 15 November 2017.On 17 May 2018, Chris Fonnesbeck wrote on behalf of the PyMC development team that the PyMC developers will officially assume control of Theano maintenance once they step down. On 29 January 2021, they started using the name Aesara for their fork of Theano.",Theano (software),0,1
1238,323392,"Theoretical computer science (TCS) is a subset of general computer science and mathematics that focuses on mathematical aspects of computer science such as the theory of computation, lambda calculus, and type theory.
It is difficult to circumscribe the theoretical areas precisely. The ACM's Special Interest Group on Algorithms and Computation Theory (SIGACT) provides the following description:
TCS covers a wide variety of topics including algorithms, data structures, computational complexity, parallel and distributed computation, probabilistic computation, quantum computation, automata theory, information theory, cryptography, program semantics and verification, algorithmic game theory,machine learning, computational biology, computational economics, computational geometry, and computational number theory and algebra. Work in this field is often distinguished by its emphasis on mathematical technique and rigor.

",Theoretical computer science,0,1
1240,31600,"Time (stylized in all caps) is an American news magazine and news website published and based in New York City. For nearly a century, it was published weekly, but starting in March 2020 it transitioned to every other week. It was first published in New York City on March 3, 1923, and for many years it was run by its influential co-founder, Henry Luce. A European edition (Time Europe, formerly known as Time Atlantic) is published in London and also covers the Middle East, Africa, and, since 2003, Latin America. An Asian edition (Time Asia) is based in Hong Kong. The South Pacific edition, which covers Australia, New Zealand, and the Pacific Islands, is based in Sydney.
Since 2018, Time has been published by Time USA, LLC, owned by Marc Benioff, who acquired it from Meredith Corporation.",Time (magazine),0,1
1241,405944,"In computer science, the time complexity is the computational complexity that describes the amount of computer time it takes to run an algorithm. Time complexity is commonly estimated by counting the number of elementary operations performed by the algorithm, supposing that each elementary operation takes a fixed amount of time to perform. Thus, the amount of time taken and the number of elementary operations performed by the algorithm are taken to be related by a constant factor.
Since an algorithm's running time may vary among different inputs of the same size, one commonly considers the worst-case time complexity, which is the maximum amount of time required for inputs of a given size. Less common, and usually specified explicitly, is the average-case complexity, which is the average of the time taken on inputs of a given size (this makes sense because there are only a finite number of possible inputs of a given size). In both cases, the time complexity is generally expressed as a function of the size of the input.: 226  Since this function is generally difficult to compute exactly, and the running time for small inputs is usually not consequential, one commonly focuses on the behavior of the complexity when the input size increases—that is, the asymptotic behavior of the complexity. Therefore, the time complexity is commonly expressed using big O notation, typically 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
  , 
  
    
      
        O
        (
        n
        log
        ⁡
        n
        )
      
    
    {\displaystyle O(n\log n)}
  , 
  
    
      
        O
        (
        
          n
          
            α
          
        
        )
      
    
    {\displaystyle O(n^{\alpha })}
  , 
  
    
      
        O
        (
        
          2
          
            n
          
        
        )
      
    
    {\displaystyle O(2^{n})}
  , etc., where n is the size in units of bits needed to represent the input.
Algorithmic complexities are classified according to the type of function appearing in the big O notation. For example, an algorithm with time complexity 
  
    
      
        O
        (
        n
        )
      
    
    {\displaystyle O(n)}
   is a linear time algorithm and an algorithm with time complexity 
  
    
      
        O
        (
        
          n
          
            α
          
        
        )
      
    
    {\displaystyle O(n^{\alpha })}
   for some constant 
  
    
      
        α
        >
        1
      
    
    {\displaystyle \alpha >1}
   is a polynomial time algorithm.",Time complexity,0,1
1243,12413470,"This is a timeline of artificial intelligence, or maybe even synthetic intelligence.",Timeline of artificial intelligence,0,1
1244,50828755,"This page is a timeline of machine learning. Major discoveries, achievements, milestones and other major events are included.",Timeline of machine learning,0,1
1245,33275304,"Tom Michael Mitchell (born August 9, 1951) is an American computer scientist and E. Fredkin University Professor at the Carnegie Mellon University (CMU). He is a former Chair of the Machine Learning Department at CMU. Mitchell is known for his contributions to the advancement of machine learning, artificial intelligence, and cognitive neuroscience and is the author of the textbook Machine Learning. He is a member of the United States National Academy of Engineering since 2010. He is also a Fellow of the American Association for the Advancement of Science and a Fellow the Association for the Advancement of Artificial Intelligence. In October 2018, Mitchell was appointed as the Interim Dean of the School of Computer Science at Carnegie Mellon.",Tom M. Mitchell,0,1
1246,25757480,"Tomasz Imieliński (born July 11, 1954 in Toruń, Poland) is a Polish-American computer scientist, most known in the areas of data mining, mobile computing, data extraction, and search engine technology. He is currently a professor of computer science at Rutgers University in New Jersey, United States.
In 2000, he co-founded Connotate Technologies, a web data extraction company based in New Brunswick, NJ. Since 2004 till 2010 he had held multiple positions at Ask.com, from vice president of data solutions intelligence to executive vice president of global search and answers and chief scientist. From 2010 to 2012 he served as VP of data solutions at IAC/Pronto.Tomasz Imieliński served as chairman of the Computer Science Department at Rutgers University from 1996 to 2003.
He co-founded Art Data Laboratories LLC company, and its product, Articker is the largest known database that aggregates dynamic non-price information about the visual artists in the global art market. Articker has been under an exclusive partnership with Phillips auction house.",Tomasz Imieliński,0,1
1247,28934119,"In statistics and natural language processing, a topic model is a type of statistical model for discovering the abstract ""topics"" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: ""dog"" and ""bone"" will appear more often in documents about dogs, ""cat"" and ""meow"" will appear in documents about cats, and ""the"" and ""is"" will appear approximately equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The ""topics"" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.
Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics and computer vision.

",Topic model,0,1
1248,42571226,"Torch is an open-source machine learning library, 
a scientific computing framework, and a script language based on the Lua programming language. It provides a wide range of algorithms for deep learning, and uses the scripting language LuaJIT, and an underlying C implementation. It was created at IDIAP at EPFL. As of 2018, Torch is no longer in active development. However PyTorch, which is based on the Torch library, is actively developed as of May 2022.",Torch (machine learning),0,1
1249,68639524,"The Toronto Declaration: Protecting the Rights to Equality and Non-Discrimination in Machine Learning Systems is a declaration that advocates responsible practices for machine learning practitioners and governing bodies. It is a joint statement issued by groups including Amnesty International and Access Now, with other notable signatories including Human Rights Watch and The Wikimedia Foundation. It was published at RightsCon on May 16, 2018.The Declaration focuses on concerns of algorithmic bias and the potential for discrimination that arises from the use of machine learning and artificial intelligence in applications that may affect people's lives, ""from policing, to welfare systems, to healthcare provision, to platforms for online discourse."" A secondary concern of the document is the potential for violations of information privacy.
The goal of the Declaration is to outline ""tangible and actionable standards for states and the private sector."" The Declaration calls for tangible solutions, such as reparations for the victims of algorithmic discrimination.

",Toronto Declaration,0,1
1252,171623,"Training Day is a 2001 American crime thriller film directed by Antoine Fuqua and written by David Ayer. It stars Denzel Washington as Alonzo Harris and Ethan Hawke as Jake Hoyt, two LAPD narcotics officers over a 24-hour period in the gang-ridden neighborhoods of Westlake, Echo Park and South Central Los Angeles. It also features Scott Glenn, Cliff Curtis, Dr. Dre, Snoop Dogg, and Macy Gray in supporting roles.
Training Day was released on October 5, 2001, by Warner Bros. Pictures. It received positive reviews from critics, who praised Washington and Hawke's performances but were divided on the screenplay. It was a commercial success, grossing $104 million worldwide against a production budget of $45 million.
The film received numerous accolades and nominations, with Washington's performance earning him the Academy Award for Best Actor and Hawke being nominated for Best Supporting Actor at the 74th Academy Awards.
A television series based on the film, produced by Jerry Bruckheimer, was announced in August 2015 and premiered February 2, 2017 on CBS, but was cancelled after one season. A prequel about a young Alonzo Harris was announced in October 2019 as currently being in development by Warner Bros. Pictures. On February 28, 2022, the prequel, named Training Day: Day of the Riot has started production in California.",Training Day,0,1
1253,61603971,"A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP) and computer vision (CV).Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.Transformers were introduced in 2017 by a team at Google Brain and are increasingly the model of choice for NLP problems, replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.

",Transformer (machine learning model),0,1
1254,35425965,"Trevor John Hastie (born 27 June 1953) is an American statistician and computer scientist. He is currently serving as the John A. Overdeck Professor of Mathematical Sciences and Professor of Statistics at Stanford University. Hastie is known for his contributions to applied statistics, especially in the field of machine learning, data mining, and bioinformatics. He has authored several popular books in statistical learning, including The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Hastie has been listed as an ISI Highly Cited Author in Mathematics by the ISI Web of Knowledge.",Trevor Hastie,0,1
1255,21391751,"The Turing test, originally called the imitation game by Alan Turing in 1950, is a test of a machine's ability to exhibit intelligent behaviour equivalent to, or indistinguishable from, that of a human. Turing proposed that a human evaluator would judge natural language conversations between a human and a machine designed to generate human-like responses. The evaluator would be aware that one of the two partners in conversation was a machine, and all participants would be separated from one another. The conversation would be limited to a text-only channel, such as a computer keyboard and screen, so the result would not depend on the machine's ability to render words as speech. If the evaluator could not reliably tell the machine from the human, the machine would be said to have passed the test. The test results would not depend on the machine's ability to give correct answers to questions, only on how closely its answers resembled those a human would give.
The test was introduced by Turing in his 1950 paper ""Computing Machinery and Intelligence"" while working at the University of Manchester. It opens with the words: ""I propose to consider the question, 'Can machines think?'"" Because ""thinking"" is difficult to define, Turing chooses to ""replace the question by another, which is closely related to it and is expressed in relatively unambiguous words."" Turing describes the new form of the problem in terms of a three-person game called the ""imitation game"", in which an interrogator asks questions of a man and a woman in another room in order to determine the correct sex of the two players. Turing's new question is: ""Are there imaginable digital computers which would do well in the imitation game?"" This question, Turing believed, was one that could actually be answered. In the remainder of the paper, he argued against all the major objections to the proposition that ""machines can think"".Since Turing introduced his test, it has been both highly influential and widely criticised, and has become an important concept in the philosophy of artificial intelligence. Some of its criticisms, such as John Searle's Chinese room, are themselves controversial.",Turing test,0,1
1256,57179040,U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 × 512 image takes less than a second on a modern GPU.,U-Net,0,1
1257,31871,"Ubiquitous computing (or ""ubicomp"") is a concept in software engineering, hardware engineering and computer science where computing is made to appear anytime and everywhere. In contrast to desktop computing, ubiquitous computing can occur using any device, in any location, and in any format. A user interacts with the computer, which can exist in many different forms, including laptop computers, tablets, smart phones and terminals in everyday objects such as a refrigerator or a pair of glasses. The underlying technologies to support ubiquitous computing include Internet, advanced middleware, operating system, mobile code, sensors, microprocessors, new I/O and user interfaces, computer networks, mobile protocols, location and positioning, and new materials.
This paradigm is also described as pervasive computing, ambient intelligence, or ""everyware"". Each term emphasizes slightly different aspects. When primarily concerning the objects involved, it is also known as physical computing, the Internet of Things, haptic computing, and ""things that think"".
Rather than propose a single definition for ubiquitous computing and for these related terms, a taxonomy of properties for ubiquitous computing has been proposed, from which different kinds or flavors of ubiquitous systems and applications can be described.Ubiquitous computing themes include: distributed computing, mobile computing, location computing, mobile networking, sensor networks, human–computer interaction, context-aware smart home technologies, and artificial intelligence.",Ubiquitous computing,0,1
1259,305843,"Vapnik–Chervonenkis theory (also known as VC theory) was developed during 1960–1990 by Vladimir Vapnik and Alexey Chervonenkis. The theory is a form of computational learning theory, which attempts to explain the learning process from a statistical point of view.
VC theory is related to statistical learning theory  and to empirical processes.  Richard M. Dudley and Vladimir Vapnik, among others, have applied VC-theory to empirical processes.",Vapnik–Chervonenkis theory,0,1
1260,62078649,"In machine learning, a variational autoencoder (VAE), is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling, belonging to the families of probabilistic graphical models and variational Bayesian methods.
Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders allow statistical inference problems (such as inferring the value of one random variable from another random variable) to be rewritten  as statistical optimization problems (i.e find the parameter values that minimize some objective function).  They are meant to map the input variable to a multivariate latent distribution. Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.",Variational autoencoder,0,1
1261,32823,"Very large-scale integration (VLSI) is the process of creating an integrated circuit (IC) by combining millions of MOS transistors onto a single chip. VLSI began in the 1970s when MOS integrated circuit (Metal Oxide Semiconductor) chips were widely adopted, enabling complex semiconductor and telecommunication technologies to be developed. The microprocessor and memory chips are VLSI devices. Before the introduction of VLSI technology, most ICs had a limited set of functions they could perform. An electronic circuit might consist of a CPU, ROM, RAM and other glue logic. VLSI enables IC designers to add all of these into one chip.",Very Large Scale Integration,0,1
1262,5363,"A video game or computer game is an electronic game that involves interaction with a user interface or input device – such as a joystick, controller, keyboard, or motion sensing device – to generate visual feedback. This feedback mostly commonly is shown on a video display device, such as a TV set, monitor, touchscreen, or virtual reality headset. Some computer games do not always depend on a graphics display, for example text adventure games and computer chess can be played through teletype printers. Video games are often augmented with audio feedback delivered through speakers or headphones, and sometimes with other types of feedback, including haptic technology.
Video games are defined based on their platform, which include arcade video games, console games, and personal computer (PC) games. More recently, the industry has expanded onto mobile gaming through smartphones and tablet computers, virtual and augmented reality systems, and remote cloud gaming. Video games are classified into a wide range of genres based on their type of gameplay and purpose.
The first video game prototypes in the 1950s and 1960s were simple extensions of electronic games using video-like output from large room-size computers. The first consumer video game was the arcade video game Computer Space in 1971. In 1972 came the iconic hit arcade game Pong, and the first home console, the Magnavox Odyssey. The industry grew quickly during the golden age of arcade video games from the late 1970s to early 1980s, but suffered from the crash of the North American video game market in 1983 due to loss of publishing control and saturation of the market. Following the crash, the industry matured, dominated by Japanese companies such as Nintendo, Sega, and Sony, and established practices and methods around the development and distribution of video games to prevent a similar crash in the future, many which continue to be followed. Today, video game development requires numerous skills to bring a game to market, including developers, publishers, distributors, retailers, console and other third-party manufacturers, and other roles.
In the 2000s, the core industry centered on ""AAA"" games, leaving little room for riskier, experimental games. Coupled with the availability of the Internet and digital distribution, this gave room for independent video game development (or indie games) to gain prominence into the 2010s. Since then, the commercial importance of the video game industry has been increasing. The emerging Asian markets and mobile games on smartphones in particular are altering player demographics towards casual gaming and increasing monetization by incorporating games as a service. As of 2020, the global video game market has estimated annual revenues of US$159 billion across hardware, software, and services. This is three times the size of the 2019 global music industry and four times that of the 2019 film industry.",Video game,0,1
1263,226779,"Vinod Khosla (born 28 January 1955) is an Indian-American billionaire businessman and venture capitalist. He is a co-founder of Sun Microsystems and the founder of Khosla Ventures. Khosla made his wealth from early venture capital investments in areas such as networking, software, and alternative energy technologies. He is considered one of the most successful and influential venture capitalists.In 2014, Forbes counted him among the 400 richest people in the United States. In 2021, he was ranked 92nd on the Forbes 400 list.Since 2010, he has been engaged in a legal dispute related to his attempt to close off public access to Martins Beach.",Vinod Khosla,0,1
1264,32353,"In computing, a virtual machine (VM) is the virtualization/emulation of a computer system.  Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.
Virtual machines differ and are organized by their function, shown here:

System virtual machines (also termed full virtualization VMs) provide a substitute for a real machine. They provide functionality needed to execute entire operating systems. A hypervisor uses native execution to share and manage hardware, allowing for multiple environments which are isolated from one another, yet exist on the same physical machine. Modern hypervisors use hardware-assisted virtualization, virtualization-specific hardware, primarily from the host CPUs.
Process virtual machines are designed to execute computer programs in a platform-independent environment.Some virtual machine emulators, such as QEMU and video game console emulators, are designed to also emulate (or ""virtually imitate"") different system architectures thus allowing execution of software applications and operating systems written for another CPU or architecture. Operating-system-level virtualization allows the resources of a computer to be partitioned via the kernel. The terms are not universally interchangeable.",Virtual machine,0,1
1265,32612,"Virtual reality (VR) is a simulated experience that can be similar to or completely different from the real world. Applications of virtual reality include entertainment (particularly video games), education (such as medical or military training) and business (such as virtual meetings). Other distinct types of VR-style technology include augmented reality and mixed reality, sometimes referred to as extended reality or XR.Currently, standard virtual reality systems use either virtual reality headsets or multi-projected environments to generate realistic images, sounds and other sensations that simulate a user's physical presence in a virtual environment. A person using virtual reality equipment is able to look around the artificial world, move around in it, and interact with virtual features or items. The effect is commonly created by VR headsets consisting of a head-mounted display with a small screen in front of the eyes, but can also be created through specially designed rooms with multiple large screens. Virtual reality typically incorporates auditory and video feedback, but may also allow other types of sensory and force feedback through haptic technology.

",Virtual reality,0,1
1266,50228744,"A vision processing unit (VPU) is (as of 2018) an emerging class of microprocessor; it is a specific type of AI accelerator, designed to accelerate machine vision tasks.",Vision processing unit,0,1
1267,68212199,"A Vision Transformer (ViT) is a transformer that is targeted at vision processing tasks such as image recognition.

",Vision transformer,0,1
1268,660850,"Visualization or visualisation (see spelling differences) is any technique for creating images, diagrams, or animations to communicate a message. Visualization through visual imagery has been an effective way to communicate both abstract and concrete ideas since the dawn of humanity. Examples from history include cave paintings, Egyptian hieroglyphs, Greek geometry, and Leonardo da Vinci's revolutionary methods of technical drawing for engineering and scientific purposes.
Visualization today has ever-expanding applications in science, education, engineering (e.g., product visualization), interactive multimedia, medicine, etc. Typical of a visualization application is the field of computer graphics. The invention of computer graphics (and 3D computer graphics) may be the most important development in visualization since the invention of central perspective in the Renaissance period. The development of animation also helped advance visualization.

",Visualization (graphics),0,1
1271,60968880,"Weak supervision is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting. This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.",Weak supervision,0,1
1272,33109,"A wearable computer, also known as a wearable or body-borne computer, is a computing device worn on the body. The definition of 'wearable computer' may be narrow or broad, extending to smartphones or even ordinary wristwatches.Wearables may be for general use, in which case they are just a particularly small example of mobile computing. Alternatively, they may be for specialized purposes such as fitness trackers. They may incorporate special sensors such as accelerometers, heart rate monitors, or on the more advanced side, electrocardiogram (ECG) and blood oxygen saturation (SpO2) monitors.  Under the definition of wearable computers, we also include novel user interfaces such as Google Glass, an optical head-mounted display controlled by gestures. It may be that specialized wearables will evolve into general all-in-one devices, as happened with the convergence of PDAs and mobile phones into smartphones.
Wearables are typically worn on the wrist (e.g. fitness trackers), hung from the neck (like a necklace), strapped to the arm or leg (smartphones when exercising), or on the head (as glasses or a helmet), though some have been located elsewhere (e.g. on a finger or in a shoe). Devices carried in a pocket or bag – such as smartphones and before them pocket calculators and PDAs, may or may not be regarded as 'worn'.
Wearable computers have various technical issues common to other mobile computing, such as batteries, heat dissipation, software architectures, wireless and personal area networks, and data management. Many wearable computers are active all the time, e.g. processing or recording data continuously.",Wearable computer,0,1
1274,3003284,"The process of weighting involves emphasizing the contribution of particular aspects of a phenomenon (or of a set of data) over others to an outcome or result; thereby highlighting those aspects in comparison to others in the analysis. That is, rather than each variable in the data set contributing equally to the final result, some of the data is adjusted to make a greater contribution than others. This is analogous to the practice of adding (extra) weight to one side of a pair of scales in order to favour either the buyer or seller.
While weighting may be applied to a set of data, such as epidemiological data, it is more commonly applied to measurements of light, heat, sound, gamma radiation, and in fact any stimulus that is spread over a spectrum of frequencies.",Weighting,0,1
1275,3829034,"Waikato Environment for Knowledge Analysis (Weka), developed at the University of Waikato, New Zealand, is free software licensed under the GNU General Public License, and the companion software to the book ""Data Mining: Practical Machine Learning Tools and Techniques"".",Weka (machine learning),0,1
1276,768799,"White-box testing (also known as clear box testing, glass box testing, transparent box testing, and structural testing) is a method of software testing that tests internal structures or workings of an application, as opposed to its functionality (i.e. black-box testing). In white-box testing an internal perspective of the system, as well as programming skills, are used to design test cases. The tester chooses inputs to exercise paths through the code and determine the expected outputs. This is analogous to testing nodes in a circuit, e.g. in-circuit testing (ICT).
White-box testing can be applied at the unit, integration and system levels of the software testing process. Although traditional testers tended to think of white-box testing as being done at the unit level, it is used for integration and system testing more frequently today. It can test paths within a unit, paths between units during integration, and between subsystems during a system–level test. Though this method of test design can uncover many errors or problems, it has the potential to miss unimplemented parts of the specification or missing requirements. Where white-box testing is design-driven, that is, driven exclusively by agreed specifications of how each component of software is required to behave (as in DO-178C and ISO 26262 processes) then white-box test techniques can accomplish assessment for unimplemented or missing requirements.
White-box test design techniques include the following code coverage criteria:

Control flow testing
Data flow testing
Branch testing
Statement coverage
Decision coverage
Modified condition/decision coverage
Prime path testing
Path testing",White-box testing,0,1
1277,47527969,"Word2vec is a technique for natural language processing published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence. As the name implies, word2vec represents each distinct word with a particular list of numbers called a vector. The vectors are chosen carefully such that a simple mathematical function (the cosine similarity between the vectors) indicates the level of semantic similarity between the words represented by those vectors.",Word2vec,0,1
1278,57307464,"A word processor (WP) is a device or computer program that provides for input, editing, formatting, and output of text, often with some additional features.
Early word processors were stand-alone devices dedicated to the function, but current word processors are word processor programs running on general purpose computers.
The functions of a word processor program fall somewhere between those of a simple text editor and a fully functioned desktop publishing program. However, the distinctions between these three have changed over time and were unclear after 2010.",Word processor,0,1
1279,33139,"The World Wide Web (WWW), commonly known as the Web, is an information system enabling documents and other web resources to be accessed over the Internet.Documents and downloadable media are made available to the network through web servers and can be accessed by programs such as web browsers. Servers and resources on the World Wide Web are identified and located through character strings called uniform resource locators (URLs). The original and still very common document type is a web page formatted in Hypertext Markup Language (HTML). This markup language supports plain text, images, embedded video and audio contents, and scripts (short programs) that implement complex user interaction. The HTML language also supports hyperlinks (embedded URLs) which provide immediate access to other web resources. Web navigation, or web surfing, is the common practice of following such hyperlinks across multiple websites. Web applications are web pages that function as application software. The information in the Web is transferred across the Internet using the Hypertext Transfer Protocol (HTTP).
Multiple web resources with a common theme and usually a common domain name make up a website. A single web server may provide multiple websites, while some websites, especially the most popular ones, may be provided by multiple servers. Website content is provided by a myriad of companies, organizations, government agencies, and individual users; and comprises an enormous mass of educational, entertainment, commercial, and government information.
The World Wide Web has become world's dominant software platform. It is the primary tool billions of people worldwide use to interact with the Internet.The Web was originally conceived as a document management system. It was invented by Tim Berners-Lee at CERN in 1989 and opened to the public in 1991.",World Wide Web,0,1
1280,49541331,"XGBoost (eXtreme Gradient Boosting) is an open-source software library which provides a regularizing gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala. It works on Linux, Windows, and macOS. From the project description, it aims to provide a ""Scalable, Portable and Distributed Gradient Boosting (GBM, GBRT, GBDT) Library"". It runs on a single machine, as well as the distributed processing frameworks Apache Hadoop, Apache Spark, Apache Flink, and Dask.It has gained much popularity and attention recently as the algorithm of choice for many winning teams of machine learning competitions.",XGBoost,0,1
1281,23534873,"Yann André LeCun ( French pronunciation: ​[ləkɛ̃]; originally spelled Le Cun; born July 8, 1960) is a French computer scientist working primarily in the fields of machine learning, computer vision, mobile robotics, and computational neuroscience. He is the Silver Professor of the Courant Institute of Mathematical Sciences at New York University, and Vice President, Chief AI Scientist at Meta.He is well known for his work on optical character recognition and computer vision using convolutional neural networks (CNN), and is a founding father of convolutional nets. He is also one of the main creators of the DjVu image compression technology (together with Léon Bottou and Patrick Haffner). He co-developed the Lush programming language with Léon Bottou.
LeCun received the 2018 Turing Award (often referred to as ""Nobel Prize of Computing""), together with Yoshua Bengio and Geoffrey Hinton, for their work on deep learning.
The three are sometimes referred to as the ""Godfathers of AI"" and ""Godfathers of Deep Learning"".",Yann LeCun,0,1
1282,43375904,"Yooreeka is a library for data mining, machine learning, soft computing, and mathematical analysis. The project started with the code of the book ""Algorithms of the Intelligent Web"". Although the term ""Web"" prevailed in the title, in essence, the algorithms are valuable in any software application.
It covers all major algorithms and provides many examples.
Yooreeka 2.x is licensed under the Apache License rather than the somewhat more restrictive LGPL (which was the license of v1.x).
The library is written 100% in the Java language.",Yooreeka,0,1
1283,47749536,"Yoshua Bengio  (born March 5, 1964) is a Canadian computer scientist, most noted for his work on artificial neural networks and deep learning. He is a professor at the Department of Computer Science and Operations Research at the Université de Montréal and scientific director of the Montreal Institute for Learning Algorithms (MILA).
Bengio received the 2018 ACM A.M. Turing Award, together with Geoffrey Hinton and Yann LeCun, for their work in deep learning. Bengio, Hinton, and LeCun, are sometimes referred to as the ""Godfathers of AI"" and ""Godfathers of Deep Learning"".",Yoshua Bengio,0,1
