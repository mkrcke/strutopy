{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "acc3e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.parsing.preprocessing import remove_stopwords, preprocess_string, strip_numeric, stem_text, strip_punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "978f5bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('poliblogs2008.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cc83c4",
   "metadata": {},
   "source": [
    "### textProcessor()\n",
    "Function that takes a vector of raw texts and performs basic operations. Uses the tm-package for these operations.\n",
    "\n",
    "Input is designed as a spreadsheet, where each document is in a single cell. \n",
    "- Stemming (snowballStemmer:: SnowballC)\n",
    "- Sparsity and Stopword Removal \n",
    "- Empty Document removal \n",
    "- Specified Metadata\n",
    "\n",
    "1. Strips text and removes all non-characters. \n",
    "2. Builds corpus\n",
    "3. Converts to lower case\n",
    "4. removes (custom) punctuation\n",
    "5. removes (custom) stopwords\n",
    "6. removing number \n",
    "7. stemming\n",
    "8. assigns metadata\n",
    "9. creates output (document term matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8545157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "data.documents = data.documents.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c602cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "data.documents = data.documents.apply(strip_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ff07f87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Punctuation\n",
    "data.documents = data.documents.apply(strip_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "92c6c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming\n",
    "data.documents = data.documents.apply(stem_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "875f4d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        week fal statements  lies  dismiss apologies  ...\n",
       "1        honestli don t know party  caucu result plai t...\n",
       "2        stand aw will troop iraq sacrif themselv natio...\n",
       "3        page recent said goodbi global warming  ironic...\n",
       "4        report enemi control inform battlefield falluj...\n",
       "                               ...                        \n",
       "13241    check thi new mailer   forward wai dem oper   ...\n",
       "13242    here  intern discuss insid hillari campaign th...\n",
       "13243    edward senior advi joe trippi ha theori clinto...\n",
       "13244    thi interesting  huffington post blockbust sco...\n",
       "13245    fox new report jame carvil paul begala re ent ...\n",
       "Name: documents, Length: 13246, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a730422b",
   "metadata": {},
   "source": [
    "## metadata: Additional data about the documents\n",
    "Specifically a dataframe or matrix object with number of rows equal to the number of documents and one column per meta-data type. The column names are used to label the metadata.  The metadata do not affect the text processing, but providing the metadata object insures that if documents are dropped the corresponding metadata rows are dropped as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b5c84677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topical content covariate: rating\n",
    "# topical prevalence covariate: blog\n",
    "metadata = ['rating', 'blog'] \n",
    "meta = data.loc[:,metadata]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12996f1",
   "metadata": {},
   "source": [
    "## Document-Term-Matrix\n",
    "- documents: documents are stored as indexed word counts\n",
    "- vocab: indexed word vocabulary\n",
    "\n",
    "**Note**: While the output for the stm() function in R is required to be a Document-Term-Matrix, gensim relies on the bag-of-words representation of text. This might be a difference between the R implementation and the Python implementation. It has to be evaluated, whether the dtm-representation should be retained or being replaced with the BoW-representation. If so, adjustments to the algorithm are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "48c6b268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize, simple_preprocess\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "85dbd55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_tokens = [simple_preprocess(doc) for doc in data.documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "74b79257",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(doc_tokens)\n",
    "# Vocabulary\n",
    "dictionary[0]\n",
    "vocab = dictionary.id2token\n",
    "# Documents\n",
    "documents = [dictionary.doc2bow(doc) for doc in doc_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986ee516",
   "metadata": {},
   "source": [
    "To compute the baseline probabilities $m$, the vector of word-counts by document needs to be unlisted and then normalized. Hence $\\textit{documents}$, need to be transformed to a vector of indexed counts with length $V$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "73b72eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_documents = [item for sublist in documents for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1c887915",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m = []\n",
    "total_sum = sum(n for _, n in flat_documents)\n",
    "\n",
    "for elem in flat_documents: \n",
    "    m.append(elem[1] / total_sum)\n",
    "    \n",
    "# drop words with less than 2 occurrences\n",
    "\n",
    "m = np.log(m) - np.log(np.mean(m)) #logit of m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
